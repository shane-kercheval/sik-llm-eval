{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ideal_responses': ['This is the ideal response (to the first prompt).\\n'\n",
      "                     'This field is optional.\\n'\n",
      "                     'It is only used if a test below is of type '\n",
      "                     '`llm-similarity`, in which case it is used as the ideal '\n",
      "                     'response and the LLM is asked to evaluate the similarity '\n",
      "                     'of the response to the ideal answer.\\n'\n",
      "                     'This value could also be used to fine-tune a model.\\n',\n",
      "                     'This is the ideal response to the second prompt.'],\n",
      " 'metadata': {'description': 'This is an example of an eval. An eval contains '\n",
      "                             'a set of prompts and tests.',\n",
      "              'difficulty': 1,\n",
      "              'name': 'Eval Example',\n",
      "              'source': 'Anaconda',\n",
      "              'tags': ['example', 'graphing', 'plotly-express'],\n",
      "              'uuid': '5f8b1b4e-3b7e-4b0e-8b1b-4e3b7e4b0e8b'},\n",
      " 'prompts': ['This is a question/prompt.',\n",
      "             'This is a followup prompt.\\n'\n",
      "             'The intent is to evaluate multiple responses in a conversation, '\n",
      "             'rather than a single response.\\n'],\n",
      " 'tests': [{'description': 'This is an optional description of the test that '\n",
      "                           'explains what the test is checking or why the '\n",
      "                           'provided answer is correct.',\n",
      "            'type': 'match',\n",
      "            'value': 'This is the expected response. It must match exactly.'},\n",
      "           {'checks': ['def check(code_blocks: list[str]) -> '\n",
      "                       'list[TestResult]:\\n'\n",
      "                       '  return True\\n'],\n",
      "            'setup': 'import os\\n'\n",
      "                     'import sys\\n'\n",
      "                     'from example_test import test_function\\n',\n",
      "            'type': 'code_blocks'},\n",
      "           {'file': 'example_test.py',\n",
      "            'function': 'test_function',\n",
      "            'type': 'python'},\n",
      "           {'function': 'def test_function(response: str) -> '\n",
      "                        'list[TestResult]:\\n'\n",
      "                        '  expectation_1 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  expectation_2 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  return [expectation_1, expectation_2]\\n',\n",
      "            'type': 'python'},\n",
      "           {'model': 'gpt-3.5-turbo-1106', 'type': 'llm-similarity'}]}\n",
      "---\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "with open('/code/eval_template.yaml') as f:\n",
    "    eval_template = yaml.safe_load(f)\n",
    "\n",
    "with open('/code/result_example.yaml') as f:\n",
    "    result_example = yaml.safe_load(f)\n",
    "\n",
    "pprint(eval_template)\n",
    "print('---')\n",
    "pprint(result_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
