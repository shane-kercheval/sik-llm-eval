{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ideal_responses': ['This is the ideal response (to the first prompt).\\n'\n",
      "                     'This field is optional.\\n'\n",
      "                     'It is only used if a test below is of type '\n",
      "                     '`llm-similarity`, in which case it is used as the ideal '\n",
      "                     'response and the LLM is asked to evaluate the similarity '\n",
      "                     'of the response to the ideal answer.\\n'\n",
      "                     'This value could also be used to fine-tune a model.\\n',\n",
      "                     'This is the ideal response to the second prompt.'],\n",
      " 'metadata': {'description': 'This is an example of an eval. An eval contains '\n",
      "                             'a set of prompts and tests.',\n",
      "              'difficulty': 1,\n",
      "              'name': 'Eval Example',\n",
      "              'source': 'Anaconda',\n",
      "              'tags': ['example', 'graphing', 'plotly-express'],\n",
      "              'uuid': '5f8b1b4e-3b7e-4b0e-8b1b-4e3b7e4b0e8b'},\n",
      " 'prompts': ['This is a question/prompt.',\n",
      "             'This is a followup prompt.\\n'\n",
      "             'The intent is to evaluate multiple responses in a conversation, '\n",
      "             'rather than a single response.\\n'],\n",
      " 'tests': [{'description': 'This is an optional description of the test that '\n",
      "                           'explains what the test is checking or why the '\n",
      "                           'provided answer is correct.',\n",
      "            'tags': ['multiple-choice'],\n",
      "            'type': 'match',\n",
      "            'value': 'This is the expected response. It must match exactly.'},\n",
      "           {'checks': ['def check(code_blocks: list[str]) -> '\n",
      "                       'list[TestResult]:\\n'\n",
      "                       '  return True\\n'],\n",
      "            'setup': 'import os\\n'\n",
      "                     'import sys\\n'\n",
      "                     'from example_test import test_function\\n',\n",
      "            'type': 'code_blocks'},\n",
      "           {'file': 'example_test.py',\n",
      "            'function': 'test_function',\n",
      "            'type': 'python'},\n",
      "           {'function': 'def test_function(response: str) -> '\n",
      "                        'list[TestResult]:\\n'\n",
      "                        '  expectation_1 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  expectation_2 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  return [expectation_1, expectation_2]\\n',\n",
      "            'type': 'python'},\n",
      "           {'model': 'gpt-3.5-turbo-1106', 'type': 'llm-similarity'}]}\n",
      "---\n",
      "{'characters_per_second': 9.996,\n",
      " 'code_blocks_passed': 2,\n",
      " 'eval_id': 'uuid',\n",
      " 'hardware': {'cpu': '...', 'gpu': '...', 'memory': '...'},\n",
      " 'model': 'gpt-3.5-turbo-1106',\n",
      " 'num_code_blocks': 2,\n",
      " 'response_characters': 1234,\n",
      " 'responses': ['This is a response to the first prompt.',\n",
      "               'This is a response to the second prompt.'],\n",
      " 'test_results': {'num_results': 10,\n",
      "                  'percent_passed': 70.0,\n",
      "                  'results': [{'description': 'This is a description of the '\n",
      "                                              'test result.',\n",
      "                               'passed': True,\n",
      "                               'type': 'match'},\n",
      "                              {'passed': False}],\n",
      "                  'results_passed': 7},\n",
      " 'total_time': 123.45}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "with open('../examples/eval_template.yaml') as f:\n",
    "    eval_template = yaml.safe_load(f)\n",
    "\n",
    "with open('../examples/result_example.yaml') as f:\n",
    "    result_example = yaml.safe_load(f)\n",
    "\n",
    "pprint(eval_template)\n",
    "print('---')\n",
    "pprint(result_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1090252522.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class TestResult:\n",
    "    def __init__(self, passed, description):\n",
    "        self.passed = passed\n",
    "        self.description = description\n",
    "\n",
    "\n",
    "class Eval:\n",
    "    def __init__(\n",
    "            self,\n",
    "            uuid: str,\n",
    "            metadata: dict,\n",
    "            prompts: list[str],\n",
    "            ideal_responses: list[str],\n",
    "            tests: list[dict],):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    name: str | None = None\n",
    "    description: str | None = None\n",
    "    difficulty: int | None = None\n",
    "    tags: list[str] | None = None\n",
    "    source: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Callable, Dict\n",
    "import functools\n",
    "\n",
    "# TestResult class to encapsulate the result of each test\n",
    "class TestResult:\n",
    "    def __init__(self, passed: bool, description: str):\n",
    "        self.passed = passed\n",
    "        self.description = description\n",
    "\n",
    "# Abstract base class for tests\n",
    "class Test(ABC):\n",
    "    @abstractmethod\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        pass\n",
    "\n",
    "# Registry for test types\n",
    "test_registry = {}\n",
    "\n",
    "# Decorator to register test functions\n",
    "def register_test(test_type: str):\n",
    "    def decorator(test_func):\n",
    "        test_registry[test_type] = test_func\n",
    "        @functools.wraps(test_func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return test_func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Specific test implementations\n",
    "@register_test(\"match\")\n",
    "class MatchTest(Test):\n",
    "    def __init__(self, value: str):\n",
    "        self.value = value\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        return [TestResult(response == self.value, \"Match test\")]\n",
    "\n",
    "@register_test(\"code_blocks\")\n",
    "class CodeBlockTest(Test):\n",
    "    # Implementation for code block tests\n",
    "    ...\n",
    "\n",
    "@register_test(\"python\")\n",
    "class PythonFunctionTest(Test):\n",
    "    # Implementation for python function tests\n",
    "    ...\n",
    "\n",
    "@register_test(\"llm-similarity\")\n",
    "class LLMSimilarityTest(Test):\n",
    "    # Implementation for LLM similarity tests\n",
    "    ...\n",
    "\n",
    "# Test runner\n",
    "def run_tests(tests_config, response: str):\n",
    "    results = []\n",
    "    for test_config in tests_config:\n",
    "        test_type = test_config['type']\n",
    "        test_class = test_registry.get(test_type)\n",
    "        if test_class:\n",
    "            test = test_class(**test_config)  # Assuming other necessary parameters are passed\n",
    "            results.extend(test.run_test(response))\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"match\", \"value\": \"Expected response\"},\n",
    "    # Other test configurations\n",
    "]\n",
    "\n",
    "response = \"Some response to test\"\n",
    "test_results = run_tests(yaml_config, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import sys\n",
    "\n",
    "@register_test(\"code_blocks\")\n",
    "class PythonCodeBlockTest(Test):\n",
    "    def __init__(self, setup: str = None, checks: List[str] = None):\n",
    "        self.setup = setup\n",
    "        self.checks = checks or []\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        # Create a separate environment to run the code\n",
    "        local_env = {}\n",
    "        results = []\n",
    "\n",
    "        # Redirect stdout to capture print statements\n",
    "        stdout = io.StringIO()\n",
    "        with contextlib.redirect_stdout(stdout):\n",
    "            try:\n",
    "                # Execute setup code if present\n",
    "                if self.setup:\n",
    "                    exec(self.setup, globals(), local_env)\n",
    "\n",
    "                # Execute the main code block (response)\n",
    "                exec(response, globals(), local_env)\n",
    "\n",
    "                # Execute checks if present\n",
    "                for check in self.checks:\n",
    "                    exec(check, globals(), local_env)\n",
    "\n",
    "                # If no errors, the code block test passes\n",
    "                results.append(TestResult(True, \"Code executed without errors\"))\n",
    "            except Exception as e:\n",
    "                # If there's an error, the test fails\n",
    "                results.append(TestResult(False, f\"Error executing code: {e}\"))\n",
    "\n",
    "        # Optionally, include captured stdout in the test result\n",
    "        captured_output = stdout.getvalue()\n",
    "        if captured_output:\n",
    "            results.append(TestResult(True, f\"Captured stdout: {captured_output}\"))\n",
    "\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"code_blocks\", \"setup\": \"import math\", \"checks\": [\"assert math.sqrt(4) == 2\"]},\n",
    "]\n",
    "\n",
    "response = \"print('Hello, world!')\"\n",
    "test_results = run_tests(yaml_config, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import types\n",
    "\n",
    "\n",
    "# TODO this needs to be ran in the same environment as the code block so we need to pass in the local_env\n",
    "\n",
    "\n",
    "@register_test(\"python\")\n",
    "class PythonFunctionTest(Test):\n",
    "    def __init__(self, file: str = None, function: str = None):\n",
    "        self.file = file\n",
    "        self.function_code = function\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        test_function = self._load_function()\n",
    "        if test_function:\n",
    "            return test_function(response)\n",
    "        else:\n",
    "            return [TestResult(False, \"Failed to load test function\")]\n",
    "\n",
    "    def _load_function(self) -> Callable:\n",
    "        if self.function_code:\n",
    "            # Execute inline-defined function\n",
    "            exec(self.function_code)\n",
    "            return locals()['test_function']\n",
    "        elif self.file:\n",
    "            # Dynamically import function from a file\n",
    "            spec = importlib.util.spec_from_file_location(\"module.name\", self.file)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            return getattr(module, 'test_function')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"python\", \"function\": \"def test_function(response): return [TestResult(response == 'expected response', 'Python function test')]\"},\n",
    "]\n",
    "\n",
    "response = \"expected response\"\n",
    "test_results = run_tests(yaml_config, response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
