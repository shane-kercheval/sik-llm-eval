# llm-eval

**llm-eval** is a simple, yet flexible, framework designed for assessing the quality of responses generated by Language Model Models (LLMs). It enables evaluation of various aspects, such as the successful execution of generated code blocks and performance metrics like characters generated per second.

In this framework, there are two fundamental concepts:

- **Eval**: An Eval represents a single evaluation scenario, which can consist of one or more prompts. Multiple prompts can be used to evaluate conversations where the LLM (i.e. the underlying client) maintains a conversational history. Each Eval is associated with custom "checks" to assess different criteria. Examples of checks include verifying if the response matches an exact value, contains specific content, includes code blocks, executes those code blocks successfully, and validating the variables/functions created by those code blocks. Users can also create custom checks.

- **Candidate**: A Candidate encapsulates an underlying LLM and its corresponding client that you want to evaluate. Candidates can be identical LLM models running on different hardware, potentially resulting in similar response quality but differing in performance, like characters per second. For instance, a Candidate can represent:
    - ChatGPT 4.0 (the LLM & client/API are synonymous)
    - Llama-2-7b-Chat (LLM) running on Hugging Face Endpoints with Nvidia 10G (client)
    - Llama-2-7b-Chat Q6_K.gguf (LLM) running locally on LM Studio (client).

## Using llm-eval

### Examples

#### Running Evals/Candidates from YAML files

You can define Candidates and Evals using YAML files. Here's an example YAML file for a ChatGPT Candidate:

```yaml
metadata:
  name: OpenAI GPT-3.5-Turbo (1106)
candidate_type: OPENAI
parameters:
  model_name: gpt-3.5-turbo-1106
  system_message: You are a helpful AI assistant.
  temperature: 0.01
  max_tokens: 4096
  seed: 42
```

Here's an example of a YAML file that defines an Eval, focusing on generating a Fibonacci sequence function and corresponding assertion statements:

```yaml
metadata:
  name: Fibonacci Sequence
test_sequence:
  - prompt: Create a Python function called `fib` that takes an integer `n` and returns the `n`th number in the Fibonacci sequence. Use type hints and docstrings.
    checks:
      - check_type: REGEX
        pattern: "def fib\\([a-zA-Z_]+\\: int\\) -> int\\:"
      - check_type: PYTHON_CODE_BLOCKS_PRESENT
  - prompt: Create a set of assertion statements that test the function.
    checks:
      - check_type: CONTAINS
        value: assert fib(
      - check_type: PYTHON_CODE_BLOCKS_PRESENT
      - check_type: PYTHON_CODE_BLOCKS_RUN
        code_setup: |
          import re
        code_tests:
          - |
            def verify_mask_emails_with_no_email_returns_original_string(code_blocks: list[str]) -> bool:
                value = 'This is a string with no email addresses'
                return mask_emails(value) == value

```

The Eval above defines various types of checks, including a `PYTHON_CODE_BLOCKS_RUN` check, which executes the code blocks generated by the LLM in an isolated environment and tracks the number of code blocks that successfully execute. It also allows you to define custom tests to directly test the variables or functions created by the code blocks (in the same isolated environment).

The following code loads the Eval and Candidate from above (with the addition of a ChatGPT 4.0 Candidate and an Eval testing the creation of a "mask_emails" function). You can then run these Evals against Candidates with an `EvalHarness`:

```python
from llm_eval.eval import EvalHarness

eval_harness = EvalHarness()
eval_harness.add_eval_from_yaml('examples/evals/simple_example.yaml')
eval_harness.add_eval_from_yaml('examples/evals/mask_emails.yaml')
eval_harness.add_candidate_from_yaml('examples/candidates/openai_3.5_1106.yaml')
eval_harness.add_candidate_from_yaml('examples/candidates/openai_4.0_1106.yaml')
results = eval_harness()

print(results[0][0])
```

`results` contains a list of lists of EvalResults. Each item in the outer list corresponds to a single Candidate and contains a list of EvalResults for all Evals ran against the Candidate. In our example, `results` is [[EvalResult, EvalResult], [EvalResult, EvalResult]] where the first list corresponds to results of the Evals associated with the first Candidate (ChatGPT 3.5) and the second list corresponds to results of the Evals associated with the second Candidate (ChatGPT 4.0).

`print(results[0][0])` will give:

```
EvalResult:
    Candidate:                  OpenAI GPT-3.5-Turbo (1106)
    Eval:                       Fibonacci Sequence
    # of Prompts Tested:        2
    Cost:                       $0.0011
    Total Response Time:        14.7 seconds
    # of Response Characters:   1,336
    # of Code Blocks Generated: 2
    Characters per Second:      90.9
    # of Checks:                4
    # of Successful Checks:     4
    % of Successful Checks:     100.0%
```

Note that you can load multiple YAML files in a directory using `add_evals_from_yamls` and `add_candidates_from_yamls`:

```python
...
eval_harness.add_evals_from_yamls('examples/evals/*.yaml')
eval_harness.add_candidate_from_yamls('examples/candidates/*.yaml')
...
```

#### Running Evals/Candidates from Python objects

You can also define Candidates and Evals using Python objects already loaded into memory:

```python
candidate_chatgpt_35 = {"metadata": {"name": "OpenAI GPT-3.5-Turbo (1106)" ... }
candidate_chatgpt_40 = {"metadata": {"name": "OpenAI GPT-4.0-Turbo (1106)" ... }
eval_simple = {metadata: ..., test_sequence: ...}
eval_mask_email = {metadata: ..., test_sequence: ...}
eval_harness = EvalHarness(callback=print_result)
eval_harness.add_eval(eval_simple)
eval_harness.add_eval(eval_mask_email)
eval_harness.add_candidate(candidate_chatgpt_35)
eval_harness.add_candidate(candidate_chatgpt_40)
results = eval_harness()
print(results[0][0])
```

#### Executing a single Eval against a single Candidate

If you are interested in evaluating a specific Eval against a particular Candidate, you can create both objects and execute the Eval:

```python
candidate = OpenAICandidate(
    {'parameters': {'model_name': 'gpt-3.5-turbo-1106'}},
)
eval_obj = Eval(test_sequence={
    'prompt': "Create a python function called `fib` that takes an integer `n` and returns the `n`th number in the Fibonacci sequence. Use type hints and docstrings."
})

result = eval_obj(candidate)
print(result)
```

This minimal example does not give much insight into the quality of the response, but could still be used to compare the response time and cost of, for example, ChatGPT 3.5 vs 4.0, as well as visually compare the responses.


## Installing

The easiest way to install the llm-eval package during this beta period is by cloning the repo and installing locally.

```
cd <to the directory you want to clone the repo>
git clone https://github.com/anaconda/llm-eval.git
<activate conda or virtual environment if necessary>
pip install -e .
```

### Environment Variables

- `OPENAI_API_KEY`: This environment variable and API key are required for using OpenAIChat and OpenAICandidate.
- `HUGGING_FACE_API_KEY`: This environment variable and API key are required for using HuggingFaceEndpointChata and HuggingFaceEndpointCandidate.

## Development Environment Setup

The easiest way to setup the development environment is by creating a docker container and then connecting VS Code to the container.

You can create and start the docker container by running `make docker_run`. 

Once docker is running select the `Attach to Running Container` option within VS Code and select the appropriate container.

### Environment Variables / API Keys

To set the required environment variables for corresponding services, follow these steps:

- `HUGGING_FACE_API_KEY`: Set this environment variable to your Hugging Face API key. This is necessary for using the HuggingFaceEndpointChat. You can set it in an .env file or your environment.
- `OPENAI_API_KEY`: Set this environment variable to your OpenAI API key. It is required for using OpenAIChat. Similar to the Hugging Face API key, you can set it in an .env file or your environment.

For testing the HuggingFaceEndpointChat in llm_eval/llms/hugging_face.py (via tests/test_hugging_face.py), you can set the `HUGGING_FACE_ENDPOINT_UNIT_TESTS` environment variable to a deployed model on Hugging Face Endpoints.

With these environment variables correctly configured, you can use the corresponding services seamlessly within the llm-eval framework.
