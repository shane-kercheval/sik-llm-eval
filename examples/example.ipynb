{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EvalHarness runs evals asychronously, so we need to install nest_asyncio to avoid errors\n",
    "# running the evals in a notebook\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating OpenAI 3.5 and 4.0 against two evals\n",
    "\n",
    "This example shows how to use the EvalHarness to evaluate OpenAI 3.5 and 4.0 against two fictitious evals. The candidates and evals in this example are defined in yaml files.\n",
    "\n",
    "Eval and Candidate objects (which take individual or lists of Eval/Candidate/dict objects) can be added to the EvalHarness directly through `add_evals()` and `add_candidates()`. Evals/candidates that are defined in yaml files can be added to the EvalHarness via `add_eval_from_yaml()` which takes a string to the yaml file. Multiple Evals/Candidates defined in yaml files can be added to the EvalHarness with `add_evals_from_yamls()` and `add_candidates_from_yamls()` which take a string containing a directory which will load all yaml files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting eval_harness\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-3.5-Turbo (1106)\n",
      "    Eval:                        Fibonacci Sequence\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0008\n",
      "    Total Response Time:         9.5 seconds\n",
      "    # of Response Characters:    1,423\n",
      "    Characters per Second:       150.5\n",
      "    # of Checks:                 5\n",
      "    # of Successful Checks:      4\n",
      "    % of Successful Checks:      80.0%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 2\n",
      "    # of Code Tests Defined:     1\n",
      "    # of Successful Code Tests:  0\n",
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-3.5-Turbo (1106)\n",
      "    Eval:                        Python Function to Mask Emails\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0007\n",
      "    Total Response Time:         8.2 seconds\n",
      "    # of Response Characters:    1,538\n",
      "    Characters per Second:       188.5\n",
      "    # of Checks:                 6\n",
      "    # of Successful Checks:      5\n",
      "    % of Successful Checks:      83.3%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 1\n",
      "    # of Code Tests Defined:     2\n",
      "    # of Successful Code Tests:  2\n",
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-4.0-Turbo\n",
      "    Eval:                        Fibonacci Sequence\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0072\n",
      "    Total Response Time:         42.4 seconds\n",
      "    # of Response Characters:    3,449\n",
      "    Characters per Second:       81.3\n",
      "    # of Checks:                 5\n",
      "    # of Successful Checks:      4\n",
      "    % of Successful Checks:      80.0%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 2\n",
      "    # of Code Tests Defined:     1\n",
      "    # of Successful Code Tests:  0\n",
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-4.0-Turbo\n",
      "    Eval:                        Python Function to Mask Emails\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0102\n",
      "    Total Response Time:         83.3 seconds\n",
      "    # of Response Characters:    5,098\n",
      "    Characters per Second:       61.2\n",
      "    # of Checks:                 6\n",
      "    # of Successful Checks:      6\n",
      "    % of Successful Checks:      100.0%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 2\n",
      "    # of Code Tests Defined:     2\n",
      "    # of Successful Code Tests:  2\n",
      "Total time: 83.42689776420593\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from llm_eval.eval import EvalHarness, EvalResult\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # needed for running async in jupyter notebook\n",
    "\n",
    "def print_result(result: EvalResult) -> None:\n",
    "    \"\"\"\n",
    "    This function is used as a callback and prints the results of each evaluation.\n",
    "\n",
    "    The callback can also be used, for example, to save the results to a file. If you're\n",
    "    running a large number of evaluations, you may want to save the results to a file\n",
    "    periodically in case there are issues/errors before the entire EvalHarness completes.\n",
    "    \"\"\"\n",
    "    print(result)\n",
    "    print('---')\n",
    "\n",
    "eval_harness = EvalHarness(callback=print_result)\n",
    "eval_harness.add_eval_from_yaml(\"../examples/evals/simple_example.yaml\")\n",
    "eval_harness.add_eval_from_yaml(\"../examples/evals/mask_emails.yaml\")\n",
    "eval_harness.add_candidate_from_yaml(\"../examples/candidates/openai_3.5_1106.yaml\")\n",
    "eval_harness.add_candidate_from_yaml(\"../examples/candidates/openai_4.0_1106.yaml\")\n",
    "\n",
    "print(\"Starting eval_harness\")\n",
    "start = time.time()\n",
    "results = eval_harness()\n",
    "end = time.time()\n",
    "print(f\"Total time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code contains an example of how to summarize the eval results.\n",
    "\n",
    "The EvalHarness returns a list of lists. The outer list corresponds to each candidate and contains the eval results for that candate. So if there were 5 candidates evaluated the `results` object would be a list of 5 items (which are also lists). If there were 10 evals (evaulated against the 5 candidates) then each inner list would contain 10 `EvalResults` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for OpenAI GPT-3.5-Turbo (1106):\n",
      "  9/11 (81.8%) successful checks\n",
      "  3/4 (75.0%) successful code blocks\n",
      "Results for OpenAI GPT-4.0-Turbo:\n",
      "  10/11 (90.9%) successful checks\n",
      "  4/4 (100.0%) successful code blocks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th># checks</th>\n",
       "      <th># checks passed</th>\n",
       "      <th>% checks passed</th>\n",
       "      <th># code blocks generated</th>\n",
       "      <th># blocks successfully executed</th>\n",
       "      <th>% blocks successfully executed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI GPT-3.5-Turbo (1106)</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenAI GPT-4.0-Turbo</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  # checks  # checks passed  % checks passed  \\\n",
       "0  OpenAI GPT-3.5-Turbo (1106)        11                9         0.818182   \n",
       "1         OpenAI GPT-4.0-Turbo        11               10         0.909091   \n",
       "\n",
       "   # code blocks generated  # blocks successfully executed  \\\n",
       "0                        4                               3   \n",
       "1                        4                               4   \n",
       "\n",
       "   % blocks successfully executed  \n",
       "0                            0.75  \n",
       "1                            1.00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_summary = []\n",
    "# each outer list in results corresponds to a candidate\n",
    "for cand_obj, cand_results in zip(eval_harness.candidates, results):\n",
    "    candidate_name = cand_obj.metadata['name']\n",
    "    num_checks = sum(r.num_checks for r in cand_results)\n",
    "    num_successful_checks = sum(r.num_successful_checks for r in cand_results)\n",
    "    percent_success = num_successful_checks / num_checks\n",
    "    num_code_blocks_generated = sum(r.num_code_blocks for r in cand_results)\n",
    "    num_code_blocks_successful = sum(r.get_num_code_blocks_successful() for r in cand_results)\n",
    "    percent_code_blocks_successful = num_code_blocks_successful / num_code_blocks_generated\n",
    "    results_summary.append({\n",
    "        'name': candidate_name,\n",
    "        '# checks': num_checks,\n",
    "        '# checks passed': num_successful_checks,\n",
    "        '% checks passed': percent_success,\n",
    "        '# code blocks generated': num_code_blocks_generated,\n",
    "        '# blocks successfully executed': num_code_blocks_successful,\n",
    "        '% blocks successfully executed': percent_code_blocks_successful,\n",
    "    })\n",
    "    print(f\"Results for {candidate_name}:\")\n",
    "    print(f\"  {num_successful_checks}/{num_checks} ({percent_success:.1%}) successful checks\")\n",
    "    print(f\"  {num_code_blocks_successful}/{num_code_blocks_generated} ({percent_code_blocks_successful:.1%}) successful code blocks\")  # noqa\n",
    "\n",
    "pd.DataFrame(results_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a single Eval against a single Candidate\n",
    "\n",
    "A less common scenario, which might be useful when generating evals or debugging, is running a single Eval against a signle Candidate. Eval objects are callable and can be executed by passing a candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult:\n",
      "    # of Prompts Tested:         1\n",
      "    Cost:                       $0.0003\n",
      "    Total Response Time:         5.4 seconds\n",
      "    # of Response Characters:    759\n",
      "    Characters per Second:       141.4\n",
      "    # of Checks:                 2\n",
      "    # of Successful Checks:      2\n",
      "    % of Successful Checks:      100.0%\n",
      "    # of Code Blocks Generated:  1\n"
     ]
    }
   ],
   "source": [
    "from llm_eval.candidates import OpenAICandidate\n",
    "from llm_eval.eval import Eval\n",
    "\n",
    "candidate = OpenAICandidate({'parameters': {'model_name': 'gpt-3.5-turbo-1106'}})\n",
    "eval_obj = Eval(prompt_sequence={\n",
    "    'prompt': \"Create a python function called `mask_emails` that uses regex to mask all emails.\",\n",
    "    'checks': [\n",
    "        {'check_type': 'CONTAINS', 'value': 'def mask_emails'},\n",
    "        {'check_type': 'PYTHON_CODE_BLOCKS_PRESENT'},\n",
    "    ],\n",
    "})\n",
    "result = eval_obj(candidate)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
