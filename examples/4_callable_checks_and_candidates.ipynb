{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main use-cases of the `llm-eval` framework to allow users to define and run many Evals against many Candidates via yaml files. The Check and Candidate classes can be serialized and deserialized via `to_dict()`/`from_dict()`, and a registration system is used to instantiate the Check/Candidate objects when deserialized.\n",
    "\n",
    "However, the framework also allows users to define Checks/Candidates through code using callable objects such as lambda functions. Users can also define tests where the prompt is any type of object (and not restricted to string/dict/numeric as it is when defining Evals in yaml files) and Candidates can return any type of eval. This functionality allows users to easily define and run Evals in code (e.g. in unit tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult:\n",
      "    # of Prompts Tested:         1\n",
      "    Total Response Time:         0.0 seconds\n",
      "    # of Response Characters:    N/A\n",
      "    Characters per Second:       N/A\n",
      "    # of Checks:                 2\n",
      "    # of Successful Checks:      1\n",
      "    % of Successful Checks:      50.0%\n",
      "    # of Code Blocks Generated:  0\n"
     ]
    }
   ],
   "source": [
    "from llm_eval.eval import PromptTest, Eval\n",
    "\n",
    "def fake_candidate(prompt: dict) -> dict:\n",
    "    return {'my_response': f\"This is a fake response for the prompt: '{prompt}'.\"}\n",
    "\n",
    "test = PromptTest(\n",
    "    prompt={'my_prompt': \"This is a user's prompt.\"},\n",
    "    checks=[\n",
    "        # a DataRequest object is passed to all checks (Check or callable) from the Eval\n",
    "        lambda data: 'fake response' in data.response,\n",
    "        lambda data: len(data.code_blocks) == 0,\n",
    "    ],\n",
    ")\n",
    "eval_ = Eval(test)\n",
    "result = eval_(fake_candidate)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Evals:  2\n",
      "# of Candidates:  2\n",
      "EvalResult:\n",
      "    # of Prompts Tested:         1\n",
      "    Total Response Time:         0.0 seconds\n",
      "    # of Response Characters:    N/A\n",
      "    Characters per Second:       N/A\n",
      "    # of Checks:                 3\n",
      "    # of Successful Checks:      3\n",
      "    % of Successful Checks:      100.0%\n",
      "    # of Code Blocks Generated:  0\n",
      "EvalResult:\n",
      "    # of Prompts Tested:         1\n",
      "    Total Response Time:         0.0 seconds\n",
      "    # of Response Characters:    N/A\n",
      "    Characters per Second:       N/A\n",
      "    # of Checks:                 3\n",
      "    # of Successful Checks:      3\n",
      "    % of Successful Checks:      100.0%\n",
      "    # of Code Blocks Generated:  0\n",
      "EvalResult:\n",
      "    # of Prompts Tested:         1\n",
      "    Total Response Time:         0.0 seconds\n",
      "    # of Response Characters:    N/A\n",
      "    Characters per Second:       N/A\n",
      "    # of Checks:                 3\n",
      "    # of Successful Checks:      3\n",
      "    % of Successful Checks:      100.0%\n",
      "    # of Code Blocks Generated:  0\n",
      "EvalResult:\n",
      "    # of Prompts Tested:         1\n",
      "    Total Response Time:         0.0 seconds\n",
      "    # of Response Characters:    N/A\n",
      "    Characters per Second:       N/A\n",
      "    # of Checks:                 3\n",
      "    # of Successful Checks:      3\n",
      "    % of Successful Checks:      100.0%\n",
      "    # of Code Blocks Generated:  0\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from llm_eval.eval import PromptTest, Eval\n",
    "from llm_eval.eval import EvalHarness, EvalResult\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # needed for running async in jupyter notebook\n",
    "\n",
    "# we are not limited to strings for prompts/responses; we can use any object\n",
    "@dataclass\n",
    "class CustomRequest:\n",
    "    prompt: str\n",
    "\n",
    "@dataclass\n",
    "class CustomResponse:\n",
    "    llm_reply: str\n",
    "\n",
    "# Candidates can be any callable that takes a single value and returns a single value. Candidates\n",
    "# can delegate to other agents, APIs, etc.\n",
    "def fake_candidate_1(request: CustomRequest) -> CustomResponse:\n",
    "    return CustomResponse(llm_reply=f\"Candidate 1 fake repsonse: '{request.prompt}'.\")\n",
    "\n",
    "def fake_candidate_2(request: CustomRequest) -> CustomResponse:\n",
    "    return CustomResponse(llm_reply=f\"Candidate 2 fake response: '{request.prompt}'.\")\n",
    "\n",
    "# define Evals\n",
    "test_1 = PromptTest(\n",
    "    prompt=CustomRequest(prompt=\"This is a user's prompt for test 1.\"),\n",
    "    checks=[\n",
    "        # a DataRequest object is passed to all checks (Check or callable) from the Eval\n",
    "        lambda data: 'test 1' in data.response.llm_reply,\n",
    "        lambda data: 'test 2' not in data.response.llm_reply,\n",
    "        lambda data: len(data.code_blocks) == 0,\n",
    "    ],\n",
    ")\n",
    "test_2 = PromptTest(\n",
    "    prompt=CustomRequest(prompt=\"This is a user's prompt for test 2.\"),\n",
    "    checks=[\n",
    "        # a DataRequest object is passed to all checks (Check or callable) from the Eval\n",
    "        lambda data: 'test 2' in data.response.llm_reply,\n",
    "        lambda data: 'test 1' not in data.response.llm_reply,\n",
    "        lambda data: len(data.code_blocks) == 0,\n",
    "    ],\n",
    ")\n",
    "# run the Evals via EvalHarness\n",
    "harness = EvalHarness(\n",
    "    # note: we cannot picke lambdas (used in the checks) so in this example we are limited to\n",
    "    # a single CPU\n",
    "    num_cpus=1,\n",
    "    evals=[Eval(test_1), Eval(test_2)],\n",
    "    candidates=[fake_candidate_1, fake_candidate_2],\n",
    ")\n",
    "print(\"# of Evals: \", len(harness.evals))\n",
    "print(\"# of Candidates: \", len(harness.candidates))\n",
    "results = harness()  # run the evals\n",
    "\n",
    "for candidate_result in results:\n",
    "    for result in candidate_result:\n",
    "        print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
