{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how we can define Evals and Candidates in dictionaries and yaml files. Now we'll show how you can use a single dictionary (or yaml) to define multiple Evals and/or multiple Candidates.\n",
    "\n",
    "Here are common use-cases:\n",
    "\n",
    "- Evaluating the effectiveness of the same prompt across multiple system messages.\n",
    "- Comparing different prompts against the same set of Checks.\n",
    "- Evaluating the same LLM across multiple model parameters (e.g. temperature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Evaluating the different system messages across the same prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we've defined a similar eval to the one we saw in the previous notebook. It defines a single eval, that tests the LLMs' ability to create a python function called masks emails, and then defines specific Checks that allow us to measure success. This Eval also specifies the `system_message` that we want to use for this particular prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {\n",
    "    'metadata': {\n",
    "        'uuid': 'f1b3b3b0-0b7b-4b1b-8b1b-0b1b0b1b0b1b',\n",
    "        'name': 'Mask Emails',\n",
    "        'description': 'Evaluates the ability of a model to mask emails in text',\n",
    "    },\n",
    "    'system_message': 'You are a helpful AI assistant.',\n",
    "    'prompt_sequence': [\n",
    "        {\n",
    "            'prompt':\n",
    "                \"\"\"\n",
    "                Create a python function called `mask_emails` that takes a single string and masks all\n",
    "                emails in that string.\n",
    "\n",
    "                For each email in the format of `x@y.z`, the local part (`x`) should be masked with\n",
    "                [MASKED].\n",
    "                \"\"\",\n",
    "            'checks': [\n",
    "                {\n",
    "                    'check_type': 'REGEX',\n",
    "                    'pattern': 'def mask_emails\\\\([a-zA-Z_]+(\\\\: str)?\\\\)( -> str)?\\\\:',\n",
    "                },\n",
    "                {\n",
    "                    'check_type': 'PYTHON_CODE_BLOCK_TESTS',\n",
    "                    'code_tests': [\n",
    "                        'assert mask_emails(\"no email\") == \"no email\"',\n",
    "                        'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"',\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add this eval to the EvalHarness, we can see that the number of evals is `1`, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Evals:  1\n",
      "# of Candidates:  0\n"
     ]
    }
   ],
   "source": [
    "from llm_eval.eval import EvalHarness\n",
    "\n",
    "harness = EvalHarness()\n",
    "harness.add_evals(eval_dict)\n",
    "\n",
    "print(\"# of Evals: \", len(harness.evals))\n",
    "print(\"# of Candidates: \", len(harness.candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we set the system message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful AI assistant.\n",
      "You are a helpful AI assistant.\n"
     ]
    }
   ],
   "source": [
    "print(eval_dict['system_message'])\n",
    "print(harness.evals[0].system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can actually set multiple system messages. This will generate two separate evals that will share all other information, and will only differ by the system message. This is great for comparing the effectiveness of different system messages across the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Evals:  2\n",
      "# of Candidates:  0\n"
     ]
    }
   ],
   "source": [
    "eval_dict['system_message'] = [\n",
    "    'You are an expert python AI assistant and your goal is to generate very high quality code.',\n",
    "    'Instead of responding with the answer, write one haiku about the wisdom of the question, and another about the solution.',\n",
    "]\n",
    "\n",
    "harness = EvalHarness()  # define a new harness\n",
    "harness.add_evals(eval_dict)\n",
    "\n",
    "print(\"# of Evals: \", len(harness.evals))\n",
    "print(\"# of Candidates: \", len(harness.candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two Evals instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 1\n",
      "f1b3b3b0-0b7b-4b1b-8b1b-0b1b0b1b0b1b\n",
      "You are an expert python AI assistant and your goal is to generate very high quality code.\n",
      "Create a python function called `mask_emails` that...\n",
      "---\n",
      "Eval 2\n",
      "f1b3b3b0-0b7b-4b1b-8b1b-0b1b0b1b0b1b\n",
      "Instead of responding with the answer, write one haiku about the wisdom of the question, and another about the solution.\n",
      "Create a python function called `mask_emails` that...\n"
     ]
    }
   ],
   "source": [
    "print(\"Eval 1\")\n",
    "print(harness.evals[0].metadata['uuid'])\n",
    "print(harness.evals[0].system_message)\n",
    "print(harness.evals[0].prompt_sequence[0].prompt[0:50] + '...')\n",
    "print('---')\n",
    "print(\"Eval 2\")\n",
    "print(harness.evals[1].metadata['uuid'])\n",
    "print(harness.evals[1].system_message)\n",
    "print(harness.evals[1].prompt_sequence[0].prompt[0:50] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-3.5-Turbo (0125)\n",
      "    Eval:                        Mask Emails\n",
      "    # of Prompts Tested:         1\n",
      "    Cost:                       $0.0003\n",
      "    Total Response Time:         3.4 seconds\n",
      "    # of Response Characters:    635\n",
      "    Characters per Second:       188.1\n",
      "    # of Checks:                 2\n",
      "    # of Successful Checks:      2\n",
      "    % of Successful Checks:      100.0%\n",
      "    # of Code Blocks Generated:  1\n",
      "    # of Successful Code Blocks: 1\n",
      "    # of Code Tests Defined:     2\n",
      "    # of Successful Code Tests:  2\n",
      "---\n",
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-3.5-Turbo (0125)\n",
      "    Eval:                        Mask Emails\n",
      "    # of Prompts Tested:         1\n",
      "    Cost:                       $0.0001\n",
      "    Total Response Time:         1.6 seconds\n",
      "    # of Response Characters:    139\n",
      "    Characters per Second:       88.1\n",
      "    # of Checks:                 2\n",
      "    # of Successful Checks:      0\n",
      "    % of Successful Checks:      0.0%\n",
      "    # of Code Blocks Generated:  0\n",
      "    # of Successful Code Blocks: 0\n",
      "    # of Code Tests Defined:     2\n",
      "    # of Successful Code Tests:  0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from llm_eval.eval import EvalResult\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # needed for running async in jupyter notebook\n",
    "\n",
    "def print_callback(result: EvalResult) -> None:\n",
    "    print(result)\n",
    "    print('---')\n",
    "\n",
    "harness.add_candidates_from_yamls('candidates/openai_3.5.yaml')\n",
    "harness.callback = print_callback\n",
    "results = harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the results above that the LLM's response for the second Eval did not contain any code blocks. We can verify below by looking viewing the repsonse for each Eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is the implementation of the `mask_emails` function:\n",
       "\n",
       "```python\n",
       "import re\n",
       "\n",
       "def mask_emails(input_string):\n",
       "    def mask_email(match):\n",
       "        local_part = match.group(1)\n",
       "        return \"[MASKED]@\" + match.group(2)\n",
       "\n",
       "    email_pattern = r\"(\\S+?)@(\\S+?\\.\\S+)\"\n",
       "    masked_string = re.sub(email_pattern, mask_email, input_string)\n",
       "    \n",
       "    return masked_string\n",
       "\n",
       "# Test the function\n",
       "input_string = \"Please contact me at john.doe@example.com for further information.\"\n",
       "masked_output = mask_emails(input_string)\n",
       "print(masked_output)\n",
       "```\n",
       "\n",
       "You can use this function to mask emails in a given string by replacing the local part with `[MASKED]`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# the outer list corresponds to the candidates (we only have one candidate, ChatGPT 3.5)\n",
    "# the inner list corresponds to the evals (we have two evals in this case)\n",
    "first_eval_result = results[0][0]\n",
    "display(Markdown(first_eval_result.responses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Inquiry of code,\n",
       "Protecting emails with care,\n",
       "Wisdom in question.\n",
       "\n",
       "Emails masked with care,\n",
       "Local part hidden from view,\n",
       "Solution is clear."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "second_eval_result = results[0][1]\n",
    "display(Markdown(second_eval_result.responses[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Comparing different prompts using the same Checks\n",
    "\n",
    "The framework also allows users to specify the same prompts across \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`prompt_comparison`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {\n",
    "    'metadata': {\n",
    "        'name': 'Mask Emails',\n",
    "    },\n",
    "    'system_message': 'You are a helpful AI assistant.',\n",
    "    'prompt_comparison': {\n",
    "        # optional 'parameters' to share between prompts\n",
    "        'prompt_parameters' : {\n",
    "            'few_shot_examples': \"\"\"\n",
    "                Few shot example 1: X\n",
    "                Few shot example 2: Y\n",
    "                Few shot example 3: Z\n",
    "                \"\"\",\n",
    "        },\n",
    "        'prompts': [\n",
    "            \"\"\"\n",
    "            Prompt A\n",
    "\n",
    "            {few_shot_examples}\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            {few_shot_examples}\n",
    "\n",
    "            Prompt B\n",
    "            \"\"\",\n",
    "        ],\n",
    "        'checks': [\n",
    "            {\n",
    "                'check_type': 'REGEX',\n",
    "                'pattern': 'def mask_emails\\\\([a-zA-Z_]+(\\\\: str)?\\\\)( -> str)?\\\\:',\n",
    "            },\n",
    "            {\n",
    "                'check_type': 'PYTHON_CODE_BLOCK_TESTS',\n",
    "                'code_tests': [\n",
    "                    'assert mask_emails(\"no email\") == \"no email\"',\n",
    "                    'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"',\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Evals:  2\n",
      "# of Candidates:  0\n",
      "---\n",
      "Eval 1\n",
      "Mask Emails\n",
      "Prompt A\n",
      "\n",
      "\n",
      "    Few shot example 1: X\n",
      "    Few shot ...\n",
      "RegexCheck(pattern='def mask_emails\\([a-zA-Z_]+(\\: str)?\\)( -> str)?\\:', metadata={})\n",
      "---\n",
      "Eval 2\n",
      "Mask Emails\n",
      "You are a helpful AI assistant.\n",
      "Few shot example 1: X\n",
      "    Few shot example 2: Y\n",
      "  ...\n",
      "RegexCheck(pattern='def mask_emails\\([a-zA-Z_]+(\\: str)?\\)( -> str)?\\:', metadata={})\n"
     ]
    }
   ],
   "source": [
    "harness = EvalHarness()  # define a new harness\n",
    "harness.add_evals(eval_dict)\n",
    "\n",
    "print(\"# of Evals: \", len(harness.evals))\n",
    "print(\"# of Candidates: \", len(harness.candidates))\n",
    "print('---')\n",
    "print(\"Eval 1\")\n",
    "print(harness.evals[0].metadata['name'])\n",
    "print(harness.evals[0].prompt_sequence[0].prompt[0:50] + '...')\n",
    "print(harness.evals[0].prompt_sequence[0].checks[0])\n",
    "print('---')\n",
    "print(\"Eval 2\")\n",
    "print(harness.evals[1].metadata['name'])\n",
    "print(harness.evals[1].system_message)\n",
    "print(harness.evals[1].prompt_sequence[0].prompt[0:50] + '...')\n",
    "print(harness.evals[1].prompt_sequence[0].checks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Evals:  4\n"
     ]
    }
   ],
   "source": [
    "eval_dict['system_message'] = [\n",
    "    'System Message 1',\n",
    "    'System Message 2',\n",
    "]\n",
    "harness = EvalHarness()  # define a new harness\n",
    "harness.add_evals(eval_dict)\n",
    "\n",
    "print(\"# of Evals: \", len(harness.evals))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
