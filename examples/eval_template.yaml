metadata:
  uuid: 5f8b1b4e-3b7e-4b0e-8b1b-4e3b7e4b0e8b
  name: Eval Example  # do we need this?
  description: This is an example of an eval. An eval contains a set of prompts and tests.
  difficulty: 1  # 1-5
  tags:
    - example
    - graphing
    - plotly-express
  source: Anaconda
prompts:
  - value: This is a question/prompt.
    ideal_response: |
      This is the ideal response (to the first prompt).
      This field is optional.
      It is only used if a test below is of type `llm-similarity`, in which case it is used as the ideal response and the LLM is asked to evaluate the similarity of the response to the ideal answer.
      This value could also be used to fine-tune a model.
  - value: |
      This is a followup prompt.
      The intent is to evaluate multiple responses in a conversation, rather than a single response.
    ideal_response: This is the ideal response to the second prompt.
# optional; default tests (e.g. % of code blocks passing) along with performance metrics
# (characters per second) will still be recorded
tests:
  # This is a test that simply checks if the response matches the value exactly.
  - type: match
    value: This is the expected response. It must match exactly.
    description: This is an optional description of the test that explains what the test is checking or why the provided answer is correct.
    tags:
      - multiple-choice
  # This is a test that checks that any code blocks in the response run without error.
  - type: python_code_blocks
    # optional; some prompts/responses assume that imports/functions are already defined so this
    # this code is run before the code blocks returned in the response.
    setup: |
      import os
      import sys
      from example_test import test_function
    # optional; if provided, this code runs after all of the code blocks in the response, and
    # a) is passed the code blocks and can either runs checks against the code blocks or b) run
    # checks in the python the environment (the code has access to all global variables defined
    # in the code blocks) (e.g. variables, functions, etc. that are created) for the expected
    # values.
    check: |
        def check(code_blocks: list[str]) -> list[TestResult]:
          return True
  # This is an test that uses a python function to evaluate the response.
  # The function is defined in `example_test.py` and is called `test_function`.
  # The function must take a single argument, the response, and return a list of TestResult objects.
  - type: python
    file: example_test.py
    function: test_function
  # This is an test that uses a python function to evaluate the response.
  # The function is defined directly in the yaml file. It can be named anything but must take a
  # single argument, the response, and return a list of TestResult objects. 
  - type: python
    function: |
      def test_function(response: str) -> list[TestResult]:
        expectation_1 = TestResult(passed=..., description='This is a description of the expecation.')
        expectation_2 = TestResult(passed=..., description='This is a description of the expecation.')
        return [expectation_1, expectation_2]
  
  # This type of test is used to evaluate the similarity of the response to the ideal answer.
  # The ideal answer should be captured in the `ideal_response` field, above.
  # If the ideal response is not provided, the test harness will fail.

  # ****TODO****: this type of test won't return True/False so we need to figure out how to handle this.
  # maybe have a result_type (Metric?) that is either boolean or scalar? not sure.
  # input would be the response and the ideal response
  - type: llm
    model: gpt-3.5-turbo-1106
    instruction: Compare the response to the ideal response. Give a score between 0 and 1, where 0 is not similar at all and 1 is very similar.
