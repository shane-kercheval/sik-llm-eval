{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set path to the root directory of the project\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir('..')\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Objects\n",
    "\n",
    "This section shows how to use Check objects directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexCheck Example\n",
    "\n",
    "A RegexCheck checks whether or not the value passed to the check matches the regex pattern provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': True,\n",
       " 'success': True,\n",
       " 'metadata': {'check_type': 'REGEX',\n",
       "  'check_pattern': '\\\\b[a-z]+@[a-z]+\\\\.[a-z]+\\\\b',\n",
       "  'check_negate': False,\n",
       "  'check_metadata': {}},\n",
       " 'result_type': 'PASS_FAIL'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llm_eval.checks import RegexCheck\n",
    "\n",
    "check = RegexCheck(pattern=r\"\\b[a-z]+@[a-z]+\\.[a-z]+\\b\")\n",
    "result = check(\"This is an email john@doe.com, the check should succeed.\")\n",
    "assert result.success is True\n",
    "result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1Score Example\n",
    "\n",
    "The F1Score is measure of overlap between the words in the ideal response and the words in the actual response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score is `0.8`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'value': 0.8,\n",
       " 'metadata': {'check_type': 'F1_SCORE', 'check_metadata': {}},\n",
       " 'result_type': 'SCORE',\n",
       " 'success': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llm_eval.checks import F1Score\n",
    "\n",
    "score = F1Score()\n",
    "result = score(\n",
    "    actual_response=\"This is the correct answer.\",\n",
    "    ideal_response=\"A correct answer was given.\",\n",
    ")\n",
    "print(f\"The F1 score is `{result.value}`.\")\n",
    "result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals\n",
    "\n",
    "In the examples above, we called the Check objects directly, passing in the values to check (i.e. the responses from the LLM/agent).\n",
    "\n",
    "Typically, you'll want to create an Eval that encapsulates a particular test case and a collection of one or more checks.\n",
    "\n",
    "Below, we will define an \"Eval\" which runs all of the checks that are defined on the Eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval': {'checks': [{'check_type': 'REGEX',\n",
       "    'pattern': '\\\\b[a-z]+@[a-z]+\\\\.[a-z]+\\\\b'},\n",
       "   {'check_type': 'F1_SCORE', 'success_threshold': 0.5}],\n",
       "  'ideal_response': 'This is an exmaple of the ideal response which contains the email jane@doe.com.'},\n",
       " 'candidate': None,\n",
       " 'response': 'This is an another email john@doe.com.',\n",
       " 'metadata': None,\n",
       " 'timestamp': '2024-12-11T19:45:54.718711+00:00',\n",
       " 'check_results': [{'value': True,\n",
       "   'success': True,\n",
       "   'metadata': {'check_type': 'REGEX',\n",
       "    'check_pattern': '\\\\b[a-z]+@[a-z]+\\\\.[a-z]+\\\\b',\n",
       "    'check_negate': False,\n",
       "    'check_metadata': {}},\n",
       "   'result_type': 'PASS_FAIL'},\n",
       "  {'value': 0.2222222222222222,\n",
       "   'success': False,\n",
       "   'metadata': {'check_type': 'F1_SCORE', 'check_metadata': {}},\n",
       "   'success_threshold': 0.5,\n",
       "   'result_type': 'SCORE'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llm_eval.eval import Eval\n",
    "from sik_llm_eval.checks import RegexCheck, F1Score\n",
    "\n",
    "evaluation = Eval(\n",
    "    ideal_response=\"This is an exmaple of the ideal response which contains the email jane@doe.com.\",\n",
    "    checks=[\n",
    "        RegexCheck(pattern=r\"\\b[a-z]+@[a-z]+\\.[a-z]+\\b\"),\n",
    "        F1Score(success_threshold=0.5),\n",
    "    ],\n",
    ")\n",
    "fake_response = \"This is an another email john@doe.com.\"\n",
    "result = evaluation(fake_response)\n",
    "result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the `ideal_response` is associated with a particular Eval so that multiple checks can use it.\n",
    "\n",
    "When using Eval objects, the actual `response`, `ideal_response`, and `input` (`input` is not shown above, but can also be attached to an eval) are sent to Check objects by the Eval. Each check knows which values to use (for example, the F1Score knew where to pull the actual response and ideal response from).\n",
    "\n",
    "We'll see below that we can modify where the Checks look for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Response\n",
    "\n",
    "In the example below, we see an hypothetical response from a RAG agent that first extracts the most relevant document, generates a response based on that document, and then returns the response, the document id that the agent used to generate the response, and some additional metadata that we may or may not use in the Eval.\n",
    "\n",
    "Here, we want the Checks to use the `generated_response` in the dictionary as the response in the RegexCheck and F1Score, and the `document_id` in the MatchCheck to check that the agent used the expected document based on this particular Eval.\n",
    "\n",
    "In order to tell the Check objects where to extract the data that's passed to them by the Eval object, we'll set the `data_path` parameter in the Check's `__init__` function. This parameter can be set to a string, a list, or a dictionary, and indicates the \"path\" to the data we want to extract, relative to the `response`, `input`, `ideal_response`, or `metadata`.\n",
    "\n",
    "So, for example, the `response` variable sent to the Check from the Eval will contain the dictionary defined in the next cell. We want the RegexCheck and F1Score to use the value corresponding to the `generated_response` key in that dictionary. We also want the MatchCheck to match against corresponding to the `document_id`. When passing a string to `data_path`, a single value will be extracted. When passing a dictionary, like we do with F1Score, the keys will correspond to the parameter names in the corresponding `__call__` function; in this case, it will be the `actual_response` and `ideal_response` of the F1Score `__call__` function. The values are the paths to the data to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses can be returned by LLMs and agents in different formats.\n",
    "fake_agent_response = {\n",
    "    'generated_response': \"This is an example response containing email john@doe.com.\",\n",
    "    # For example, `document_id` could be returned by an agent using RAG to indicate the document\n",
    "    # that was used to generate the response.\n",
    "    # This information can be tested by the Eval to check if the correct document was used.\n",
    "    'document_id': 'doc_123',\n",
    "    'example_metadata': {\n",
    "        'cost': 0.5,\n",
    "        'num_tokens': 100,\n",
    "        'foo': 'bar',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the response contain an email (RegexCheck)? True\n",
      "Does the document_id match the expected value (MatchCheck)? True\n",
      "Is the F1 score greater than the threshold of `0.5`? False\n",
      "The F1 score is `0.36`.\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval': {'checks': [{'check_type': 'REGEX',\n",
       "    'data_path': \"response['generated_response']\",\n",
       "    'pattern': '\\\\b[a-z]+@[a-z]+\\\\.[a-z]+\\\\b'},\n",
       "   {'check_type': 'MATCH',\n",
       "    'data_path': \"response['document_id']\",\n",
       "    'value': 'doc_123'},\n",
       "   {'check_type': 'F1_SCORE',\n",
       "    'data_path': {'actual_response': \"response['generated_response']\",\n",
       "     'ideal_response': 'ideal_response'},\n",
       "    'success_threshold': 0.5}],\n",
       "  'ideal_response': 'This is an exmaple of the ideal response which contains the email jane@doe.com.'},\n",
       " 'candidate': None,\n",
       " 'response': {'generated_response': 'This is an example response containing email john@doe.com.',\n",
       "  'document_id': 'doc_123',\n",
       "  'example_metadata': {'cost': 0.5, 'num_tokens': 100, 'foo': 'bar'}},\n",
       " 'metadata': None,\n",
       " 'timestamp': '2024-12-11T19:45:54.738828+00:00',\n",
       " 'check_results': [{'value': True,\n",
       "   'success': True,\n",
       "   'metadata': {'check_type': 'REGEX',\n",
       "    'check_pattern': '\\\\b[a-z]+@[a-z]+\\\\.[a-z]+\\\\b',\n",
       "    'check_negate': False,\n",
       "    'check_metadata': {},\n",
       "    'data_path': \"response['generated_response']\",\n",
       "    'value_extracted': 'This is an example response containing email john@doe.com.'},\n",
       "   'result_type': 'PASS_FAIL'},\n",
       "  {'value': True,\n",
       "   'success': True,\n",
       "   'metadata': {'check_type': 'MATCH',\n",
       "    'check_value': 'doc_123',\n",
       "    'check_negate': False,\n",
       "    'check_metadata': {},\n",
       "    'data_path': \"response['document_id']\",\n",
       "    'value_extracted': 'doc_123'},\n",
       "   'result_type': 'PASS_FAIL'},\n",
       "  {'value': 0.3636363636363636,\n",
       "   'success': False,\n",
       "   'metadata': {'check_type': 'F1_SCORE',\n",
       "    'check_metadata': {},\n",
       "    'data_path': {'actual_response': \"response['generated_response']\",\n",
       "     'ideal_response': 'ideal_response'},\n",
       "    'value_extracted': {'actual_response': 'This is an example response containing email john@doe.com.',\n",
       "     'ideal_response': 'This is an exmaple of the ideal response which contains the email jane@doe.com.'}},\n",
       "   'success_threshold': 0.5,\n",
       "   'result_type': 'SCORE'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llm_eval.checks import RegexCheck, MatchCheck, F1Score\n",
    "from sik_llm_eval.eval import Eval\n",
    "\n",
    "evaluation = Eval(\n",
    "    ideal_response=\"This is an exmaple of the ideal response which contains the email jane@doe.com.\",\n",
    "    checks=[\n",
    "        RegexCheck(\n",
    "            data_path=\"response['generated_response']\",\n",
    "            pattern=r\"\\b[a-z]+@[a-z]+\\.[a-z]+\\b\",\n",
    "        ),\n",
    "        MatchCheck(\n",
    "            data_path=\"response['document_id']\",\n",
    "            value=\"doc_123\",\n",
    "        ),\n",
    "        F1Score(\n",
    "            data_path={\n",
    "                # The keys of the dictionary `actual_response` and `ideal_response` correspond to\n",
    "                # the arguments of the F1Score's __call__ method\n",
    "                # The values of the dictionary, `response` and `ideal_response` are properties on\n",
    "                # the ResponseModel object that is passed to the Check from the Eval object.\n",
    "                'actual_response': \"response['generated_response']\",\n",
    "                'ideal_response': \"ideal_response\",\n",
    "            },\n",
    "            success_threshold=0.5,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = evaluation(fake_agent_response)\n",
    "print(f\"Does the response contain an email (RegexCheck)? {result.check_results[0].success}\")\n",
    "print(f\"Does the document_id match the expected value (MatchCheck)? {result.check_results[1].success}\")\n",
    "print(f\"Is the F1 score greater than the threshold of `{result.check_results[2].success_threshold}`? {result.check_results[2].success}\")\n",
    "print(f\"The F1 score is `{result.check_results[2].value:.2f}`.\")\n",
    "print(\"\\n---\\n\")\n",
    "result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Multiple Evals and \"Candidates\" w/ EvalHarness\n",
    "\n",
    "In the examples above, we only evaluated a single Eval against a single LLM or agent.\n",
    "\n",
    "But typically, we'd like to evaluate many Evals against one or more LLMs/agents and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()  # needed for running async in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sik_llm_eval.checks import RegexCheck\n",
    "from sik_llm_eval.eval import Eval\n",
    "\n",
    "eval_a = Eval(\n",
    "    metadata={'id': 'shooter'},\n",
    "    input=\"In a single sentence, who is the greatest basketball shooter of all time?\",\n",
    "    checks=[RegexCheck(pattern=r\"Steph(en)?\\sCurry\")],\n",
    ")\n",
    "eval_b = Eval(\n",
    "    metadata={'id': 'goat'},\n",
    "    input=\"In a single sentence, who is the GOAT of basketball?\",\n",
    "    checks=[RegexCheck(pattern=r\"Michael\\s(Jeffrey\\s)?Jordan\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder from above, this is what it would like look if we manually passed in a response from a single model to a single Eval. \n",
    "\n",
    "We'll use `OpenAICompletion` which is a simple wrapper around OpenAI which makes the input/output of the model easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many consider Stephen Curry to be the greatest basketball shooter of all time due to his exceptional shooting accuracy, range, and impact on the game.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from sik_llm_eval.openai import OpenAICompletion, user_message\n",
    "\n",
    "client = OpenAI()\n",
    "model = OpenAICompletion(\n",
    "    client=client,\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.1,\n",
    ")\n",
    "messages = [user_message(eval_a.input)]\n",
    "response = model(messages=messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct response? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval': {'metadata': {'id': 'shooter'},\n",
       "  'input': 'In a single sentence, who is the greatest basketball shooter of all time?',\n",
       "  'checks': [{'check_type': 'REGEX', 'pattern': 'Steph(en)?\\\\sCurry'}]},\n",
       " 'candidate': None,\n",
       " 'response': 'Many consider Stephen Curry to be the greatest basketball shooter of all time due to his exceptional shooting accuracy, range, and impact on the game.',\n",
       " 'metadata': None,\n",
       " 'timestamp': '2024-12-11T19:45:55.631037+00:00',\n",
       " 'check_results': [{'value': True,\n",
       "   'success': True,\n",
       "   'metadata': {'check_type': 'REGEX',\n",
       "    'check_pattern': 'Steph(en)?\\\\sCurry',\n",
       "    'check_negate': False,\n",
       "    'check_metadata': {}},\n",
       "   'result_type': 'PASS_FAIL'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = eval_a(response.content)\n",
    "print(f\"Correct response? {result.check_results[0].success}\")\n",
    "result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we want to run multiple Evals against multiple LLMs (or agents, or variations of LLM settings such as temperature), we can use the `EvalHarness`.\n",
    "\n",
    "However, we have one slight problem: different models/APIs/agents/etc will expect different data structures for both inputs and outputs. So how can we ensure that the `input` defined on the Eval object matches the inputs expected by the models, and that the outputs generated by the models are in the correct format that our Evals/Checks expect?\n",
    "\n",
    "We can use a `Candidate` object, which is just a light-weight wrapper around the model that defines a particular interface.\n",
    "\n",
    "User's can create custom Candidate objects for their own APIs/agents, or they can use built-in candidates. For example, there is an OpenAICandidate that can be used to evaluate OpenAI against Evals.\n",
    "\n",
    "However, let's create our own for demonstration purposes. A candidate can be a Candidate object, or it can simply be a callable object that takes the input from the Eval and returns a CandidateResponse object.\n",
    "\n",
    "Additionally, we can pass metadata to the CandidateResponse so that we can track, for example, token usage and costs associated with the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sik_llm_eval.eval.CandidateRunResults at 0x1173dfad0>,\n",
       " <sik_llm_eval.eval.CandidateRunResults at 0x111b93a90>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sik_llm_eval.candidates import CandidateResponse\n",
    "from sik_llm_eval.eval import EvalHarness\n",
    "\n",
    "class MyCandidate:\n",
    "    def __init__(self, temperature: float):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, input: str) -> CandidateResponse:\n",
    "        model = OpenAICompletion(\n",
    "            client=OpenAI(),\n",
    "            model='gpt-4o-mini',\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        response = model(messages=[user_message(input)])\n",
    "        return CandidateResponse(\n",
    "            response=response.content,\n",
    "            metadata=response.usage,\n",
    "        )\n",
    "\n",
    "harness = EvalHarness(\n",
    "    evals=[eval_a, eval_b],\n",
    "    candidates=[\n",
    "        MyCandidate(temperature=0.1),\n",
    "        MyCandidate(temperature=1.0),\n",
    "    ],\n",
    "    \n",
    ")\n",
    "results = harness()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval': {'metadata': {'id': 'shooter'},\n",
       "  'input': 'In a single sentence, who is the greatest basketball shooter of all time?',\n",
       "  'checks': [{'check_type': 'REGEX', 'pattern': 'Steph(en)?\\\\sCurry'}]},\n",
       " 'candidate': '<__main__.MyCandidate object at 0x1174d0450>',\n",
       " 'response': 'Many consider Stephen Curry to be the greatest basketball shooter of all time due to his unparalleled shooting accuracy and ability to make shots from long distances.',\n",
       " 'metadata': {'response_metadata': {'completion_tokens': 28,\n",
       "   'prompt_tokens': 22,\n",
       "   'total_tokens': 50,\n",
       "   'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "   'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'accepted_prediction_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0}},\n",
       "  'response_timestamp': '2024-12-11T19:45:56.468490+00:00'},\n",
       " 'timestamp': '2024-12-11T19:45:59.306845+00:00',\n",
       " 'check_results': [{'value': True,\n",
       "   'success': True,\n",
       "   'metadata': {'check_type': 'REGEX',\n",
       "    'check_pattern': 'Steph(en)?\\\\sCurry',\n",
       "    'check_negate': False,\n",
       "    'check_metadata': {}},\n",
       "   'result_type': 'PASS_FAIL'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first candidate, first eval\n",
    "results[0].eval_results[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate (temp=0.1) Success rate: 100.0% (2/2)\n",
      "Candidate (temp=1.0) Success rate: 100.0% (2/2)\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    eval_results = r.eval_results  # list of EvalResult objects\n",
    "    num_evals = len(eval_results)\n",
    "    num_successes = sum([er.check_results[0].success for er in eval_results])\n",
    "    success_rate = num_successes / num_evals\n",
    "    print(f\"Candidate (temp={r.candidate.temperature}) Success rate: {success_rate:.1%} ({num_successes}/{num_evals})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating OpenAI `4.0` and `4o-mini` against two evals\n",
    "\n",
    "In this example, we'll show how to use a built-in Candidate object, the OpenAICandidate, to evaluate `ChatGPT 4.0` against `4o-mini`.\n",
    "\n",
    "We'll also show how to load evals and candidates from yaml files. Loading candidates from yaml (or from json, which is also supported), requires the Candidate object to use the `@Candidate.register(<candidate name>)` decorator, which tells the Candidate class which sub-class to instantiate when loading from yaml/json or an in-memory python dictionary.\n",
    "\n",
    "Please refer to the `examples/evals` and `examples/candidates` folder to view the underlying yaml files.\n",
    "\n",
    "The Evals that are loading are basic python generation prompts (e.g. \"Create a python function that does X\") along with checks that validate the underlying python code is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we also tell the EvalHarness to generate the responses (from the `input` of the `Eval` object) asynchronously, and to run the Evals (i.e. the underlying Checks) using multiprocessing. Running the Checks in parallel is useful for checks that are compute intensive (e.g. extracting and executing Python Code blocks via `PythonCodeBlockTests`, cleaning/tokenizing text via `F1Score`) The asynchronous batch size and number of cpus can be set in the EvalHarness `__init__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Evals:  3\n",
      "# of Candidates:  2\n",
      "Starting eval_harness\n",
      "Total time: 3.790855407714844e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sik_llm_eval.eval import EvalHarness, Mode\n",
    "\n",
    "harness = EvalHarness(\n",
    "    response_mode=Mode.ASYNC,\n",
    "    eval_mode=Mode.PARALLEL,\n",
    ")\n",
    "harness.add_evals_from_files('examples/evals/*.yaml')\n",
    "harness.add_candidate_from_file('examples/candidates/openai_4.0.yaml')\n",
    "harness.add_candidate_from_file('examples/candidates/openai_4o-mini.yaml')\n",
    "\n",
    "print(\"# of Evals: \", len(harness.evals))\n",
    "print(\"# of Candidates: \", len(harness.candidates))\n",
    "\n",
    "print(\"Starting eval_harness\")\n",
    "start = time.time()\n",
    "results = harness()  # run the evals\n",
    "end = time.time()\n",
    "print(f\"Total time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a dictionary representation of the third Eval Result (`[2]`) from the first candidate (`[0]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_asyncio.Task' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39meval_results[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "\u001b[0;31mTypeError\u001b[0m: '_asyncio.Task' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "results[0].eval_results[2].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results[0].eval_results[2]\n",
    "duration = result.metadata['response_metadata']['duration_seconds']\n",
    "print(f\"Duration of 3rd Eval for 1st Candidate: {duration:.2f} seconds\")\n",
    "print(f\"Total Runtime of EvalHarness: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see the benefit of running response generation asynchronously. The time it took ChatGPT 4.0 to generate a response for the 3rd Eval was just under the time it took to run all 6 Evaluations (response generation and check evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code contains an example of how to summarize the eval results.\n",
    "\n",
    "The EvalHarness returns a list of lists. The outer list corresponds to each candidate and contains the eval results for that candate. So if there were 5 candidates evaluated the `results` object would be a list of 5 items (which are also lists). If there were 10 evals (evaulated against the 5 candidates) then each inner list would contain 10 `EvalResults` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_summary = []\n",
    "# each outer list in results corresponds to a candidate\n",
    "for cand_results in results:\n",
    "    candidate_name = cand_results.candidate.metadata['name']\n",
    "    eval_results = cand_results.eval_results\n",
    "    num_characters = sum(len(r.response) for r in eval_results)\n",
    "    response_duration = sum(r.metadata['response_metadata']['duration_seconds'] for r in eval_results)\n",
    "    avg_chars_per_second = num_characters / response_duration\n",
    "    avg_cost = sum(r.metadata['response_metadata']['total_cost'] for r in eval_results) / len(eval_results)\n",
    "    num_checks = sum(len(r.check_results) for r in eval_results)\n",
    "    num_successful_checks = sum(r.num_successful_checks for r in eval_results)\n",
    "    percent_success = num_successful_checks / num_checks\n",
    "    results_summary.append({\n",
    "        'name': candidate_name,\n",
    "        'Avg chars per second': avg_chars_per_second,\n",
    "        'Avg cost': avg_cost,\n",
    "        '# checks': num_checks,\n",
    "        '# checks passed': num_successful_checks,\n",
    "        '% checks passed': percent_success,\n",
    "    })\n",
    "    print(f\"Results for {candidate_name}:\")\n",
    "    print(f\"  {num_successful_checks}/{num_checks} ({percent_success:.1%}) successful checks\")\n",
    "\n",
    "pd.DataFrame(results_summary).style.format({\n",
    "    'Avg chars per second': '{:.1f}',\n",
    "    'Avg cost': '{:.4f}',\n",
    "    '% checks passed': '{:.1%}',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
