{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Eval is a way to define a test scenario of one or more prompts. (Multiple (sequential) prompts can be used to evaluate conversations where the LLM (i.e. the underlying client) maintains a conversational history.) The response of the LLM is evaluated based on the \"checks\" that are defined for that Eval. Evals can be defined with a dictionary/yaml, or with llm_eval python classes.\n",
    "\n",
    "Here's a simple example where we want to test the LLM's ability to create a python function that we've described. (A more detailed version of this Eval can be found in `./evals/mask_emails.yaml`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a single Eval and single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llm_eval.eval import Eval, PromptTest\n",
    "from llm_eval.checks import RegexCheck, PythonCodeBlockTests\n",
    "\n",
    "eval_obj = Eval(\n",
    "    metadata={\n",
    "        # metadata is a dictionary that can contain any key-value string pairs\n",
    "        'name': 'Mask Emails',\n",
    "        'description': 'Evaluates the ability of a model to mask emails in text',\n",
    "    },\n",
    "    prompt_sequence=[\n",
    "        PromptTest(\n",
    "            prompt=\"\"\"\n",
    "            Create a python function called `mask_emails` that takes a single string and masks all\n",
    "            emails in that string.\n",
    "\n",
    "            For each email in the format of `x@y.z`, the local part (`x`) should be masked with\n",
    "            [MASKED].\n",
    "            \"\"\",\n",
    "            checks=[\n",
    "                RegexCheck(pattern=r'def mask_emails\\([a-zA-Z_]+(\\: str)?\\)( -> str)?\\:'),\n",
    "                PythonCodeBlockTests(\n",
    "                    code_tests=[\n",
    "                        'assert mask_emails(\"no email\") == \"no email\"',\n",
    "                        'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"',\n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PythonCodeBlockTests` check object extracts all of the code blocks that were generated by the LLM and runs the code in the background in an isolated environment. It tracks the number of code blocks that were generated and the number of code blocks that successfully executed. The object also has a set of `code_test` where the user can write python code (either single statements (assertion or statements that resolve to booleans) or functions (which return boolean values)). The code that is defined in these `code_tests` is run in the same isolated enviornment and can test any function, variable, or class that is created from code generated by the LLM.\n",
    "\n",
    "See `./evals/mask_emails.yaml` for additional examples, and the documentation for `PythonCodeBlockTests` in `llm_eval/checks.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eval can also be defined as a dictionary (or, for example, loaded from a yaml file into a dictionary) as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {\n",
    "    'metadata': {\n",
    "        'name': 'Mask Emails',\n",
    "        'description': 'Evaluates the ability of a model to mask emails in text',\n",
    "    },\n",
    "    'prompt_sequence': [\n",
    "        {\n",
    "            'prompt':\n",
    "                \"\"\"\n",
    "                Create a python function called `mask_emails` that takes a single string and masks all\n",
    "                emails in that string.\n",
    "\n",
    "                For each email in the format of `x@y.z`, the local part (`x`) should be masked with\n",
    "                [MASKED].\n",
    "                \"\"\",\n",
    "            'checks': [\n",
    "                {\n",
    "                    'check_type': 'REGEX',\n",
    "                    'pattern': 'def mask_emails\\\\([a-zA-Z_]+(\\\\: str)?\\\\)( -> str)?\\\\:',\n",
    "                },\n",
    "                {\n",
    "                    'check_type': 'PYTHON_CODE_BLOCK_TESTS',\n",
    "                    'code_tests': [\n",
    "                        'assert mask_emails(\"no email\") == \"no email\"',\n",
    "                        'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"',\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Eval above, we can define checks with dictionaries using the `check_type` key and values like `REGEX` and `PYTHON_CODE_BLOCK_TESTS`, which are \"registered\" so the classes can be instantiated in real time. Built-in registration types can be found in the `CheckType` enum.\n",
    "\n",
    "For example, here is the class definition for `RegexCheck`\n",
    "\n",
    "```\n",
    "@Check.register(CheckType.REGEX)\n",
    "class RegexCheck(Check):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Users can create their own custom checks and use the same registration system to register their checks, for example:\n",
    "\n",
    "```\n",
    "@Check.register('my-custom-check')\n",
    "class CustomXYZCheck(Check):\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval objects can be created directly from dictionaries, as shown below. However, the most common pattern is to define Evals as yaml files, and load many evals and run them with the `EvalHarness`, which will be described in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'name': 'Mask Emails',\n",
       "  'description': 'Evaluates the ability of a model to mask emails in text'},\n",
       " 'prompt_sequence': [{'prompt': 'Create a python function called `mask_emails` that takes a single string and masks all\\nemails in that string.\\n\\nFor each email in the format of `x@y.z`, the local part (`x`) should be masked with\\n[MASKED].\\n',\n",
       "   'checks': [{'pattern': 'def mask_emails\\\\([a-zA-Z_]+(\\\\: str)?\\\\)( -> str)?\\\\:',\n",
       "     'check_type': 'REGEX'},\n",
       "    {'code_tests': ['assert mask_emails(\"no email\") == \"no email\"',\n",
       "      'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"'],\n",
       "     'check_type': 'PYTHON_CODE_BLOCK_TESTS'}]}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eval(**eval_dict).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple (sequential prompts)\n",
    "\n",
    "Multiple (sequential) prompts can be used to evaluate conversations where the LLM (i.e. the underlying client) maintains a conversational history.\n",
    "\n",
    "For example, if we're testing the ability of an LLM to generate a function that masks emails, a logical next step (that someone using the LLM for a similar use case) is to create unit tests or assertion statements that test the accuracy of the function.\n",
    "\n",
    "Let's modify our original Eval to define another prompt and set of checks that includes this followup request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the modification to the Eval defined with classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_eval.eval import Eval, PromptTest\n",
    "from llm_eval.checks import MatchCheck, RegexCheck, PythonCodeBlocksPresent, PythonCodeBlockTests\n",
    "\n",
    "eval_obj = Eval(\n",
    "    metadata={\n",
    "        # metadata is a dictionary that can contain any key-value string pairs\n",
    "        'name': 'Mask Emails',\n",
    "        'description': 'Evaluates the ability of a model to mask emails in text',\n",
    "    },\n",
    "    prompt_sequence=[\n",
    "        PromptTest(\n",
    "            prompt=\"\"\"\n",
    "            Create a python function called `mask_emails` that takes a single string and masks all\n",
    "            emails in that string.\n",
    "\n",
    "            For each email in the format of `x@y.z`, the local part (`x`) should be masked with\n",
    "            [MASKED].\n",
    "            \"\"\",\n",
    "            checks=[\n",
    "                RegexCheck(pattern=r'def mask_emails\\([a-zA-Z_]+(\\: str)?\\)( -> str)?\\:'),\n",
    "            ],\n",
    "        ),\n",
    "        PromptTest(\n",
    "            prompt=\"Create a set of assertion statements that test the function.\",\n",
    "            checks=[\n",
    "                MatchCheck(value=\"assert \"),\n",
    "                PythonCodeBlockTests(\n",
    "                    code_tests=[\n",
    "                        'assert mask_emails(\"no email\") == \"no email\"',\n",
    "                        'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"',\n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've included a second `PromptTest` object. It's now a more clear that the PromptTest class is a way to define a single prompt and list of checks to test that specific prompt. We've also added a `MatchCheck` which ensures the response contains at least one assertion statement.\n",
    "\n",
    "**NOTE**: One important caveat is that we've moved `PythonCodeBlockTests` to the 2nd/last PromptTest. The reason for this (also explained in the class documentation) is that this check runs all of the code blocks generated across all responses. Therefore, this check should only be included once, on the last PromptTest object.\n",
    "\n",
    "Here is the equivalent change in our the dictionary version of our eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {\n",
    "    'metadata': {\n",
    "        'name': 'Mask Emails',\n",
    "        'description': 'Evaluates the ability of a model to mask emails in text',\n",
    "    },\n",
    "    'prompt_sequence': [\n",
    "        {\n",
    "            'prompt':\n",
    "                \"\"\"\n",
    "                Create a python function called `mask_emails` that takes a single string and masks all\n",
    "                emails in that string.\n",
    "\n",
    "                For each email in the format of `x@y.z`, the local part (`x`) should be masked with\n",
    "                [MASKED].\n",
    "                \"\"\",\n",
    "            'checks': [\n",
    "                {\n",
    "                    'check_type': 'REGEX',\n",
    "                    'pattern': 'def mask_emails\\\\([a-zA-Z_]+(\\\\: str)?\\\\)( -> str)?\\\\:',\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            'prompt':\" Create a set of assertion statements that test the function.\",\n",
    "            'checks': [\n",
    "                {\n",
    "                    'check_type': 'MATCH',\n",
    "                    'value': 'assert ',\n",
    "                },\n",
    "                {\n",
    "                    'check_type': 'PYTHON_CODE_BLOCK_TESTS',\n",
    "                    'code_tests': [\n",
    "                        'assert mask_emails(\"no email\") == \"no email\"',\n",
    "                        'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"',\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Candidates\n",
    "\n",
    "A candidate is just a wrapper around an LLM/service that has a similar registration system (as Checks) that allows Candidates to be instantiated dynamically from dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_eval.candidates import OpenAICandidate\n",
    "\n",
    "candidate = OpenAICandidate(\n",
    "    metadata={'name': 'OpenAI GPT-3.5-Turbo (0125)'},\n",
    "    parameters={\n",
    "        'model_name': 'gpt-3.5-turbo-0125',\n",
    "        'system_message': \"You are a helpful AI assistant.\",\n",
    "        'temperature': 0.1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent dictionary representation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_dict = {\n",
    "    'metadata': {'name': 'OpenAI GPT-3.5-Turbo (0125)'},\n",
    "    'candidate_type': 'OPENAI',\n",
    "    'parameters': {\n",
    "        'model_name': 'gpt-3.5-turbo-0125',\n",
    "        'system_message': \"You are a helpful AI assistant.\",\n",
    "        'temperature': 0.1,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert OpenAICandidate.from_dict(candidate_dict).to_dict() == candidate_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a single Eval against a single Candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows running a single Eval against a Eingle candidate. However, as mentioned above, the most common pattern is to define Evals as yaml files, and load many evals and run them with the `EvalHarness`, which will be described in another notebook.\n",
    "\n",
    "The Eval object is callable, and takes a single candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-3.5-Turbo (0125)\n",
      "    Eval:                        Mask Emails\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0007\n",
      "    Total Response Time:         9.5 seconds\n",
      "    # of Response Characters:    1,510\n",
      "    Characters per Second:       159.3\n",
      "    # of Checks:                 3\n",
      "    # of Successful Checks:      1\n",
      "    % of Successful Checks:      33.3%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 2\n",
      "    # of Code Tests Defined:     2\n",
      "    # of Successful Code Tests:  1\n"
     ]
    }
   ],
   "source": [
    "result = eval_obj(candidate)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm_eval.eval.EvalResult"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Checks are stored in a `results` property of the `EvalResult`. The `results` property is a list of lists. Each outer lists corresponds to each (sequential) prompt we tested. Our Eval object had two prompts (PromptTests). The first was a request to generate the `mask_emails` function. The second was a request to create a set of assertion statements to test the function. \n",
    "\n",
    "Therefore, `result.results` will be a list of length `2`.\n",
    "\n",
    "Each item in the list is a list of CheckResult objects corresponding to the Checks in the PromptTest. The first test had one check (RegexCheck), the second had two checks (MatchCheck, and PythonCodeBlockTests).\n",
    "\n",
    "Therefore, `result.results[0]` will be a list of `1` CheckResult objects, and `result.results[1]` will be a list of `2` `CheckResult` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lists (which correspond to the number of prompts testsed): 2\n",
      "Number of CheckResults associated with the first prompt: 1\n",
      "Number of CheckResults associated with the second prompt: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of lists (which correspond to the number of prompts testsed): {len(result.results)}\")\n",
    "print(f\"Number of CheckResults associated with the first prompt: {len(result.results[0])}\")\n",
    "print(f\"Number of CheckResults associated with the second prompt: {len(result.results[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': True,\n",
       " 'success': True,\n",
       " 'metadata': {'check_type': 'REGEX',\n",
       "  'check_pattern': 'def mask_emails\\\\([a-zA-Z_]+(\\\\: str)?\\\\)( -> str)?\\\\:',\n",
       "  'check_negate': False,\n",
       "  'check_metadata': {}},\n",
       " 'result_type': 'PASS_FAIL'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.results[0][0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'check_type': 'REGEX',\n",
       " 'check_pattern': 'def mask_emails\\\\([a-zA-Z_]+(\\\\: str)?\\\\)( -> str)?\\\\:',\n",
       " 'check_negate': False,\n",
       " 'check_metadata': {}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.results[0][0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'check_type': 'PYTHON_CODE_BLOCK_TESTS',\n",
       " 'num_code_blocks': 2,\n",
       " 'num_code_blocks_successful': 2,\n",
       " 'code_blocks': ['import re\\n\\ndef mask_emails(input_string):\\n    email_pattern = r\\'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\\'\\n    masked_string = re.sub(email_pattern, \\'[MASKED]@[MASKED].[MASKED]\\', input_string)\\n    return masked_string\\n\\n# Test the function\\ninput_string = \"Please send the report to john.doe@example.com and jane.smith@example.org\"\\nmasked_output = mask_emails(input_string)\\nprint(masked_output)',\n",
       "  '# Test cases\\nassert mask_emails(\"Please send the report to john.doe@example.com\") == \"Please send the report to [MASKED]@[MASKED].[MASKED]\"\\nassert mask_emails(\"Contact us at info@example.org for more information\") == \"Contact us at [MASKED]@[MASKED].[MASKED] for more information\"\\nassert mask_emails(\"Emails: alice@example.com, bob@example.net\") == \"Emails: [MASKED]@[MASKED].[MASKED], [MASKED]@[MASKED].[MASKED]\"\\nassert mask_emails(\"No emails in this string\") == \"No emails in this string\"\\n\\nprint(\"All assertions passed. Function is working correctly.\")'],\n",
       " 'code_block_errors': [None, None],\n",
       " 'code_tests': ['assert mask_emails(\"no email\") == \"no email\"',\n",
       "  'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"'],\n",
       " 'num_code_tests': 2,\n",
       " 'num_code_tests_successful': 1,\n",
       " 'code_test_results': [True, False],\n",
       " 'code_test_errors': [None, None]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.results[1][1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.all_check_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.75,\n",
       " 'success': False,\n",
       " 'metadata': {'check_type': 'PYTHON_CODE_BLOCK_TESTS',\n",
       "  'num_code_blocks': 2,\n",
       "  'num_code_blocks_successful': 2,\n",
       "  'code_blocks': ['import re\\n\\ndef mask_emails(input_string):\\n    email_pattern = r\\'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\\'\\n    masked_string = re.sub(email_pattern, \\'[MASKED]@[MASKED].[MASKED]\\', input_string)\\n    return masked_string\\n\\n# Test the function\\ninput_string = \"Please send the report to john.doe@example.com and jane.smith@example.org\"\\nmasked_output = mask_emails(input_string)\\nprint(masked_output)',\n",
       "   '# Test cases\\nassert mask_emails(\"Please send the report to john.doe@example.com\") == \"Please send the report to [MASKED]@[MASKED].[MASKED]\"\\nassert mask_emails(\"Contact us at info@example.org for more information\") == \"Contact us at [MASKED]@[MASKED].[MASKED] for more information\"\\nassert mask_emails(\"Emails: alice@example.com, bob@example.net\") == \"Emails: [MASKED]@[MASKED].[MASKED], [MASKED]@[MASKED].[MASKED]\"\\nassert mask_emails(\"No emails in this string\") == \"No emails in this string\"\\n\\nprint(\"All assertions passed. Function is working correctly.\")'],\n",
       "  'code_block_errors': [None, None],\n",
       "  'code_tests': ['assert mask_emails(\"no email\") == \"no email\"',\n",
       "   'assert mask_emails(\"my email is a@b.c\") == \"my email is [MASKED]@b.c\"'],\n",
       "  'num_code_tests': 2,\n",
       "  'num_code_tests_successful': 1,\n",
       "  'code_test_results': [True, False],\n",
       "  'code_test_errors': [None, None]},\n",
       " 'success_threshold': 1.0,\n",
       " 'result_type': 'SCORE'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.all_check_results[-1].to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
