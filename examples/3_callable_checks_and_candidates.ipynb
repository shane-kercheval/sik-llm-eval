{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main use-cases of the `llm-eval` framework to allow users to define and run many Evals against many Candidates via yaml files. The Check and Candidate classes can be serialized and deserialized via `to_dict()`/`from_dict()`, and a registration system is used to instantiate the Check/Candidate objects when deserialized.\n",
    "\n",
    "However, the framework also allows users to define Checks/Candidates through code using callable objects such as lambda functions. Users can also define tests where the prompt is any type of object (and not restricted to string/dict/numeric as it is when defining Evals in yaml files) and Candidates can return any type of eval. This functionality allows users to easily define and run Evals in code (e.g. in unit tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to the root directory of the project\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_eval.candidates import CandidateResponse\n",
    "from llm_eval.eval import Eval\n",
    "\n",
    "def fake_candidate(input: dict) -> CandidateResponse:\n",
    "    fake_response = {'my_response': f\"This is a fake response for the prompt: '{str(input)}'.\"}\n",
    "    # all Candidates must return a CandidateResponse object\n",
    "    return CandidateResponse(response=fake_response)\n",
    "\n",
    "eval_ = Eval(\n",
    "    input=[{'my_prompt': \"This is a user's prompt.\"}],\n",
    "    checks=[\n",
    "        # a ResponseData object is passed to all checks (Check or callable) from the Eval\n",
    "        lambda data: 'my_response' in data.response,\n",
    "        lambda data: 'fake response' in str(data.response),\n",
    "        lambda data: 'my_prompt' in data.input,\n",
    "    ],\n",
    ")\n",
    "result = eval_(fake_candidate)\n",
    "print(f\"Num checks: {result.num_checks}\")\n",
    "print(f\"Num passed: {result.num_successful_checks}\")\n",
    "print(f\"Perc Passed: {result.perc_successful_checks:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
