{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: you will need to create a `.env` file with this entry `OPENAI_API_KEY=<your key here>` that contains your OpenAI API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating OpenAI 3.5 and 4.0 against two evals\n",
    "\n",
    "This example shows how the EvalHarness is most commonly used, which is to evaluate a large number of test cases (Evals) against a small to large number of LLMs (Candidates). \n",
    "\n",
    "For demonstration purposes, we'll show how to use the EvalHarness to evaluate OpenAI 3.5 and 4.0 against two fictitious evals. The candidates and evals in this example are defined in yaml files.\n",
    "\n",
    "An EvalHarness is a class that encapsulates a set of Evals and set of Candidates, and runs the Evals against each of the Candidates. It also provides additional options for multiprocessing, asynchronous requests (to the underlying LLMs), and callback mechansism for saving files or logging/handling errors.\n",
    "\n",
    "In this example, the Eval and Candidate objects are added to the EvalHarness via `add_evals_from_yamls(...)` and `add_candidates_from_yamls(...)` which, in this case, load all yaml files found in the directories provided. Eval and Candidate objects can be added to the EvalHarness directly through `add_evals()` and `add_candidates()`. Evals/candidates that are defined in yaml files can be individually added to the EvalHarness via `add_eval_from_yaml()` which takes a string to the yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /root/.local/lib/python3.11/site-packages (1.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# EvalHarness runs evals asychronously, so we need to install nest_asyncio to avoid errors\n",
    "# running the evals in a notebook\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Evals:  2\n",
      "# of Candidates:  2\n",
      "Starting eval_harness\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-3.5-Turbo (0125)\n",
      "    Eval:                        Fibonacci Sequence\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0007\n",
      "    Total Response Time:         9.5 seconds\n",
      "    # of Response Characters:    1,248\n",
      "    Characters per Second:       131.2\n",
      "    # of Checks:                 5\n",
      "    # of Successful Checks:      4\n",
      "    % of Successful Checks:      80.0%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 2\n",
      "    # of Code Tests Defined:     1\n",
      "    # of Successful Code Tests:  0\n",
      "---\n",
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-3.5-Turbo (0125)\n",
      "    Eval:                        Python Function to Mask Emails\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0007\n",
      "    Total Response Time:         8.4 seconds\n",
      "    # of Response Characters:    1,428\n",
      "    Characters per Second:       170.5\n",
      "    # of Checks:                 6\n",
      "    # of Successful Checks:      5\n",
      "    % of Successful Checks:      83.3%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 1\n",
      "    # of Code Tests Defined:     2\n",
      "    # of Successful Code Tests:  1\n",
      "---\n",
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-4.0-Turbo (2024-04-09)\n",
      "    Eval:                        Fibonacci Sequence\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0371\n",
      "    Total Response Time:         51.1 seconds\n",
      "    # of Response Characters:    4,121\n",
      "    Characters per Second:       80.7\n",
      "    # of Checks:                 5\n",
      "    # of Successful Checks:      4\n",
      "    % of Successful Checks:      80.0%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 2\n",
      "    # of Code Tests Defined:     1\n",
      "    # of Successful Code Tests:  0\n",
      "---\n",
      "EvalResult:\n",
      "    Candidate:                  OpenAI GPT-4.0-Turbo (2024-04-09)\n",
      "    Eval:                        Python Function to Mask Emails\n",
      "    # of Prompts Tested:         2\n",
      "    Cost:                       $0.0474\n",
      "    Total Response Time:         57.0 seconds\n",
      "    # of Response Characters:    5,909\n",
      "    Characters per Second:       103.7\n",
      "    # of Checks:                 6\n",
      "    # of Successful Checks:      5\n",
      "    % of Successful Checks:      83.3%\n",
      "    # of Code Blocks Generated:  2\n",
      "    # of Successful Code Blocks: 1\n",
      "    # of Code Tests Defined:     2\n",
      "    # of Successful Code Tests:  2\n",
      "---\n",
      "Total time: 57.15618133544922\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from llm_eval.eval import EvalHarness, EvalResult\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # needed for running async in jupyter notebook\n",
    "\n",
    "def print_result(result: EvalResult) -> None:\n",
    "    \"\"\"\n",
    "    This function is used as a callback and prints the results of each evaluation.\n",
    "\n",
    "    The callback can also be used, for example, to save the results to a file. If you're\n",
    "    running a large number of evaluations, you may want to save the results to a file\n",
    "    periodically in case there are issues/errors before the entire EvalHarness completes.\n",
    "    \"\"\"\n",
    "    print(result)\n",
    "    print('---')\n",
    "\n",
    "harness = EvalHarness(callback=print_result)\n",
    "harness.add_evals_from_yamls('evals/*.yaml')\n",
    "harness.add_candidate_from_yaml('candidates/openai_3.5.yaml')\n",
    "harness.add_candidate_from_yaml('candidates/openai_4.0.yaml')\n",
    "\n",
    "print(\"# of Evals: \", len(harness.evals))\n",
    "print(\"# of Candidates: \", len(harness.candidates))\n",
    "\n",
    "print(\"Starting eval_harness\")\n",
    "start = time.time()\n",
    "results = harness()  # run the evals\n",
    "end = time.time()\n",
    "print(f\"Total time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code contains an example of how to summarize the eval results.\n",
    "\n",
    "The EvalHarness returns a list of lists. The outer list corresponds to each candidate and contains the eval results for that candate. So if there were 5 candidates evaluated the `results` object would be a list of 5 items (which are also lists). If there were 10 evals (evaulated against the 5 candidates) then each inner list would contain 10 `EvalResults` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for OpenAI GPT-3.5-Turbo (0125):\n",
      "  9/11 (81.8%) successful checks\n",
      "  3/4 (75.0%) successful code blocks\n",
      "Results for OpenAI GPT-4.0-Turbo (2024-04-09):\n",
      "  9/11 (81.8%) successful checks\n",
      "  3/4 (75.0%) successful code blocks\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_116ca\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_116ca_level0_col0\" class=\"col_heading level0 col0\" >name</th>\n",
       "      <th id=\"T_116ca_level0_col1\" class=\"col_heading level0 col1\" >Avg chars per second</th>\n",
       "      <th id=\"T_116ca_level0_col2\" class=\"col_heading level0 col2\" >Avg cost</th>\n",
       "      <th id=\"T_116ca_level0_col3\" class=\"col_heading level0 col3\" ># checks</th>\n",
       "      <th id=\"T_116ca_level0_col4\" class=\"col_heading level0 col4\" ># checks passed</th>\n",
       "      <th id=\"T_116ca_level0_col5\" class=\"col_heading level0 col5\" >% checks passed</th>\n",
       "      <th id=\"T_116ca_level0_col6\" class=\"col_heading level0 col6\" ># code blocks generated</th>\n",
       "      <th id=\"T_116ca_level0_col7\" class=\"col_heading level0 col7\" ># blocks successfully executed</th>\n",
       "      <th id=\"T_116ca_level0_col8\" class=\"col_heading level0 col8\" >% blocks successfully executed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_116ca_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_116ca_row0_col0\" class=\"data row0 col0\" >OpenAI GPT-3.5-Turbo (0125)</td>\n",
       "      <td id=\"T_116ca_row0_col1\" class=\"data row0 col1\" >150.9</td>\n",
       "      <td id=\"T_116ca_row0_col2\" class=\"data row0 col2\" >0.0007</td>\n",
       "      <td id=\"T_116ca_row0_col3\" class=\"data row0 col3\" >11</td>\n",
       "      <td id=\"T_116ca_row0_col4\" class=\"data row0 col4\" >9</td>\n",
       "      <td id=\"T_116ca_row0_col5\" class=\"data row0 col5\" >81.8%</td>\n",
       "      <td id=\"T_116ca_row0_col6\" class=\"data row0 col6\" >4</td>\n",
       "      <td id=\"T_116ca_row0_col7\" class=\"data row0 col7\" >3</td>\n",
       "      <td id=\"T_116ca_row0_col8\" class=\"data row0 col8\" >75.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_116ca_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_116ca_row1_col0\" class=\"data row1 col0\" >OpenAI GPT-4.0-Turbo (2024-04-09)</td>\n",
       "      <td id=\"T_116ca_row1_col1\" class=\"data row1 col1\" >92.2</td>\n",
       "      <td id=\"T_116ca_row1_col2\" class=\"data row1 col2\" >0.0423</td>\n",
       "      <td id=\"T_116ca_row1_col3\" class=\"data row1 col3\" >11</td>\n",
       "      <td id=\"T_116ca_row1_col4\" class=\"data row1 col4\" >9</td>\n",
       "      <td id=\"T_116ca_row1_col5\" class=\"data row1 col5\" >81.8%</td>\n",
       "      <td id=\"T_116ca_row1_col6\" class=\"data row1 col6\" >4</td>\n",
       "      <td id=\"T_116ca_row1_col7\" class=\"data row1 col7\" >3</td>\n",
       "      <td id=\"T_116ca_row1_col8\" class=\"data row1 col8\" >75.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff47e380d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_summary = []\n",
    "# each outer list in results corresponds to a candidate\n",
    "for cand_obj, cand_results in zip(harness.candidates, results):\n",
    "    candidate_name = cand_obj.metadata['name']\n",
    "    avg_chars_per_second = sum(r.characters_per_second for r in cand_results) / len(cand_results)\n",
    "    avg_cost = sum(r.cost for r in cand_results) / len(cand_results)\n",
    "    num_checks = sum(r.num_checks for r in cand_results)\n",
    "    num_successful_checks = sum(r.num_successful_checks for r in cand_results)\n",
    "    percent_success = num_successful_checks / num_checks\n",
    "    num_code_blocks_generated = sum(r.num_code_blocks for r in cand_results)\n",
    "    num_code_blocks_successful = sum(r.get_num_code_blocks_successful() for r in cand_results)\n",
    "    percent_code_blocks_successful = num_code_blocks_successful / num_code_blocks_generated\n",
    "    results_summary.append({\n",
    "        'name': candidate_name,\n",
    "        'Avg chars per second': avg_chars_per_second,\n",
    "        'Avg cost': avg_cost,\n",
    "        '# checks': num_checks,\n",
    "        '# checks passed': num_successful_checks,\n",
    "        '% checks passed': percent_success,\n",
    "        '# code blocks generated': num_code_blocks_generated,\n",
    "        '# blocks successfully executed': num_code_blocks_successful,\n",
    "        '% blocks successfully executed': percent_code_blocks_successful,\n",
    "    })\n",
    "    print(f\"Results for {candidate_name}:\")\n",
    "    print(f\"  {num_successful_checks}/{num_checks} ({percent_success:.1%}) successful checks\")\n",
    "    print(f\"  {num_code_blocks_successful}/{num_code_blocks_generated} ({percent_code_blocks_successful:.1%}) successful code blocks\")  # noqa\n",
    "\n",
    "pd.DataFrame(results_summary).style.format({\n",
    "    'Avg chars per second': '{:.1f}',\n",
    "    'Avg cost': '{:.4f}',\n",
    "    '% checks passed': '{:.1%}',\n",
    "    '% blocks successfully executed': '{:.1%}',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a single Eval against a single Candidate\n",
    "\n",
    "A less common scenario, which might be useful when generating evals or debugging, is running a single Eval against a signle Candidate. Eval objects are callable and can be executed by passing a candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult:\n",
      "    # of Prompts Tested:         1\n",
      "    Cost:                       $0.0002\n",
      "    Total Response Time:         2.7 seconds\n",
      "    # of Response Characters:    346\n",
      "    Characters per Second:       130.2\n",
      "    # of Checks:                 2\n",
      "    # of Successful Checks:      2\n",
      "    % of Successful Checks:      100.0%\n",
      "    # of Code Blocks Generated:  1\n"
     ]
    }
   ],
   "source": [
    "from llm_eval.candidates import OpenAICandidate\n",
    "from llm_eval.eval import Eval\n",
    "\n",
    "candidate = OpenAICandidate({'parameters': {'model_name': 'gpt-3.5-turbo-1106'}})\n",
    "eval_obj = Eval(prompt_sequence={\n",
    "    'prompt': \"Create a python function called `mask_emails` that uses regex to mask all emails.\",\n",
    "    'checks': [\n",
    "        {'check_type': 'CONTAINS', 'value': 'def mask_emails'},\n",
    "        {'check_type': 'PYTHON_CODE_BLOCKS_PRESENT'},\n",
    "    ],\n",
    "})\n",
    "result = eval_obj(candidate)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_obj': {'prompt_sequence': [{'prompt': 'Create a python function called `mask_emails` that uses regex to mask all emails.',\n",
       "    'checks': [{'value': 'def mask_emails', 'check_type': 'CONTAINS'},\n",
       "     {'check_type': 'PYTHON_CODE_BLOCKS_PRESENT'}]}]},\n",
       " 'candidate_obj': {'metadata': {'parameters': {'model_name': 'gpt-3.5-turbo-1106'}},\n",
       "  'candidate_type': 'OPENAI'},\n",
       " 'responses': ['```python\\nimport re\\n\\ndef mask_emails(text):\\n    email_pattern = r\\'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\\'\\n    masked_text = re.sub(email_pattern, \\'[email]\\', text)\\n    return masked_text\\n\\n# Example usage\\ntext = \"You can contact me at john.doe@example.com or jane_smith@gmail.com\"\\nmasked_text = mask_emails(text)\\nprint(masked_text)\\n```'],\n",
       " 'total_time_seconds': 2.658125400543213,\n",
       " 'num_code_blocks': 1,\n",
       " 'cost': 0.000175,\n",
       " 'timestamp': '2024-06-25 16:23:21 UTC',\n",
       " 'results': [[{'value': True,\n",
       "    'success': True,\n",
       "    'metadata': {'check_type': 'CONTAINS',\n",
       "     'check_value': 'def mask_emails',\n",
       "     'check_negate': False,\n",
       "     'check_metadata': {}},\n",
       "    'result_type': 'PASS_FAIL'},\n",
       "   {'value': True,\n",
       "    'success': True,\n",
       "    'metadata': {'check_type': 'PYTHON_CODE_BLOCKS_PRESENT',\n",
       "     'num_code_blocks': 1,\n",
       "     'min_code_blocks': 1,\n",
       "     'code_blocks': ['import re\\n\\ndef mask_emails(text):\\n    email_pattern = r\\'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\\'\\n    masked_text = re.sub(email_pattern, \\'[email]\\', text)\\n    return masked_text\\n\\n# Example usage\\ntext = \"You can contact me at john.doe@example.com or jane_smith@gmail.com\"\\nmasked_text = mask_emails(text)\\nprint(masked_text)']},\n",
       "    'result_type': 'PASS_FAIL'}]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
