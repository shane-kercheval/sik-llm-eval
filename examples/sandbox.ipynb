{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'description': 'This is an example of an eval. An eval contains '\n",
      "                             'a set of prompts and tests.',\n",
      "              'difficulty': 1,\n",
      "              'name': 'Eval Example',\n",
      "              'source': 'Anaconda',\n",
      "              'tags': ['example', 'graphing', 'plotly-express'],\n",
      "              'uuid': '5f8b1b4e-3b7e-4b0e-8b1b-4e3b7e4b0e8b'},\n",
      " 'prompts': [{'ideal_response': 'This is the ideal response (to the first '\n",
      "                                'prompt).\\n'\n",
      "                                'This field is optional.\\n'\n",
      "                                'It is only used if a test below is of type '\n",
      "                                '`llm-similarity`, in which case it is used as '\n",
      "                                'the ideal response and the LLM is asked to '\n",
      "                                'evaluate the similarity of the response to '\n",
      "                                'the ideal answer.\\n'\n",
      "                                'This value could also be used to fine-tune a '\n",
      "                                'model.\\n',\n",
      "              'prompt': 'This is a question/prompt.'},\n",
      "             {'ideal_response': 'This is the ideal response to the second '\n",
      "                                'prompt.',\n",
      "              'prompt': 'This is a followup prompt.\\n'\n",
      "                        'The intent is to evaluate multiple responses in a '\n",
      "                        'conversation, rather than a single response.\\n'}],\n",
      " 'tests': [{'metadata': {'description': 'This is an optional description of '\n",
      "                                        'the test that explains what the test '\n",
      "                                        'is checking or why the provided '\n",
      "                                        'answer is correct.',\n",
      "                         'tags': ['multiple-choice']},\n",
      "            'type': 'MATCH',\n",
      "            'values': [None,\n",
      "                       'This is the expected response. It must match '\n",
      "                       'exactly.']},\n",
      "           {'check': 'def check(code_blocks: list[str]) -> list[TestResult]:\\n'\n",
      "                     '  return True\\n',\n",
      "            'setup': 'import os\\n'\n",
      "                     'import sys\\n'\n",
      "                     'from example_test import test_function\\n',\n",
      "            'type': 'PYTHON_CODE_BLOCKS'},\n",
      "           {'file': 'example_test.py',\n",
      "            'function': 'test_function',\n",
      "            'type': 'PYTHON_FUNCTION'},\n",
      "           {'function': 'def test_function(response: str) -> '\n",
      "                        'list[TestResult]:\\n'\n",
      "                        '  expectation_1 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  expectation_2 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  return [expectation_1, expectation_2]\\n',\n",
      "            'type': 'python'},\n",
      "           {'instruction': 'Compare the response to the ideal response. Give a '\n",
      "                           'score between 0 and 1, where 0 is not similar at '\n",
      "                           'all and 1 is very similar.',\n",
      "            'model': 'gpt-3.5-turbo-1106',\n",
      "            'type': 'LLM'}]}\n",
      "---\n",
      "{'characters_per_second': 9.996,\n",
      " 'code_blocks_passed': 2,\n",
      " 'eval_id': 'uuid',\n",
      " 'model': 'gpt-3.5-turbo-1106',\n",
      " 'num_code_blocks': 2,\n",
      " 'response_characters': 1234,\n",
      " 'responses': ['This is a response to the first prompt.',\n",
      "               'This is a response to the second prompt.'],\n",
      " 'system': {'cpu': '...', 'gpu': '...', 'memory': '...'},\n",
      " 'test_results': {'num_results': 10,\n",
      "                  'percent_passed': 70.0,\n",
      "                  'results': [{'description': 'This is a description of the '\n",
      "                                              'test result.',\n",
      "                               'passed': True,\n",
      "                               'type': 'match'},\n",
      "                              {'passed': False}],\n",
      "                  'results_passed': 7},\n",
      " 'total_time': 123.45}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "with open('eval_template.yaml') as f:\n",
    "    eval_template = yaml.safe_load(f)\n",
    "\n",
    "with open('eval_result_example.yaml') as f:\n",
    "    eval_result_example = yaml.safe_load(f)\n",
    "\n",
    "pprint(eval_template)\n",
    "print('---')\n",
    "pprint(eval_result_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'attribution': 'OpenAI ChatGPT-4 was used to create the initial '\n",
      "                             'ideal_response values, and were slightly '\n",
      "                             'modified.',\n",
      "              'author': 'Anaconda',\n",
      "              'description': 'Create a function that returns the nth fibonacci '\n",
      "                             'number. Then, create a set of assertion states '\n",
      "                             'to test the function.',\n",
      "              'difficulty': 2,\n",
      "              'name': 'Fibonacci',\n",
      "              'tags': ['python']},\n",
      " 'prompts': [{'ideal_response': \"Here's a Python function named `fib` that \"\n",
      "                                'calculates and returns a list of the first '\n",
      "                                '`n` integers in the Fibonacci sequence. The '\n",
      "                                'function includes type hints and docstrings '\n",
      "                                'for clarity.\\n'\n",
      "                                '\\n'\n",
      "                                '```python\\n'\n",
      "                                'def fib(n: int) -> list[int]:\\n'\n",
      "                                '  \"\"\"\\n'\n",
      "                                '  Calculate the first n integers in the '\n",
      "                                'Fibonacci sequence.\\n'\n",
      "                                '\\n'\n",
      "                                '  Args:\\n'\n",
      "                                '      n (int): The number of elements in the '\n",
      "                                'Fibonacci sequence to generate.\\n'\n",
      "                                '\\n'\n",
      "                                '  Examples:\\n'\n",
      "                                '\\n'\n",
      "                                '  >>> fib(5)\\n'\n",
      "                                '  [0, 1, 1, 2, 3]\\n'\n",
      "                                '  >>> fib(10)\\n'\n",
      "                                '  [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\\n'\n",
      "                                '  \"\"\"\\n'\n",
      "                                '  if n <= 0:\\n'\n",
      "                                '      return []\\n'\n",
      "                                '  elif n == 1:\\n'\n",
      "                                '      return [0]\\n'\n",
      "                                '  else:\\n'\n",
      "                                '      fib_sequence = [0, 1]\\n'\n",
      "                                '      for _ in range(2, n):\\n'\n",
      "                                '          '\n",
      "                                'fib_sequence.append(fib_sequence[-1] + '\n",
      "                                'fib_sequence[-2])\\n'\n",
      "                                '      return fib_sequence\\n'\n",
      "                                '```\\n'\n",
      "                                '\\n'\n",
      "                                'This function starts the Fibonacci sequence '\n",
      "                                'with 0 and 1, and then iterates to calculate '\n",
      "                                'subsequent numbers by summing the two '\n",
      "                                'preceding numbers in the sequence. The '\n",
      "                                'function handles edge cases such as when n is '\n",
      "                                'less than or equal to 0.\\n',\n",
      "              'prompt': 'Create a python function named `fib` that calculates '\n",
      "                        'and returns a list of the first `n` integers in the '\n",
      "                        'fibonacci sequence. Use type hints and docstrings.'},\n",
      "             {'ideal_response': 'To thoroughly test the fib function, we need '\n",
      "                                'to create a set of assertion statements '\n",
      "                                'covering various scenarios, including typical '\n",
      "                                'use cases, edge cases, and any potential '\n",
      "                                'boundary conditions. Here are the assertion '\n",
      "                                'statements:\\n'\n",
      "                                '\\n'\n",
      "                                '```python\\n'\n",
      "                                '# Test for typical input values\\n'\n",
      "                                'assert fib(5) == [0, 1, 1, 2, 3], \"Test '\n",
      "                                'failed for n = 5\"\\n'\n",
      "                                'assert fib(10) == [0, 1, 1, 2, 3, 5, 8, 13, '\n",
      "                                '21, 34], \"Test failed for n = 10\"\\n'\n",
      "                                '\\n'\n",
      "                                '# Test for edge cases\\n'\n",
      "                                'assert fib(0) == [], \"Test failed for n = 0 '\n",
      "                                '(no elements)\"\\n'\n",
      "                                'assert fib(1) == [0], \"Test failed for n = 1 '\n",
      "                                '(single element)\"\\n'\n",
      "                                'assert fib(2) == [0, 1], \"Test failed for n = '\n",
      "                                '2 (two elements)\"\\n'\n",
      "                                '\\n'\n",
      "                                '# Test for negative input values\\n'\n",
      "                                'assert fib(-1) == [], \"Test failed for n = -1 '\n",
      "                                '(negative input)\"\\n'\n",
      "                                'assert fib(-10) == [], \"Test failed for n = '\n",
      "                                '-10 (negative input)\"\\n'\n",
      "                                '\\n'\n",
      "                                '# Test for large input value\\n'\n",
      "                                'assert len(fib(100)) == 100, \"Test failed for '\n",
      "                                'n = 100 (large input)\"\\n'\n",
      "                                '\\n'\n",
      "                                '# Test for non-integer input (should raise a '\n",
      "                                'TypeError)\\n'\n",
      "                                'try:\\n'\n",
      "                                '    fib(\"5\")\\n'\n",
      "                                '    assert False, \"Test failed for '\n",
      "                                'non-integer input (should raise TypeError)\"\\n'\n",
      "                                'except TypeError:\\n'\n",
      "                                '    pass\\n'\n",
      "                                '```\\n'\n",
      "                                '\\n'\n",
      "                                'This set of assertions covers the following '\n",
      "                                'scenarios:\\n'\n",
      "                                '\\n'\n",
      "                                '- Typical Cases: Tests with normal input '\n",
      "                                'values like 5 and 10.\\n'\n",
      "                                '- Edge Cases: Tests with edge values like 0, '\n",
      "                                '1, and 2.\\n'\n",
      "                                '- Negative Input: Tests the function with '\n",
      "                                'negative numbers to ensure it handles them '\n",
      "                                'gracefully.\\n'\n",
      "                                '- Large Input: Tests the function with a '\n",
      "                                'large value to check if it scales properly.\\n'\n",
      "                                '- Non-Integer Input: Verifies that the '\n",
      "                                'function raises an error when the input is '\n",
      "                                'not an integer.\\n'\n",
      "                                '\\n'\n",
      "                                'The last test case, which checks for '\n",
      "                                'non-integer input, will raise a TypeError '\n",
      "                                'since the function is not designed to handle '\n",
      "                                'this type of input. This test case is '\n",
      "                                'included to ensure that the function behaves '\n",
      "                                'as expected when encountering invalid input '\n",
      "                                'types.\\n',\n",
      "              'prompt': 'Create a set of assertion statements to test the '\n",
      "                        'function including all edge-cases.'}],\n",
      " 'tests': [{'checks': ['def check(code_blocks: list[list[str]]) -> '\n",
      "                       'list[TestResult]:\\n'\n",
      "                       '    assert fib(5) == [0, 1, 1, 2, 3], \"Test failed for '\n",
      "                       'n = 5\"\\n'\n",
      "                       '    assert fib(10) == [0, 1, 1, 2, 3, 5, 8, 13, 21, '\n",
      "                       '34], \"Test failed for n = 10\"\\n'\n",
      "                       '    # Test for edge cases\\n'\n",
      "                       '    assert fib(0) == [], \"Test failed for n = 0 (no '\n",
      "                       'elements)\"\\n'\n",
      "                       '    assert fib(1) == [0], \"Test failed for n = 1 '\n",
      "                       '(single element)\"\\n'\n",
      "                       '    assert fib(2) == [0, 1], \"Test failed for n = 2 '\n",
      "                       '(two elements)\"\\n'\n",
      "                       '    # Test for negative input values\\n'\n",
      "                       '    assert fib(-1) == [], \"Test failed for n = -1 '\n",
      "                       '(negative input)\"\\n'\n",
      "                       '    assert fib(-10) == [], \"Test failed for n = -10 '\n",
      "                       '(negative input)\"\\n'\n",
      "                       '    # Test for large input value\\n'\n",
      "                       '    assert len(fib(100)) == 100, \"Test failed for n = '\n",
      "                       '100 (large input)\"\\n'\n",
      "                       '    # Test for non-integer input (should raise a '\n",
      "                       'TypeError)\\n'\n",
      "                       '    try:\\n'\n",
      "                       '        fib(\"5\")\\n'\n",
      "                       '        assert False, \"Test failed for non-integer '\n",
      "                       'input (should raise TypeError)\"\\n'\n",
      "                       '    except TypeError:\\n'\n",
      "                       '        pass\\n',\n",
      "                       'def check(code_blocks: list[list[str]]) -> '\n",
      "                       'list[TestResult]:\\n'\n",
      "                       \"    assert 'def fib(n: int) -> list[int]:' in \"\n",
      "                       'code_blocks[0][0], \"First code block should have the '\n",
      "                       'function definition with type-hints.\"\\n'],\n",
      "            'type': 'python_code_blocks'}],\n",
      " 'uuid': 'F392362B-BB18-425B-84F3-385D7B39A0EB'}\n"
     ]
    }
   ],
   "source": [
    "with open('eval_fibonacci.yaml') as f:\n",
    "    eval_fibonacci_config = yaml.safe_load(f)\n",
    "pprint(eval_fibonacci_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Calculate the first n integers in the Fibonacci sequence.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of elements in the Fibonacci sequence to generate.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> fib(5)\n",
    "    [0, 1, 1, 2, 3]\n",
    "    >>> fib(10)\n",
    "    [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [0]\n",
    "    else:\n",
    "        fib_sequence = [0, 1]\n",
    "        for _ in range(2, n):\n",
    "            fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
    "        return fib_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for typical input values\n",
    "assert fib(5) == [0, 1, 1, 2, 3], \"Test failed for n = 5\"\n",
    "assert fib(10) == [0, 1, 1, 2, 3, 5, 8, 13, 21, 34], \"Test failed for n = 10\"\n",
    "\n",
    "# Test for edge cases\n",
    "assert fib(0) == [], \"Test failed for n = 0 (no elements)\"\n",
    "assert fib(1) == [0], \"Test failed for n = 1 (single element)\"\n",
    "assert fib(2) == [0, 1], \"Test failed for n = 2 (two elements)\"\n",
    "\n",
    "# Test for negative input values\n",
    "assert fib(-1) == [], \"Test failed for n = -1 (negative input)\"\n",
    "assert fib(-10) == [], \"Test failed for n = -10 (negative input)\"\n",
    "\n",
    "# Test for large input value\n",
    "assert len(fib(100)) == 100, \"Test failed for n = 100 (large input)\"\n",
    "\n",
    "# Test for non-integer input (should raise a TypeError)\n",
    "try:\n",
    "    fib(\"5\")\n",
    "    assert False, \"Test failed for non-integer input (should raise TypeError)\"\n",
    "except TypeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're correct to be concerned. The code I provided in the previous example does not inherently prevent the imported module from affecting the current environment. When you import a module using importlib.import_module, it behaves just like a regular import statement. This means that if the module has any code at the top level (outside of function or class definitions), that code will be executed upon import, potentially affecting the global state or the current environment.\n",
    "\n",
    "To truly isolate the execution of an imported module, you would need a more robust solution, such as running the code in a separate process or using a sandboxing technique. Python's standard library doesn't provide a built-in way to completely sandbox a module, but you can use multiprocessing to achieve a similar effect. Here's an example using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import importlib\n",
    "\n",
    "def module_function_executor(module_name, function_name, args=(), kwargs={}):\n",
    "    module = importlib.import_module(module_name)\n",
    "    func = getattr(module, function_name)\n",
    "    return func(*args, **kwargs)\n",
    "\n",
    "def execute_in_process(module_name, function_name, args=(), kwargs={}):\n",
    "    result_queue = multiprocessing.Queue()\n",
    "    \n",
    "    def worker():\n",
    "        try:\n",
    "            result = module_function_executor(module_name, function_name, args, kwargs)\n",
    "            result_queue.put(result)\n",
    "        except Exception as e:\n",
    "            result_queue.put(e)\n",
    "    \n",
    "    process = multiprocessing.Process(target=worker)\n",
    "    process.start()\n",
    "    process.join()\n",
    "\n",
    "    result = result_queue.get()\n",
    "    if isinstance(result, Exception):\n",
    "        raise result\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "module_name = 'my_module'  # The name of the Python file without '.py'\n",
    "function_name = 'my_function'  # The name of the function in the module\n",
    "result = execute_in_process(module_name, function_name, (arg1,), {'kwarg_name': kwarg_value})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def ____test_function____(arg1, arg2):\n",
      "    return arg1 + arg2\n",
      "\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Original function string\n",
    "original_function_str = \"\"\"\n",
    "def some_unknown_function(arg1, arg2):\n",
    "    return arg1 + arg2\n",
    "\"\"\"\n",
    "\n",
    "# Replace the original function name with a generic one\n",
    "generic_function_name = \"____test_function____\"\n",
    "modified_function_str = re.sub(r'def \\w+', f'def {generic_function_name}', original_function_str)\n",
    "print(modified_function_str)\n",
    "\n",
    "# Execute the modified function string\n",
    "exec(modified_function_str)\n",
    "\n",
    "# Now call the function\n",
    "result = locals()[generic_function_name](10, 20)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum, auto\n",
    "import time\n",
    "from typing import Callable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class TestType(Enum):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    MATCH = auto()\n",
    "    PYTHON_FUNCTION = auto()\n",
    "    PYTHON_CODE_BLOCKS = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def to_enum(name: str) -> 'TestType':\n",
    "        \"\"\"Get a TestType from its name.\"\"\"\n",
    "        if isinstance(name, TestType):\n",
    "            return name\n",
    "        try:\n",
    "            return TestType[name.upper()]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"{name.upper()} is not a valid name for a TestType member\")\n",
    "\n",
    "\n",
    "class TestResult(BaseModel):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    result: bool | int | float | object\n",
    "    description: str\n",
    "    metadata: dict | None\n",
    "\n",
    "\n",
    "class EvalTest(ABC):\n",
    "    \"\"\"\n",
    "    An EvalTest corresponds to a single test defined in an Eval (an Eval can have multiple tests).\n",
    "    The EvalTest is responsible for evaluating the responses to the prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: not sure if i need eval_uuid since it's in the EvalResult object\n",
    "    def __init__(self, eval_uuid: str, metadata: dict | None = None) -> None:\n",
    "        super().__init__()\n",
    "        self.eval_uuid = eval_uuid\n",
    "        self.metadata = metadata or {}\n",
    "        self.result = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, responses: list[str]) -> None:\n",
    "        \"\"\"TODO document.\"\"\"\n",
    "\n",
    "\n",
    "class TestRegistry:\n",
    "    \"\"\"Registry for models.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._registry: dict[str, TestType] = {}\n",
    "\n",
    "    def register(self, name: str, cls: TestType) -> None:\n",
    "        \"\"\"Register a model with the registry.\"\"\"\n",
    "        if name in self._registry:\n",
    "            raise ValueError(f\"A model with name '{name}' is already registered.\")\n",
    "        self._registry[name] = cls\n",
    "\n",
    "    def create_test(self, test_type: TestType, params: dict) -> EvalTest:\n",
    "        \"\"\"Create a test from a config.\"\"\"\n",
    "        if test_type not in self._registry:\n",
    "            raise ValueError(f\"TestType '{test_type}' not found in registry.\")\n",
    "        return self._registry[test_type](**params)\n",
    "\n",
    "    def __contains__(self, value: str) -> bool:\n",
    "        \"\"\"Check if a model is registered.\"\"\"\n",
    "        return value in self._registry\n",
    "\n",
    "\n",
    "def register_test(test_type: TestType) -> EvalTest:\n",
    "    \"\"\"Decorator to register an EvalTest.\"\"\"\n",
    "    def decorator(cls: EvalTest) -> EvalTest:\n",
    "        assert issubclass(cls, EvalTest), \\\n",
    "            f\"Test '{test_type}' ({cls.__name__}) must extend TestType\"\n",
    "        assert (test_type not in TEST_REGISTRY), \\\n",
    "            f\"Test '{test_type}' already registered.\"\n",
    "        TEST_REGISTRY.register(test_type, cls)\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "TEST_REGISTRY = TestRegistry()\n",
    "\n",
    "\n",
    "@register_test(TestType.MATCH)\n",
    "class MatchTest(EvalTest):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "            eval_uuid: str,\n",
    "            values: list[str],\n",
    "            metadata: dict | None = None) -> None:\n",
    "        super().__init__(eval_uuid=eval_uuid, metadata=metadata)\n",
    "        self.values = values\n",
    "\n",
    "    def __call__(self, responses: list[str]) -> None:\n",
    "        \"\"\"TODO: document.\"\"\"\n",
    "        assert len(responses) == len(self.values), \\\n",
    "            f\"Number of responses ({len(responses)}) does not equal number of match values \" \\\n",
    "            f\"({len(self.values)})\"\n",
    "        # self.results = [r == v if v is not None else None for r, v in zip(responses, self.values)]\n",
    "        self.results = []\n",
    "        for r, v in zip(responses, self.values):\n",
    "            if v is None:\n",
    "                self.results.append(TestResult(result=None, description=\"TODO\", metadata={}))\n",
    "            else:\n",
    "                self.results.append(TestResult(result=r == v, description=\"TODO\", metadata={}))\n",
    "\n",
    "\n",
    "@register_test(TestType.PYTHON_FUNCTION)\n",
    "class PythonFunctionTest(EvalTest):\n",
    "    \"\"\"\n",
    "    Runs a Python function (using the LLM responses as input. A Python function is either\n",
    "    provided as a string, or the name of the function and the file path containing the function.\n",
    "    A Python function test could be used for anything from a simple regex check to using an LLM\n",
    "    to evaluate the responses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "            eval_uuid: str,\n",
    "            function: str | None = None,\n",
    "            function_name: str | None = None,\n",
    "            function_file: str | None = None,\n",
    "            metadata: dict | None = None) -> None:\n",
    "        super().__init__(eval_uuid=eval_uuid, metadata=metadata)\n",
    "        if function is None:\n",
    "            assert function_name is not None and function_file is not None, \\\n",
    "                \"Either function or function_name and function_file must be provided.\"  # noqa: PT018\n",
    "        self._function_str = function\n",
    "        self._function_name = function_name\n",
    "        self._function_file = function_file\n",
    "\n",
    "    def __call__(self, responses: list[str]) -> None:\n",
    "        \"\"\"TODO document.\"\"\"\n",
    "        return responses\n",
    "        # A slightly different requirement is that I have a python file and the name of a function in that file. I need to dynamically import everything in that file and execute the provided function, while passing in arguments. I don't want anything imported to affect the environment that is running it.\n",
    "\n",
    "\n",
    "@register_test(TestType.PYTHON_CODE_BLOCKS)\n",
    "class PythonCodeBlocksTest(EvalTest):\n",
    "    \"\"\"\n",
    "    This class is responsible for executing Python code blocks returned by the LLM and then\n",
    "    running the python function(s) defined in the test in the same environment as code blocks.\n",
    "    For example, if the code blocks define a pandas DataFrame, the function could be used to\n",
    "    check that the shape or data of the DataFrame matches expectations.\n",
    "\n",
    "    The difference between this class and PythonFunctionTest is that this class is responsible\n",
    "    for running tests against the code blocks returned by the LLM, whereas PythonFunctionTest\n",
    "    is responsible for running tests against the (string) responses returned by the LLM.\n",
    "    \"\"\"  # noqa: D404\n",
    "\n",
    "    def __init__(self,\n",
    "            eval_uuid: str,\n",
    "            code_setup: str | None = None,\n",
    "            checks: list[dict] | None = None,\n",
    "            # function: str | None = None,\n",
    "            # function_name: str | None = None,\n",
    "            # function_file: str | None = None,\n",
    "            metadata: dict | None = None) -> None:\n",
    "        super().__init__(eval_uuid=eval_uuid, metadata=metadata)\n",
    "        # if function is None:\n",
    "        #     assert function_name is not None and function_file is not None, \\\n",
    "        #         \"Either function or function_name and function_file must be provided.\"  # noqa: PT018\n",
    "        # self._function_str = function\n",
    "        # self._function_name = function_name\n",
    "        # self._function_file = function_file\n",
    "        self._checks = checks\n",
    "        self._code_setup = code_setup\n",
    "\n",
    "    def __call__(self, responses: list[str]) -> None:\n",
    "        \"\"\"TODO document.\"\"\"\n",
    "        # extract code blocks\n",
    "        # run code setup if provided\n",
    "        # run code blocks\n",
    "        # run function in same environent as code blocks\n",
    "        pass\n",
    "\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    prompt: str\n",
    "    ideal_response: str | None = None\n",
    "\n",
    "\n",
    "class EvalResult(BaseModel):\n",
    "    \"\"\"\n",
    "    An EvalResult is the result of evaluating a specific LLM against a specific Eval, potentially\n",
    "    using specific hardware. The hardware is not applicable for services like OpenAI's API, but\n",
    "    would be applicable for running locally or against specific/configurable hardware like Hugging\n",
    "    Face Endpoints or a custom server.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_id: str\n",
    "    eval_id: str\n",
    "    system: dict\n",
    "    # potential duplication of information, but i think we need it on this object\n",
    "    responses: list[str]\n",
    "    total_time: float\n",
    "    response_characters: int\n",
    "    characters_per_second: float\n",
    "    # this depends on a particular type of test, not sure i like that\n",
    "    num_code_blocks: int\n",
    "    code_blocks_passed: int\n",
    "    test_results: list[object]\n",
    "\n",
    "# need to Register the different types of Tests\n",
    "\n",
    "class Eval:\n",
    "    \"\"\"\n",
    "    An Eval defines a set of one or more prompts and tests that can be used to evaluate an LLM. If\n",
    "    more than one prompt is provided, the intent is evaluate the the conversation and, therefore,\n",
    "    it's expected that the underlying model/object will maintain state between prompts.\n",
    "\n",
    "    The Eval object is evaluated by calling it with a single model_id and a callable (wrapping the\n",
    "    LLM) that takes a prompt (string) and returns a response (string).\n",
    "\n",
    "    An Eval corresponds to a set of prompts, while the result of the Eval corresponds to the Eval\n",
    "    and a specific LLM, and potentially specific to the hardware used to run the LLM.\n",
    "\n",
    "    The tests are ran after all the prompts have been evaluated. Each test is passed a list of\n",
    "    responses (strings) and returns a TestResult object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            uuid: str,\n",
    "            metadata: dict,\n",
    "            prompts: list[Prompt],\n",
    "            tests: list[EvalTest],\n",
    "            ):\n",
    "        self.uuid = uuid\n",
    "        self.metadata = metadata\n",
    "        self.prompts = prompts\n",
    "        self.tests = tests\n",
    "        self.results = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config: dict, results: dict | None = None) -> 'Eval':  # noqa: ANN102\n",
    "        \"\"\"Creates an Eval object from a config/dictionary.\"\"\"\n",
    "        assert 'uuid' in config, \"uuid is a required field when creating an Eval object\"\n",
    "        prompts = [Prompt(**prompt) for prompt in config['prompts']]\n",
    "        # need to register the different types of tests\n",
    "        # tests = [EvalTest(**test) for test in config['tests']]\n",
    "        tests = []\n",
    "        for test in config['tests']:\n",
    "            test['eval_uuid'] = config['uuid']\n",
    "            tests.append(TEST_REGISTRY.create_test(\n",
    "                test_type=TestType.to_enum(test.pop('type')),\n",
    "                params=test,\n",
    "            ))\n",
    "        # tests = [\n",
    "        #     TEST_REGISTRY.create_test(test_type=TestType.to_enum(t.pop('type')), params=t)\n",
    "        #     for t in config['tests']\n",
    "        # ]\n",
    "        obj = cls(\n",
    "            uuid=config['uuid'],\n",
    "            metadata=config['metadata'] if 'metadata' in config else {},\n",
    "            prompts=prompts,\n",
    "            tests=tests,\n",
    "        )\n",
    "        if results is not None:\n",
    "            obj.results = EvalResult(**results)\n",
    "        return obj\n",
    "\n",
    "\n",
    "    def __call__(self, llm_id: str, llm: Callable[[str], str]) -> dict:\n",
    "        \"\"\"Evaluates the model against the prompts and tests.\"\"\"\n",
    "        start = time.time()\n",
    "        responses = [llm(p.prompt) for p in self.prompts]\n",
    "        end = time.time()\n",
    "        self._duration = end - start\n",
    "\n",
    "        # TODO\n",
    "        results = [test(responses) for test in self.tests]\n",
    "\n",
    "        self.results = EvalResult(\n",
    "            llm_id=llm_id,\n",
    "            eval_id=self.uuid,\n",
    "            system=self.metadata,\n",
    "            responses=responses,\n",
    "            total_time=self._duration,\n",
    "            response_characters=sum([len(r) for r in responses]),\n",
    "            characters_per_second=sum([len(r) for r in responses]) / self._duration,\n",
    "            num_code_blocks=0,\n",
    "            code_blocks_passed=0,\n",
    "            test_results=results,\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Returns a string representation of the Eval.\"\"\"\n",
    "        from textwrap import dedent\n",
    "        prompts = ',\\n                '.join([str(p) for p in self.prompts])\n",
    "        metadata = '' if not self.metadata else f'\\n            metadata={self.metadata},'\n",
    "        return dedent(f\"\"\"\n",
    "        Eval(\n",
    "            uuid={self.uuid},{metadata}\n",
    "            prompts=[\n",
    "                {prompts}\n",
    "            ],\n",
    "            tests=[{', '.join([str(type(t)) for t in self.tests])}]\n",
    "        )\n",
    "        \"\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_fibonacci.yaml') as f:\n",
    "    eval_fibonacci_config = yaml.safe_load(f)\n",
    "\n",
    "eval_fib = Eval.from_dict(eval_fibonacci_config)\n",
    "eval_fib(llm_id='test', llm=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval(\n",
      "    uuid=F392362B-BB18-425B-84F3-385D7B39A0EB,\n",
      "    metadata={'name': 'Fibonacci', 'description': 'Create a function that returns the nth fibonacci number. Then, create a set of assertion states to test the function.', 'author': 'Anaconda', 'attribution': 'OpenAI ChatGPT-4 was used to create the initial ideal_response values, and were slightly modified.', 'difficulty': 2, 'tags': ['python']},\n",
      "    prompts=[\n",
      "        prompt='Create a python function named `fib` that calculates and returns a list of the first `n` integers in the fibonacci sequence. Use type hints and docstrings.' ideal_response='Here\\'s a Python function named `fib` that calculates and returns a list of the first `n` integers in the Fibonacci sequence. The function includes type hints and docstrings for clarity.\\n\\n```python\\ndef fib(n: int) -> list[int]:\\n  \"\"\"\\n  Calculate the first n integers in the Fibonacci sequence.\\n\\n  Args:\\n      n (int): The number of elements in the Fibonacci sequence to generate.\\n\\n  Examples:\\n\\n  >>> fib(5)\\n  [0, 1, 1, 2, 3]\\n  >>> fib(10)\\n  [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\\n  \"\"\"\\n  if n <= 0:\\n      return []\\n  elif n == 1:\\n      return [0]\\n  else:\\n      fib_sequence = [0, 1]\\n      for _ in range(2, n):\\n          fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\\n      return fib_sequence\\n```\\n\\nThis function starts the Fibonacci sequence with 0 and 1, and then iterates to calculate subsequent numbers by summing the two preceding numbers in the sequence. The function handles edge cases such as when n is less than or equal to 0.\\n',\n",
      "        prompt='Create a set of assertion statements to test the function including all edge-cases.' ideal_response='To thoroughly test the fib function, we need to create a set of assertion statements covering various scenarios, including typical use cases, edge cases, and any potential boundary conditions. Here are the assertion statements:\\n\\n```python\\n# Test for typical input values\\nassert fib(5) == [0, 1, 1, 2, 3], \"Test failed for n = 5\"\\nassert fib(10) == [0, 1, 1, 2, 3, 5, 8, 13, 21, 34], \"Test failed for n = 10\"\\n\\n# Test for edge cases\\nassert fib(0) == [], \"Test failed for n = 0 (no elements)\"\\nassert fib(1) == [0], \"Test failed for n = 1 (single element)\"\\nassert fib(2) == [0, 1], \"Test failed for n = 2 (two elements)\"\\n\\n# Test for negative input values\\nassert fib(-1) == [], \"Test failed for n = -1 (negative input)\"\\nassert fib(-10) == [], \"Test failed for n = -10 (negative input)\"\\n\\n# Test for large input value\\nassert len(fib(100)) == 100, \"Test failed for n = 100 (large input)\"\\n\\n# Test for non-integer input (should raise a TypeError)\\ntry:\\n    fib(\"5\")\\n    assert False, \"Test failed for non-integer input (should raise TypeError)\"\\nexcept TypeError:\\n    pass\\n```\\n\\nThis set of assertions covers the following scenarios:\\n\\n- Typical Cases: Tests with normal input values like 5 and 10.\\n- Edge Cases: Tests with edge values like 0, 1, and 2.\\n- Negative Input: Tests the function with negative numbers to ensure it handles them gracefully.\\n- Large Input: Tests the function with a large value to check if it scales properly.\\n- Non-Integer Input: Verifies that the function raises an error when the input is not an integer.\\n\\nThe last test case, which checks for non-integer input, will raise a TypeError since the function is not designed to handle this type of input. This test case is included to ensure that the function behaves as expected when encountering invalid input types.\\n'\n",
      "    ],\n",
      "    tests=[<class '__main__.PythonCodeBlocksTest'>]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(eval_fib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalResult(llm_id='test', eval_id='F392362B-BB18-425B-84F3-385D7B39A0EB', system={'name': 'Fibonacci', 'description': 'Create a function that returns the nth fibonacci number. Then, create a set of assertion states to test the function.', 'author': 'Anaconda', 'attribution': 'OpenAI ChatGPT-4 was used to create the initial ideal_response values, and were slightly modified.', 'difficulty': 2, 'tags': ['python']}, responses=['Create a python function named `fib` that calculates and returns a list of the first `n` integers in the fibonacci sequence. Use type hints and docstrings.', 'Create a set of assertion statements to test the function including all edge-cases.'], total_time=3.0994415283203125e-06, response_characters=238, characters_per_second=76788027.07692307, num_code_blocks=0, code_blocks_passed=0, test_results=[])\n"
     ]
    }
   ],
   "source": [
    "pprint(eval_fib.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<__main__.Eval object at 0xffff8723e110>'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_fib.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    name: str | None = None\n",
    "    description: str | None = None\n",
    "    difficulty: int | None = None\n",
    "    tags: list[str] | None = None\n",
    "    source: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Callable, Dict\n",
    "import functools\n",
    "\n",
    "# TestResult class to encapsulate the result of each test\n",
    "class TestResult:\n",
    "    def __init__(self, passed: bool, description: str):\n",
    "        self.passed = passed\n",
    "        self.description = description\n",
    "\n",
    "# Abstract base class for tests\n",
    "class Test(ABC):\n",
    "    @abstractmethod\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        pass\n",
    "\n",
    "# Registry for test types\n",
    "test_registry = {}\n",
    "\n",
    "# Decorator to register test functions\n",
    "def register_test(test_type: str):\n",
    "    def decorator(test_func):\n",
    "        test_registry[test_type] = test_func\n",
    "        @functools.wraps(test_func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return test_func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Specific test implementations\n",
    "@register_test(\"match\")\n",
    "class MatchTest(Test):\n",
    "    def __init__(self, value: str):\n",
    "        self.value = value\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        return [TestResult(response == self.value, \"Match test\")]\n",
    "\n",
    "@register_test(\"code_blocks\")\n",
    "class CodeBlockTest(Test):\n",
    "    # Implementation for code block tests\n",
    "    ...\n",
    "\n",
    "@register_test(\"python\")\n",
    "class PythonFunctionTest(Test):\n",
    "    # Implementation for python function tests\n",
    "    ...\n",
    "\n",
    "@register_test(\"llm-similarity\")\n",
    "class LLMSimilarityTest(Test):\n",
    "    # Implementation for LLM similarity tests\n",
    "    ...\n",
    "\n",
    "# Test runner\n",
    "def run_tests(tests_config, response: str):\n",
    "    results = []\n",
    "    for test_config in tests_config:\n",
    "        test_type = test_config['type']\n",
    "        test_class = test_registry.get(test_type)\n",
    "        if test_class:\n",
    "            test = test_class(**test_config)  # Assuming other necessary parameters are passed\n",
    "            results.extend(test.run_test(response))\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"match\", \"value\": \"Expected response\"},\n",
    "    # Other test configurations\n",
    "]\n",
    "\n",
    "code_blocks = \"Some response to test\"\n",
    "test_results = run_tests(yaml_config, code_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import sys\n",
    "\n",
    "\n",
    "class CodeBlocksTestResult:\n",
    "    def __init__(\n",
    "            self,\n",
    "            code_blocks: list[list[str]],\n",
    "            ran_successfully: list[bool],\n",
    "            results: list[list[bool]],\n",
    "            metadata: dict | None = None):\n",
    "        self.passed = passed\n",
    "        self.description = description\n",
    "\n",
    "\n",
    "@register_test(\"code_blocks\")\n",
    "class PythonCodeBlockTest(Test):\n",
    "    def __init__(self, setup: str = None, checks: List[str] = None):\n",
    "        self.setup = setup\n",
    "        self.checks = checks or []\n",
    "\n",
    "    def run_test(self, code_blocks: list[list[str]]) -> List[TestResult]:\n",
    "        # each list item corresponds to a single response and may contain multiple code blocks\n",
    "\n",
    "\n",
    "        # Create a separate environment to run the code\n",
    "        local_env = {}\n",
    "        results = []\n",
    "\n",
    "        # Redirect stdout to capture print statements\n",
    "        stdout = io.StringIO()\n",
    "        with contextlib.redirect_stdout(stdout):\n",
    "            try:\n",
    "                # Execute setup code if present\n",
    "                if self.setup:\n",
    "                    exec(self.setup, globals(), local_env)\n",
    "\n",
    "                # Execute the main code block (response)\n",
    "                exec(code_blocks, globals(), local_env)\n",
    "\n",
    "                # Execute checks if present\n",
    "                for check in self.checks:\n",
    "                    exec(check, globals(), local_env)\n",
    "\n",
    "                # If no errors, the code block test passes\n",
    "                results.append(TestResult(True, \"Code executed without errors\"))\n",
    "            except Exception as e:\n",
    "                # If there's an error, the test fails\n",
    "                results.append(TestResult(False, f\"Error executing code: {e}\"))\n",
    "\n",
    "        # Optionally, include captured stdout in the test result\n",
    "        captured_output = stdout.getvalue()\n",
    "        if captured_output:\n",
    "            results.append(TestResult(True, f\"Captured stdout: {captured_output}\"))\n",
    "\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"code_blocks\", \"setup\": \"import math\", \"checks\": [\"assert math.sqrt(4) == 2\"]},\n",
    "]\n",
    "\n",
    "response = \"print('Hello, world!')\"\n",
    "test_results = run_tests(yaml_config, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import types\n",
    "\n",
    "\n",
    "# TODO this needs to be ran in the same environment as the code block so we need to pass in the local_env\n",
    "\n",
    "\n",
    "@register_test(\"python\")\n",
    "class PythonFunctionTest(Test):\n",
    "    def __init__(self, file: str = None, function: str = None):\n",
    "        self.file = file\n",
    "        self.function_code = function\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        test_function = self._load_function()\n",
    "        if test_function:\n",
    "            return test_function(response)\n",
    "        else:\n",
    "            return [TestResult(False, \"Failed to load test function\")]\n",
    "\n",
    "    def _load_function(self) -> Callable:\n",
    "        if self.function_code:\n",
    "            # Execute inline-defined function\n",
    "            exec(self.function_code)\n",
    "            return locals()['test_function']\n",
    "        elif self.file:\n",
    "            # Dynamically import function from a file\n",
    "            spec = importlib.util.spec_from_file_location(\"module.name\", self.file)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            return getattr(module, 'test_function')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"python\", \"function\": \"def test_function(response): return [TestResult(response == 'expected response', 'Python function test')]\"},\n",
    "]\n",
    "\n",
    "code_blocks = \"expected response\"\n",
    "test_results = run_tests(yaml_config, code_blocks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
