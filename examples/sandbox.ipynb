{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'description': 'This is an example of an eval. An eval contains '\n",
      "                             'a set of prompts and tests.',\n",
      "              'difficulty': 1,\n",
      "              'name': 'Eval Example',\n",
      "              'source': 'Anaconda',\n",
      "              'tags': ['example', 'graphing', 'plotly-express'],\n",
      "              'uuid': '5f8b1b4e-3b7e-4b0e-8b1b-4e3b7e4b0e8b'},\n",
      " 'prompts': [{'ideal_response': 'This is the ideal response (to the first '\n",
      "                                'prompt).\\n'\n",
      "                                'This field is optional.\\n'\n",
      "                                'It is only used if a test below is of type '\n",
      "                                '`llm-similarity`, in which case it is used as '\n",
      "                                'the ideal response and the LLM is asked to '\n",
      "                                'evaluate the similarity of the response to '\n",
      "                                'the ideal answer.\\n'\n",
      "                                'This value could also be used to fine-tune a '\n",
      "                                'model.\\n',\n",
      "              'value': 'This is a question/prompt.'},\n",
      "             {'ideal_response': 'This is the ideal response to the second '\n",
      "                                'prompt.',\n",
      "              'value': 'This is a followup prompt.\\n'\n",
      "                       'The intent is to evaluate multiple responses in a '\n",
      "                       'conversation, rather than a single response.\\n'}],\n",
      " 'tests': [{'description': 'This is an optional description of the test that '\n",
      "                           'explains what the test is checking or why the '\n",
      "                           'provided answer is correct.',\n",
      "            'tags': ['multiple-choice'],\n",
      "            'type': 'match',\n",
      "            'value': 'This is the expected response. It must match exactly.'},\n",
      "           {'checks': ['def check(code_blocks: list[str]) -> '\n",
      "                       'list[TestResult]:\\n'\n",
      "                       '  return True\\n'],\n",
      "            'setup': 'import os\\n'\n",
      "                     'import sys\\n'\n",
      "                     'from example_test import test_function\\n',\n",
      "            'type': 'python_code_blocks'},\n",
      "           {'file': 'example_test.py',\n",
      "            'function': 'test_function',\n",
      "            'type': 'python'},\n",
      "           {'function': 'def test_function(response: str) -> '\n",
      "                        'list[TestResult]:\\n'\n",
      "                        '  expectation_1 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  expectation_2 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  return [expectation_1, expectation_2]\\n',\n",
      "            'type': 'python'},\n",
      "           {'instruction': 'Compare the response to the ideal response. Give a '\n",
      "                           'score between 0 and 1, where 0 is not similar at '\n",
      "                           'all and 1 is very similar.',\n",
      "            'model': 'gpt-3.5-turbo-1106',\n",
      "            'type': 'llm'}]}\n",
      "---\n",
      "{'characters_per_second': 9.996,\n",
      " 'code_blocks_passed': 2,\n",
      " 'eval_id': 'uuid',\n",
      " 'model': 'gpt-3.5-turbo-1106',\n",
      " 'num_code_blocks': 2,\n",
      " 'response_characters': 1234,\n",
      " 'responses': ['This is a response to the first prompt.',\n",
      "               'This is a response to the second prompt.'],\n",
      " 'system': {'cpu': '...', 'gpu': '...', 'memory': '...'},\n",
      " 'test_results': {'num_results': 10,\n",
      "                  'percent_passed': 70.0,\n",
      "                  'results': [{'description': 'This is a description of the '\n",
      "                                              'test result.',\n",
      "                               'passed': True,\n",
      "                               'type': 'match'},\n",
      "                              {'passed': False}],\n",
      "                  'results_passed': 7},\n",
      " 'total_time': 123.45}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "with open('eval_template.yaml') as f:\n",
    "    eval_template = yaml.safe_load(f)\n",
    "\n",
    "with open('eval_result_example.yaml') as f:\n",
    "    eval_result_example = yaml.safe_load(f)\n",
    "\n",
    "pprint(eval_template)\n",
    "print('---')\n",
    "pprint(eval_result_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'description': 'This eval tests asks the LLM to create a '\n",
      "                             'function that returns the nth fibonacci number. '\n",
      "                             'It then follows up and asks it can create a set '\n",
      "                             'of assertion states to test the function.',\n",
      "              'difficulty': 2,\n",
      "              'name': 'Fibonacci',\n",
      "              'source': 'Anaconda',\n",
      "              'tags': ['python'],\n",
      "              'uuid': 'F392362B-BB18-425B-84F3-385D7B39A0EB'},\n",
      " 'prompts': [{'ideal_response': \"Here's a Python function named `fib` that \"\n",
      "                                'calculates and returns a list of the first '\n",
      "                                '`n` integers in the Fibonacci sequence. The '\n",
      "                                'function includes type hints and docstrings '\n",
      "                                'for clarity.\\n'\n",
      "                                '\\n'\n",
      "                                '```python\\n'\n",
      "                                'def fib(n: int) -> list[int]:\\n'\n",
      "                                '  \"\"\"\\n'\n",
      "                                '  Calculate the first n integers in the '\n",
      "                                'Fibonacci sequence.\\n'\n",
      "                                '\\n'\n",
      "                                '  Args:\\n'\n",
      "                                '      n (int): The number of elements in the '\n",
      "                                'Fibonacci sequence to generate.\\n'\n",
      "                                '\\n'\n",
      "                                '  Examples:\\n'\n",
      "                                '\\n'\n",
      "                                '  >>> fib(5)\\n'\n",
      "                                '  [0, 1, 1, 2, 3]\\n'\n",
      "                                '  >>> fib(10)\\n'\n",
      "                                '  [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\\n'\n",
      "                                '  \"\"\"\\n'\n",
      "                                '  if n <= 0:\\n'\n",
      "                                '      return []\\n'\n",
      "                                '  elif n == 1:\\n'\n",
      "                                '      return [0]\\n'\n",
      "                                '  else:\\n'\n",
      "                                '      fib_sequence = [0, 1]\\n'\n",
      "                                '      for i in range(2, n):\\n'\n",
      "                                '          '\n",
      "                                'fib_sequence.append(fib_sequence[-1] + '\n",
      "                                'fib_sequence[-2])\\n'\n",
      "                                '      return fib_sequence\\n'\n",
      "                                '```\\n'\n",
      "                                '\\n'\n",
      "                                'This function starts the Fibonacci sequence '\n",
      "                                'with 0 and 1, and then iterates to calculate '\n",
      "                                'subsequent numbers by summing the two '\n",
      "                                'preceding numbers in the sequence. The '\n",
      "                                'function handles edge cases such as when n is '\n",
      "                                'less than or equal to 0.\\n',\n",
      "              'value': 'Create a python function named `fib` that calculates '\n",
      "                       'and returns a list of the first `n` integers in the '\n",
      "                       'fibonacci sequence. Use type hints and docstrings.'},\n",
      "             {'ideal_response': 'aaa\\n',\n",
      "              'value': 'Create a set of assertion statements to test the '\n",
      "                       'function including all edge-cases.'}],\n",
      " 'tests': [{'description': 'This is an optional description of the test that '\n",
      "                           'explains what the test is checking or why the '\n",
      "                           'provided answer is correct.',\n",
      "            'tags': ['multiple-choice'],\n",
      "            'type': 'match',\n",
      "            'value': 'This is the expected response. It must match exactly.'},\n",
      "           {'checks': ['def check(code_blocks: list[str]) -> '\n",
      "                       'list[TestResult]:\\n'\n",
      "                       '  return True\\n'],\n",
      "            'setup': 'import os\\n'\n",
      "                     'import sys\\n'\n",
      "                     'from example_test import test_function\\n',\n",
      "            'type': 'python_code_blocks'},\n",
      "           {'file': 'example_test.py',\n",
      "            'function': 'test_function',\n",
      "            'type': 'python'},\n",
      "           {'function': 'def test_function(response: str) -> '\n",
      "                        'list[TestResult]:\\n'\n",
      "                        '  expectation_1 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  expectation_2 = TestResult(passed=..., '\n",
      "                        \"description='This is a description of the \"\n",
      "                        \"expecation.')\\n\"\n",
      "                        '  return [expectation_1, expectation_2]\\n',\n",
      "            'type': 'python'},\n",
      "           {'instruction': 'Compare the response to the ideal response. Give a '\n",
      "                           'score between 0 and 1, where 0 is not similar at '\n",
      "                           'all and 1 is very similar.',\n",
      "            'model': 'gpt-3.5-turbo-1106',\n",
      "            'type': 'llm'}]}\n"
     ]
    }
   ],
   "source": [
    "with open('eval_fibonacci.yaml') as f:\n",
    "    eval_fibonacci = yaml.safe_load(f)\n",
    "pprint(eval_fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Calculate the first n integers in the Fibonacci sequence.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of elements in the Fibonacci sequence to generate.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> fib(5)\n",
    "    [0, 1, 1, 2, 3]\n",
    "    >>> fib(10)\n",
    "    [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [0]\n",
    "    else:\n",
    "        fib_sequence = [0, 1]\n",
    "        for _ in range(2, n):\n",
    "            fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
    "        return fib_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for typical input values\n",
    "assert fib(5) == [0, 1, 1, 2, 3], \"Test failed for n = 5\"\n",
    "assert fib(10) == [0, 1, 1, 2, 3, 5, 8, 13, 21, 34], \"Test failed for n = 10\"\n",
    "\n",
    "# Test for edge cases\n",
    "assert fib(0) == [], \"Test failed for n = 0 (no elements)\"\n",
    "assert fib(1) == [0], \"Test failed for n = 1 (single element)\"\n",
    "assert fib(2) == [0, 1], \"Test failed for n = 2 (two elements)\"\n",
    "\n",
    "# Test for negative input values\n",
    "assert fib(-1) == [], \"Test failed for n = -1 (negative input)\"\n",
    "assert fib(-10) == [], \"Test failed for n = -10 (negative input)\"\n",
    "\n",
    "# Test for large input value\n",
    "assert len(fib(100)) == 100, \"Test failed for n = 100 (large input)\"\n",
    "\n",
    "# Test for non-integer input (should raise a TypeError)\n",
    "try:\n",
    "    fib(\"5\")\n",
    "    assert False, \"Test failed for non-integer input (should raise TypeError)\"\n",
    "except TypeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum, auto\n",
    "import time\n",
    "from typing import Callable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class TestType(Enum):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    MATCH = auto()\n",
    "    PYTHON_CODE = auto()\n",
    "    PYTHON_CODE_BLOCKS = auto()\n",
    "    LLM = auto()\n",
    "\n",
    "\n",
    "class TestResult(BaseModel):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    result: bool | int | float | object\n",
    "    description: str\n",
    "    metadata: dict = {}\n",
    "\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    prompt: str\n",
    "    ideal_response: str | None = None\n",
    "\n",
    "\n",
    "class EvalTest(ABC):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    def __init__(self, eval_uuid: str) -> None:\n",
    "        super().__init__()\n",
    "        self.eval_uuid = eval_uuid\n",
    "        self._result = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, responses: list[str]) -> TestResult:\n",
    "        \"\"\"TODO document.\"\"\"\n",
    "\n",
    "\n",
    "class EvalResult(BaseModel):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    llm_id: str\n",
    "    eval_id: str\n",
    "    system: dict\n",
    "    # potential duplication of information, but i think we need it on this object\n",
    "    responses: list[str]\n",
    "    total_time: float\n",
    "    response_characters: int\n",
    "    characters_per_second: float\n",
    "    # this depends on a particular type of test, not sure i like that\n",
    "    num_code_blocks: int\n",
    "    code_blocks_passed: int\n",
    "    test_results: list[TestResult]\n",
    "\n",
    "# need to Register the different types of Tests\n",
    "\n",
    "class Eval:\n",
    "    \"\"\"\n",
    "    An Eval defines a set of one or more prompts and tests that can be used to evaluate an LLM. If\n",
    "    more than one prompt is provided, the intent is evaluate the the conversation and, therefore,\n",
    "    it's expected that the underlying model/object will maintain state between prompts.\n",
    "\n",
    "    The Eval object is evaluated by calling it with a single model_id and a callable (wrapping the\n",
    "    LLM) that takes a prompt (string) and returns a response (string).\n",
    "\n",
    "    The tests are ran after all the prompts have been evaluated. Each test is passed a list of\n",
    "    responses (strings) and returns a TestResult object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            uuid: str,\n",
    "            metadata: dict,\n",
    "            prompts: list[Prompt],\n",
    "            tests: list[EvalTest],\n",
    "            ):\n",
    "        self.uuid = uuid\n",
    "        self.metadata = metadata\n",
    "        self.prompts = prompts\n",
    "        self.tests = tests\n",
    "        self.results = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config: dict) -> 'Eval':  # noqa: ANN102\n",
    "        \"\"\"Creates an Eval object from a config/dictionary.\"\"\"\n",
    "        assert 'uuid' in config, \"uuid is a required field when creating an Eval object\"\n",
    "        prompts = [Prompt(**prompt) for prompt in config['prompts']]\n",
    "        # need to register the different types of tests\n",
    "        tests = [EvalTest(**test) for test in config['tests']]\n",
    "        return cls(\n",
    "            uuid=config['uuid'],\n",
    "            metadata=config['metadata'] if 'metadata' in config else {},\n",
    "            prompts=prompts,\n",
    "            tests=tests,\n",
    "        )\n",
    "\n",
    "    def __call__(self, llm_id: str, llm: Callable[[str], str]) -> dict:\n",
    "        \"\"\"Evaluates the model against the prompts and tests.\"\"\"\n",
    "        start = time.time()\n",
    "        responses = [llm(p.prompt) for p in self.prompts]\n",
    "        end = time.time()\n",
    "        self._duration = end - start\n",
    "\n",
    "        # TODO\n",
    "        results = [test(responses) for test in self.tests]\n",
    "\n",
    "        self.results = EvalResult(\n",
    "            llm_id=llm_id,\n",
    "            eval_id=self.uuid,\n",
    "            system=self.metadata,\n",
    "            responses=responses,\n",
    "            total_time=self._duration,\n",
    "            response_characters=sum([len(r) for r in responses]),\n",
    "            characters_per_second=sum([len(r) for r in responses]) / self._duration,\n",
    "            num_code_blocks=0,\n",
    "            code_blocks_passed=0,\n",
    "            test_results=results,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    name: str | None = None\n",
    "    description: str | None = None\n",
    "    difficulty: int | None = None\n",
    "    tags: list[str] | None = None\n",
    "    source: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Callable, Dict\n",
    "import functools\n",
    "\n",
    "# TestResult class to encapsulate the result of each test\n",
    "class TestResult:\n",
    "    def __init__(self, passed: bool, description: str):\n",
    "        self.passed = passed\n",
    "        self.description = description\n",
    "\n",
    "# Abstract base class for tests\n",
    "class Test(ABC):\n",
    "    @abstractmethod\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        pass\n",
    "\n",
    "# Registry for test types\n",
    "test_registry = {}\n",
    "\n",
    "# Decorator to register test functions\n",
    "def register_test(test_type: str):\n",
    "    def decorator(test_func):\n",
    "        test_registry[test_type] = test_func\n",
    "        @functools.wraps(test_func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return test_func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Specific test implementations\n",
    "@register_test(\"match\")\n",
    "class MatchTest(Test):\n",
    "    def __init__(self, value: str):\n",
    "        self.value = value\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        return [TestResult(response == self.value, \"Match test\")]\n",
    "\n",
    "@register_test(\"code_blocks\")\n",
    "class CodeBlockTest(Test):\n",
    "    # Implementation for code block tests\n",
    "    ...\n",
    "\n",
    "@register_test(\"python\")\n",
    "class PythonFunctionTest(Test):\n",
    "    # Implementation for python function tests\n",
    "    ...\n",
    "\n",
    "@register_test(\"llm-similarity\")\n",
    "class LLMSimilarityTest(Test):\n",
    "    # Implementation for LLM similarity tests\n",
    "    ...\n",
    "\n",
    "# Test runner\n",
    "def run_tests(tests_config, response: str):\n",
    "    results = []\n",
    "    for test_config in tests_config:\n",
    "        test_type = test_config['type']\n",
    "        test_class = test_registry.get(test_type)\n",
    "        if test_class:\n",
    "            test = test_class(**test_config)  # Assuming other necessary parameters are passed\n",
    "            results.extend(test.run_test(response))\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"match\", \"value\": \"Expected response\"},\n",
    "    # Other test configurations\n",
    "]\n",
    "\n",
    "code_blocks = \"Some response to test\"\n",
    "test_results = run_tests(yaml_config, code_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import sys\n",
    "\n",
    "\n",
    "class CodeBlocksTestResult:\n",
    "    def __init__(\n",
    "            self,\n",
    "            code_blocks: list[list[str]],\n",
    "            ran_successfully: list[bool],\n",
    "            results: list[list[bool]],\n",
    "            metadata: dict | None = None):\n",
    "        self.passed = passed\n",
    "        self.description = description\n",
    "\n",
    "\n",
    "@register_test(\"code_blocks\")\n",
    "class PythonCodeBlockTest(Test):\n",
    "    def __init__(self, setup: str = None, checks: List[str] = None):\n",
    "        self.setup = setup\n",
    "        self.checks = checks or []\n",
    "\n",
    "    def run_test(self, code_blocks: list[list[str]]) -> List[TestResult]:\n",
    "        # each list item corresponds to a single response and may contain multiple code blocks\n",
    "\n",
    "\n",
    "        # Create a separate environment to run the code\n",
    "        local_env = {}\n",
    "        results = []\n",
    "\n",
    "        # Redirect stdout to capture print statements\n",
    "        stdout = io.StringIO()\n",
    "        with contextlib.redirect_stdout(stdout):\n",
    "            try:\n",
    "                # Execute setup code if present\n",
    "                if self.setup:\n",
    "                    exec(self.setup, globals(), local_env)\n",
    "\n",
    "                # Execute the main code block (response)\n",
    "                exec(code_blocks, globals(), local_env)\n",
    "\n",
    "                # Execute checks if present\n",
    "                for check in self.checks:\n",
    "                    exec(check, globals(), local_env)\n",
    "\n",
    "                # If no errors, the code block test passes\n",
    "                results.append(TestResult(True, \"Code executed without errors\"))\n",
    "            except Exception as e:\n",
    "                # If there's an error, the test fails\n",
    "                results.append(TestResult(False, f\"Error executing code: {e}\"))\n",
    "\n",
    "        # Optionally, include captured stdout in the test result\n",
    "        captured_output = stdout.getvalue()\n",
    "        if captured_output:\n",
    "            results.append(TestResult(True, f\"Captured stdout: {captured_output}\"))\n",
    "\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"code_blocks\", \"setup\": \"import math\", \"checks\": [\"assert math.sqrt(4) == 2\"]},\n",
    "]\n",
    "\n",
    "response = \"print('Hello, world!')\"\n",
    "test_results = run_tests(yaml_config, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import types\n",
    "\n",
    "\n",
    "# TODO this needs to be ran in the same environment as the code block so we need to pass in the local_env\n",
    "\n",
    "\n",
    "@register_test(\"python\")\n",
    "class PythonFunctionTest(Test):\n",
    "    def __init__(self, file: str = None, function: str = None):\n",
    "        self.file = file\n",
    "        self.function_code = function\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        test_function = self._load_function()\n",
    "        if test_function:\n",
    "            return test_function(response)\n",
    "        else:\n",
    "            return [TestResult(False, \"Failed to load test function\")]\n",
    "\n",
    "    def _load_function(self) -> Callable:\n",
    "        if self.function_code:\n",
    "            # Execute inline-defined function\n",
    "            exec(self.function_code)\n",
    "            return locals()['test_function']\n",
    "        elif self.file:\n",
    "            # Dynamically import function from a file\n",
    "            spec = importlib.util.spec_from_file_location(\"module.name\", self.file)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            return getattr(module, 'test_function')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"python\", \"function\": \"def test_function(response): return [TestResult(response == 'expected response', 'Python function test')]\"},\n",
    "]\n",
    "\n",
    "code_blocks = \"expected response\"\n",
    "test_results = run_tests(yaml_config, code_blocks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
