[project]
name = "sik-llm-eval"
version = "0.0.1"
description = "sik-llm-eval is a simple, yet flexible, framework primarily designed for evaluating Language Model Models (LLMs) on custom use cases."
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "anthropic>=0.54.0",
    "openai>=1.86.0",
    "pandas>=2.3.0",
    "pydantic>=2.11.6",
    "python-dotenv>=1.1.0",
    "pyyaml>=6.0.2",
    "requests>=2.32.4",
    "ruamel-yaml>=0.18.14",
    "tenacity>=9.1.2",
    "tiktoken>=0.9.0",
]

[dependency-groups]
dev = [
    "coverage>=7.9.1",
    "faker>=37.4.0",
    "ipykernel>=6.29.5",
    "pytest>=8.4.0",
    "pytest-asyncio>=1.0.0",
    "pytest-mock>=3.14.1",
    "ruff>=0.11.13",
]

[tool.hatch.metadata]
allow-direct-references = true

[tool.hatch.build.targets.wheel]
packages = ["src/sik-llm-eval"]

[build-system]
requires = ["hatchling>=1.17.1"]
build-backend = "hatchling.build"

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
pythonpath = ["src"]
timeout = 60
timeout_method = "signal"  # note this only works on unix; "thread" method (default) is safer but might not catch hanging subprocesses
asyncio_default_fixture_loop_scope = "function"
