{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import yaml\n",
    "from llm_evals.eval import Eval, Candidate\n",
    "\n",
    "async def fetch_data(i, eval_, candidate):\n",
    "    # if asyncio.iscoroutinefunction(eval_._call_candidate):\n",
    "    #     # If _call_candidate is a coroutine function\n",
    "    #     await eval_._call_candidate(candidate)\n",
    "    # else:\n",
    "        # If _call_candidate is a regular function\n",
    "    eval_._call_candidate(candidate)\n",
    "    return {\n",
    "        'i': i,\n",
    "        'eval_obj': eval_,\n",
    "    }\n",
    "\n",
    "\n",
    "async def main():\n",
    "    with open('../examples/candidates/openai_3.5_1106.yaml') as f:\n",
    "        cand_config = yaml.safe_load(f)\n",
    "\n",
    "    with open('../tests/fake_data/fake_eval_sum_two_numbers.yaml') as f:\n",
    "        eval_config = yaml.safe_load(f)\n",
    "\n",
    "    tasks = [\n",
    "        fetch_data(\n",
    "            i,\n",
    "            Eval(**eval_config),\n",
    "            Candidate.from_dict(cand_config),\n",
    "        )\n",
    "        for i in range(10)\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "# Run the main coroutine in the existing event loop\n",
    "results = await main()\n",
    "# You can now work with the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.0, 'h'),\n",
       " (2.0, 'h'),\n",
       " (2.0, 'h'),\n",
       " (2.0, 'h'),\n",
       " (2.0, 'h'),\n",
       " (2.0, 'h'),\n",
       " (2.0, 'h'),\n",
       " (2.0, 'h'),\n",
       " (2.0, 'h'),\n",
       " (2.0, 'h')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (round(result['eval_obj'].duration, 1), result['eval_obj'].responses[0][0:50])\n",
    "    for result in results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/functools.py:58: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import yaml\n",
    "from llm_evals.eval import Eval, Candidate\n",
    "\n",
    "async def fetch_data(i, eval_, candidate):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    # Run the synchronous _call_candidate in a separate thread\n",
    "    await loop.run_in_executor(None, eval_._call_candidate, candidate)\n",
    "    return {\n",
    "        'i': i,\n",
    "        'eval_obj': eval_,\n",
    "    }\n",
    "\n",
    "async def main():\n",
    "    with open('../examples/candidates/openai_3.5_1106.yaml') as f:\n",
    "        cand_config = yaml.safe_load(f)\n",
    "\n",
    "    with open('../tests/fake_data/fake_eval_sum_two_numbers.yaml') as f:\n",
    "        eval_config = yaml.safe_load(f)\n",
    "\n",
    "    tasks = [\n",
    "        fetch_data(\n",
    "            i,\n",
    "            Eval(**eval_config),\n",
    "            Candidate.from_dict(cand_config),\n",
    "        )\n",
    "        for i in range(10)\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "# Run the main coroutine in the existing event loop\n",
    "results = await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.7, 'Certainly! Below is the Python code for the `sum_t'),\n",
       " (5.8, \"Certainly! Here's a simple Python function called \"),\n",
       " (2.6, 'Sure! Here is a Python function called `sum_two_nu'),\n",
       " (5.0, 'Sure, here is a simple function in Python that tak'),\n",
       " (7.2, \"Certainly! Here's a simple implementation of the `\"),\n",
       " (4.1, \"Sure, here's a simple function in Python that take\"),\n",
       " (2.7, \"Sure! Here's a simple implementation of the `sum_t\"),\n",
       " (6.7, 'Certainly! Here is a simple Python function called'),\n",
       " (4.2, \"Sure! Here's a simple implementation in Python:\\n\\n`\"),\n",
       " (6.4, \"Sure! Here's a simple Python function called `sum_\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    (round(result['eval_obj'].duration, 1), result['eval_obj'].responses[0][0:50])\n",
    "    for result in results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def fetch_data(i):\n",
    "#     print(f\"{i}Starting data fetch...\")\n",
    "#     start = time.time()\n",
    "#     time.sleep(2)  # Simulates a delay, e.g., network delay\n",
    "#     end = time.time()\n",
    "#     print(f\"{i}Data fetched\")\n",
    "#     return {i: i, \"time\": end - start}\n",
    "\n",
    "# fetch_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Certainly! Below is an example of how you can write the `sum_two_numbers` function in Python:\\n\\n```python\\ndef sum_two_numbers(num1, num2):\\n    return num1 + num2\\n```\\n\\nYou can then call the function `sum_two_numbers` and pass in two numbers to get their sum. For example:\\n```python\\nresult = sum_two_numbers(5, 3)\\nprint(result)  # Output: 8\\n```']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "from llm_evals.eval import Eval, Candidate\n",
    "from llm_evals.llms.openai import OpenAIChat\n",
    "\n",
    "def fetch_data(i, eval_, candidate):\n",
    "    # print(f\"{i}Starting data fetch...\", flush=True)\n",
    "    # start = time.time()\n",
    "    eval_._call_candidate(candidate)\n",
    "    # response = eval_(candidate)\n",
    "    # time.sleep(2)  # Simulates a delay, e.g., network delay\n",
    "    # response = 'hello'\n",
    "    # chat = OpenAIChat(temperature=1)\n",
    "    # response = chat(\"Hello\")\n",
    "    # response = candidate(\"Hello. How are you?\")\n",
    "    # end = time.time()\n",
    "    # print(f\"{i}Data fetched\")\n",
    "    return {\n",
    "        'i': i,\n",
    "        'eval_obj': eval_,\n",
    "        # \"time\": duration,\n",
    "        # 'response_len': len(responses[0]),\n",
    "        # 'response': responses[0][0:50],\n",
    "    }\n",
    "\n",
    "with open('../examples/candidates/openai_3.5_1106.yaml') as f:\n",
    "    cand_config = yaml.safe_load(f)\n",
    "\n",
    "# with open('../examples/candidates/hugging_face_endpoint_mistral_a10g.yaml') as f:\n",
    "#     cand_config = yaml.safe_load(f)\n",
    "#     cand_config['model_parameters']['endpoint_url'] = os.getenv('HUGGING_FACE_ENDPOINT_UNIT_TESTS')\n",
    "\n",
    "with open('../tests/fake_data/fake_eval_sum_two_numbers.yaml') as f:\n",
    "    eval_config = yaml.safe_load(f)\n",
    "\n",
    "candidate = Candidate.from_dict(cand_config)\n",
    "eval_ = Eval(**eval_config)\n",
    "\n",
    "results = fetch_data(1, eval_, candidate)\n",
    "results['eval_obj'].responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.0, \"Sure! Here's a Python function that does that:\\n\\n``\"),\n",
       " (5.6, \"Certainly! Here's a simple Python function called \"),\n",
       " (1.9, \"Certainly! Here's a simple Python function that ta\"),\n",
       " (4.5, \"Certainly! Here's a simple implementation of the `\"),\n",
       " (1.9, \"Here's an example of the `sum_two_numbers` functio\"),\n",
       " (5.2, \"Certainly! Here's an example of the `sum_two_numbe\"),\n",
       " (3.4, \"Here's a simple Python function that does exactly \"),\n",
       " (3.2, 'Certainly! Below is the Python code for the `sum_t'),\n",
       " (3.2, 'Certainly! Here is a simple implementation of the '),\n",
       " (5.2, \"Sure, here's a simple Python function called `sum_\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import asyncio\n",
    "\n",
    "# async def main():\n",
    "# loop = asyncio.get_running_loop()\n",
    "loop = asyncio.get_event_loop()\n",
    "# List of tasks to run fetch_data with different arguments\n",
    "tasks = [\n",
    "    loop.run_in_executor(\n",
    "        None,\n",
    "        fetch_data,\n",
    "        i,\n",
    "        Eval(**eval_config),\n",
    "        Candidate.from_dict(cand_config),\n",
    "    )\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "# Wait for all tasks to complete and gather results\n",
    "results = await asyncio.gather(*tasks)\n",
    "    # print(\"Fetched data:\", results)\n",
    "# asyncio.run(main())\n",
    "# Directly await the coroutine in a Jupyter Notebook cell\n",
    "# await main()\n",
    "[\n",
    "    (round(result['eval_obj'].duration, 1), result['eval_obj'].responses[0][0:50])\n",
    "    for result in results\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = [r['eval_obj']._execute_eval() for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 272, 'total_time_seconds': 1.992722749710083, 'characters_per_second': 136.4965916824009, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 2}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 305, 'total_time_seconds': 5.599591970443726, 'characters_per_second': 54.46824467597527, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 1}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 298, 'total_time_seconds': 1.852891445159912, 'characters_per_second': 160.82962655410978, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 1}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 367, 'total_time_seconds': 4.454535007476807, 'characters_per_second': 82.38792982793301, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 2}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 270, 'total_time_seconds': 1.9410581588745117, 'characters_per_second': 139.09931532254518, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 2}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 294, 'total_time_seconds': 5.1993935108184814, 'characters_per_second': 56.54504565642566, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 2}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 304, 'total_time_seconds': 3.404085159301758, 'characters_per_second': 89.30443760047369, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 2}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 130, 'total_time_seconds': 3.239652395248413, 'characters_per_second': 40.12774952736317, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 1}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 223, 'total_time_seconds': 3.239128589630127, 'characters_per_second': 68.84565554707063, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 1}\n",
      "{'eval_uuid': '8F9FBF37-C18D-46AD-8C34-A0196739643D', 'candidate_uuid': '0E454A44-627C-4B56-915B-8DCFE9DF2A55', 'num_prompts': 1, 'response_characters': 333, 'total_time_seconds': 5.170685052871704, 'characters_per_second': 64.40151202277266, 'num_checks': 2, 'num_successful_checks': 2, 'perc_successful_checks': 1.0, 'num_code_blocks': 2}\n"
     ]
    }
   ],
   "source": [
    "from llm_evals.eval import eval_result_summarizer\n",
    "\n",
    "for er in eval_results:\n",
    "    print(eval_result_summarizer(er))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'gpt-3.5-turbo-1106',\n",
       " 'system_message': 'You are a helpful AI assistant.',\n",
       " 'temperature': 1,\n",
       " 'max_tokens': 4096}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate.model_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "\n",
    "# async def main():\n",
    "#     loop = asyncio.get_running_loop()\n",
    "    \n",
    "#     # Run the non-async function in a separate thread\n",
    "#     data = await loop.run_in_executor(None, fetch_data)\n",
    "#     print(\"Fetched:\", data)\n",
    "\n",
    "# # Run the async main function\n",
    "# asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
