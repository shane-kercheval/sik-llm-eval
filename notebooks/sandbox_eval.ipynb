{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "with open('/code/examples/eval_template.yaml') as f:\n",
    "    eval_template = yaml.safe_load(f)\n",
    "\n",
    "with open('/code/examples/eval_result_example.yaml') as f:\n",
    "    eval_result_example = yaml.safe_load(f)\n",
    "\n",
    "pprint(eval_template)\n",
    "print('---')\n",
    "pprint(eval_result_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/code/examples/eval_fibonacci.yaml') as f:\n",
    "    eval_fibonacci_config = yaml.safe_load(f)\n",
    "pprint(eval_fibonacci_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Calculate the first n integers in the Fibonacci sequence.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of elements in the Fibonacci sequence to generate.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> fib(5)\n",
    "    [0, 1, 1, 2, 3]\n",
    "    >>> fib(10)\n",
    "    [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [0]\n",
    "    else:\n",
    "        fib_sequence = [0, 1]\n",
    "        for _ in range(2, n):\n",
    "            fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
    "        return fib_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for typical input values\n",
    "assert fib(5) == [0, 1, 1, 2, 3], \"Test failed for n = 5\"\n",
    "assert fib(10) == [0, 1, 1, 2, 3, 5, 8, 13, 21, 34], \"Test failed for n = 10\"\n",
    "\n",
    "# Test for edge cases\n",
    "assert fib(0) == [], \"Test failed for n = 0 (no elements)\"\n",
    "assert fib(1) == [0], \"Test failed for n = 1 (single element)\"\n",
    "assert fib(2) == [0, 1], \"Test failed for n = 2 (two elements)\"\n",
    "\n",
    "# Test for negative input values\n",
    "assert fib(-1) == [], \"Test failed for n = -1 (negative input)\"\n",
    "assert fib(-10) == [], \"Test failed for n = -10 (negative input)\"\n",
    "\n",
    "# Test for large input value\n",
    "assert len(fib(100)) == 100, \"Test failed for n = 100 (large input)\"\n",
    "\n",
    "# Test for non-integer input (should raise a TypeError)\n",
    "try:\n",
    "    fib(\"5\")\n",
    "    assert False, \"Test failed for non-integer input (should raise TypeError)\"\n",
    "except TypeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_evals.checks import CHECK_REGISTRY\n",
    "CHECK_REGISTRY.registered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval(\n",
      "    uuid=8C297EA8-42B9-4237-9571-5C95A042E3BA,\n",
      "    metadata={'name': 'Python Function to Mask Emails', 'description': 'Creates a python function that uses regex to mask all emails.', 'difficulty': 1, 'tags': ['python', 'regex'], 'source': 'Anaconda. The ideal_responses were generated by ChatGPT 4.0 Turbo.'},\n",
      "    scenarios=[\n",
      "        Scenario(\n",
      "            prompt='Create a python function called `mask_emails` that uses regex to mask all emails. For each email in the format of `x@y.z`, the local part (`x`) should be masked, but the domain (`@y.z`) should be retained. Use type hints and docstrings.',\n",
      "            ideal_response=\"To create a Python function called `mask_emails` t...\",\n",
      "            checks=[\n",
      "                MatchContainsCheck(metadata={}),\n",
      "                MatchRegexCheck(metadata={}),\n",
      "                PythonCodeBlocksCheck(metadata={})\n",
      "            ],\n",
      "        ),\n",
      "        Scenario(\n",
      "            prompt='Create a set of assertion statements that test the function.',\n",
      "            ideal_response=\"To thoroughly test the mask_emails function, you s...\",\n",
      "            checks=[\n",
      "                MatchContainsCheck(metadata={}),\n",
      "                PythonCodeBlocksCheck(metadata={})\n",
      "            ],\n",
      "        )\n",
      "    ],\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from llm_evals.eval import Eval\n",
    "eval_fib = Eval.from_yaml('../evals/mask_emails.yaml')\n",
    "print(eval_fib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval(\n",
      "    uuid=None,\n",
      "    metadata={'name': 'Python Function to Mask Emails', 'description': 'Creates a python function that uses regex to mask all emails.', 'difficulty': 1, 'tags': ['python', 'regex'], 'source': 'Anaconda'},\n",
      "    scenarios=[\n",
      "        Scenario(\n",
      "            prompt='Create a python function called `mask_emails` that uses regex to mask all emails. For each email in the format of `x@y.z`, the local part (`x`) should be masked, but the domain (`@y.z`) should be retained. Use type hints and docstrings.',\n",
      "            ideal_response=\"...\",\n",
      "            checks=[\n",
      "                MatchContainsCheck(metadata={}),\n",
      "                MatchRegexCheck(metadata={}),\n",
      "                PythonCodeBlocksCheck(metadata={})\n",
      "            ],\n",
      "        ),\n",
      "        Scenario(\n",
      "            prompt='Create a set of assertion statements that test the function.',\n",
      "            ideal_response=\"...\",\n",
      "            checks=[\n",
      "                MatchContainsCheck(metadata={}),\n",
      "                PythonCodeBlocksCheck(metadata={})\n",
      "            ],\n",
      "        )\n",
      "    ],\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from llm_evals.checks import PythonCodeBlocksCheck, MatchContainsCheck, MatchRegexCheck\n",
    "from llm_evals.eval import Eval, Scenario\n",
    "\n",
    "def verify_mask_emails_with_no_email_returns_original_string(code_blocks: list[str]) -> bool:  # noqa\n",
    "    assert len(code_blocks) >= 1\n",
    "    value = 'This is a string with no email addresses'\n",
    "    # mask_emails should be defined in the code blocks and will be available in the environment\n",
    "    # that this function is executed in (if the code blocks successfully run).\n",
    "    assert mask_emails(value) == value  # noqa\n",
    "\n",
    "def verify_mask_emails_with_single_email_returns_masked_string(code_blocks: list[str]) -> bool:  # noqa\n",
    "    value = 'This is a string with an email Susie@McSusers.com.'\n",
    "    # mask_emails should be defined in the code blocks and will be available in the environment\n",
    "    # that this function is executed in (if the code blocks successfully run).\n",
    "    assert mask_emails(value) == 'This is a string with an email [EMAIL].'  # noqa\n",
    "\n",
    "regex_pattern = 'def mask_email\\s*\\(\\s*[a-zA-Z_][a-zA-Z0-9_]*\\s*(\\s*:\\s*[a-zA-Z_][a-zA-Z0-9_]*\\s*)?\\)\\s*:'  # noqa\n",
    "\n",
    "\n",
    "eval_fib = Eval(\n",
    "    metadata={\n",
    "        'name': 'Python Function to Mask Emails',\n",
    "        'description': 'Creates a python function that uses regex to mask all emails.',\n",
    "        'difficulty': 1,  # 1-5\n",
    "        'tags': ['python', 'regex'],\n",
    "        'source': 'Anaconda',\n",
    "    },\n",
    "    scenarios=[\n",
    "        Scenario(\n",
    "            prompt='Create a python function called `mask_emails` that uses regex to mask all emails. For each email in the format of `x@y.z`, the local part (`x`) should be masked, but the domain (`@y.z`) should be retained. Use type hints and docstrings.',  # noqa: E501\n",
    "            ideal_response='...',\n",
    "            checks=[\n",
    "                MatchContainsCheck(values=[\n",
    "                    'def mask_emails(',\n",
    "                    're.sub(',\n",
    "                ]),\n",
    "                MatchRegexCheck(patterns=[regex_pattern]),\n",
    "                PythonCodeBlocksCheck(\n",
    "                    # The code_setup is ran before the extracted code blocks are executed, in the\n",
    "                    # same environment as the code blocks. Providing code_setup is optional.\n",
    "                    code_setup='import re',\n",
    "                    # These functions run after the code blocks are extracted from the response\n",
    "                    # and executed. They run in the same environment as the code blocks.\n",
    "                    # Providing functions is optional. Regardless if functions are provided, the\n",
    "                    # PythonCodeBlocksCheck checks if the code blocks are successfully ran.\n",
    "                    functions=[\n",
    "                        verify_mask_emails_with_no_email_returns_original_string,\n",
    "                        verify_mask_emails_with_single_email_returns_masked_string,\n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        Scenario(\n",
    "            prompt='Create a set of assertion statements that test the function.',\n",
    "            ideal_response='...',\n",
    "            checks=[\n",
    "                MatchContainsCheck(values=['assert mask_emails(']),\n",
    "                PythonCodeBlocksCheck(),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "print(eval_fib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval_fib(\n",
    "    llm_id='test',\n",
    "    llm=lambda x: x,\n",
    "    metadata={\n",
    "        'system_info': {\n",
    "            'GPUS': 100,\n",
    "        },\n",
    "        'model_info': {\n",
    "            'system_message': 'You are a helpful AI assistant.',\n",
    "            'temperature': 0.01,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're correct to be concerned. The code I provided in the previous example does not inherently prevent the imported module from affecting the current environment. When you import a module using importlib.import_module, it behaves just like a regular import statement. This means that if the module has any code at the top level (outside of function or class definitions), that code will be executed upon import, potentially affecting the global state or the current environment.\n",
    "\n",
    "To truly isolate the execution of an imported module, you would need a more robust solution, such as running the code in a separate process or using a sandboxing technique. Python's standard library doesn't provide a built-in way to completely sandbox a module, but you can use multiprocessing to achieve a similar effect. Here's an example using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import importlib\n",
    "\n",
    "def module_function_executor(module_name, function_name, args=(), kwargs={}):\n",
    "    module = importlib.import_module(module_name)\n",
    "    func = getattr(module, function_name)\n",
    "    return func(*args, **kwargs)\n",
    "\n",
    "def execute_in_process(module_name, function_name, args=(), kwargs={}):\n",
    "    result_queue = multiprocessing.Queue()\n",
    "    \n",
    "    def worker():\n",
    "        try:\n",
    "            result = module_function_executor(module_name, function_name, args, kwargs)\n",
    "            result_queue.put(result)\n",
    "        except Exception as e:\n",
    "            result_queue.put(e)\n",
    "    \n",
    "    process = multiprocessing.Process(target=worker)\n",
    "    process.start()\n",
    "    process.join()\n",
    "\n",
    "    result = result_queue.get()\n",
    "    if isinstance(result, Exception):\n",
    "        raise result\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "module_name = 'my_module'  # The name of the Python file without '.py'\n",
    "function_name = 'my_function'  # The name of the function in the module\n",
    "result = execute_in_process(module_name, function_name, (arg1,), {'kwarg_name': kwarg_value})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Original function string\n",
    "original_function_str = \"\"\"\n",
    "def some_unknown_function(arg1, arg2):\n",
    "    return arg1 + arg2\n",
    "\"\"\"\n",
    "\n",
    "# Replace the original function name with a generic one\n",
    "generic_function_name = \"____test_function____\"\n",
    "modified_function_str = re.sub(r'def \\w+', f'def {generic_function_name}', original_function_str)\n",
    "print(modified_function_str)\n",
    "\n",
    "# Execute the modified function string\n",
    "exec(modified_function_str)\n",
    "\n",
    "# Now call the function\n",
    "result = locals()[generic_function_name](10, 20)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum, auto\n",
    "import time\n",
    "from typing import Callable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class TestType(Enum):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    MATCH = auto()\n",
    "    PYTHON_FUNCTION = auto()\n",
    "    PYTHON_CODE_BLOCKS = auto()\n",
    "\n",
    "    @staticmethod\n",
    "    def to_enum(name: str) -> 'TestType':\n",
    "        \"\"\"Get a TestType from its name.\"\"\"\n",
    "        if isinstance(name, TestType):\n",
    "            return name\n",
    "        try:\n",
    "            return TestType[name.upper()]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"{name.upper()} is not a valid name for a TestType member\")\n",
    "\n",
    "\n",
    "class TestResult(BaseModel):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    result: bool | int | float | object\n",
    "    description: str\n",
    "    metadata: dict | None\n",
    "\n",
    "\n",
    "class EvalTest(ABC):\n",
    "    \"\"\"\n",
    "    An EvalTest corresponds to a single test defined in an Eval (an Eval can have multiple tests).\n",
    "    The EvalTest is responsible for evaluating the responses to the prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: not sure if i need eval_uuid since it's in the EvalResult object\n",
    "    def __init__(self, eval_uuid: str, metadata: dict | None = None) -> None:\n",
    "        super().__init__()\n",
    "        self.eval_uuid = eval_uuid\n",
    "        self.metadata = metadata or {}\n",
    "        self.result = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, responses: list[str]) -> None:\n",
    "        \"\"\"TODO document.\"\"\"\n",
    "\n",
    "\n",
    "class TestRegistry:\n",
    "    \"\"\"Registry for models.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._registry: dict[str, TestType] = {}\n",
    "\n",
    "    def register(self, name: str, cls: TestType) -> None:\n",
    "        \"\"\"Register a model with the registry.\"\"\"\n",
    "        if name in self._registry:\n",
    "            raise ValueError(f\"A model with name '{name}' is already registered.\")\n",
    "        self._registry[name] = cls\n",
    "\n",
    "    def create_test(self, test_type: TestType, params: dict) -> EvalTest:\n",
    "        \"\"\"Create a test from a config.\"\"\"\n",
    "        if test_type not in self._registry:\n",
    "            raise ValueError(f\"TestType '{test_type}' not found in registry.\")\n",
    "        return self._registry[test_type](**params)\n",
    "\n",
    "    def __contains__(self, value: str) -> bool:\n",
    "        \"\"\"Check if a model is registered.\"\"\"\n",
    "        return value in self._registry\n",
    "\n",
    "\n",
    "def register_test(test_type: TestType) -> EvalTest:\n",
    "    \"\"\"Decorator to register an EvalTest.\"\"\"\n",
    "    def decorator(cls: EvalTest) -> EvalTest:\n",
    "        assert issubclass(cls, EvalTest), \\\n",
    "            f\"Test '{test_type}' ({cls.__name__}) must extend TestType\"\n",
    "        assert (test_type not in TEST_REGISTRY), \\\n",
    "            f\"Test '{test_type}' already registered.\"\n",
    "        TEST_REGISTRY.register(test_type, cls)\n",
    "        return cls\n",
    "    return decorator\n",
    "\n",
    "\n",
    "TEST_REGISTRY = TestRegistry()\n",
    "\n",
    "\n",
    "@register_test(TestType.MATCH)\n",
    "class MatchTest(EvalTest):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "            eval_uuid: str,\n",
    "            values: list[str],\n",
    "            metadata: dict | None = None) -> None:\n",
    "        super().__init__(eval_uuid=eval_uuid, metadata=metadata)\n",
    "        self.values = values\n",
    "\n",
    "    def __call__(self, responses: list[str]) -> None:\n",
    "        \"\"\"TODO: document.\"\"\"\n",
    "        assert len(responses) == len(self.values), \\\n",
    "            f\"Number of responses ({len(responses)}) does not equal number of match values \" \\\n",
    "            f\"({len(self.values)})\"\n",
    "        # self.results = [r == v if v is not None else None for r, v in zip(responses, self.values)]\n",
    "        self.results = []\n",
    "        for r, v in zip(responses, self.values):\n",
    "            if v is None:\n",
    "                self.results.append(TestResult(result=None, description=\"TODO\", metadata={}))\n",
    "            else:\n",
    "                self.results.append(TestResult(result=r == v, description=\"TODO\", metadata={}))\n",
    "\n",
    "\n",
    "@register_test(TestType.PYTHON_FUNCTION)\n",
    "class PythonFunctionTest(EvalTest):\n",
    "    \"\"\"\n",
    "    Runs a Python function (using the LLM responses as input. A Python function is either\n",
    "    provided as a string, or the name of the function and the file path containing the function.\n",
    "    A Python function test could be used for anything from a simple regex check to using an LLM\n",
    "    to evaluate the responses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "            eval_uuid: str,\n",
    "            function: str | None = None,\n",
    "            function_name: str | None = None,\n",
    "            function_file: str | None = None,\n",
    "            metadata: dict | None = None) -> None:\n",
    "        super().__init__(eval_uuid=eval_uuid, metadata=metadata)\n",
    "        if function is None:\n",
    "            assert function_name is not None and function_file is not None, \\\n",
    "                \"Either function or function_name and function_file must be provided.\"  # noqa: PT018\n",
    "        self._function_str = function\n",
    "        self._function_name = function_name\n",
    "        self._function_file = function_file\n",
    "\n",
    "    def __call__(self, responses: list[str]) -> None:\n",
    "        \"\"\"TODO document.\"\"\"\n",
    "        return responses\n",
    "        # A slightly different requirement is that I have a python file and the name of a function in that file. I need to dynamically import everything in that file and execute the provided function, while passing in arguments. I don't want anything imported to affect the environment that is running it.\n",
    "\n",
    "\n",
    "@register_test(TestType.PYTHON_CODE_BLOCKS)\n",
    "class PythonCodeBlocksTest(EvalTest):\n",
    "    \"\"\"\n",
    "    This class is responsible for executing Python code blocks returned by the LLM and then\n",
    "    running the python function(s) defined in the test in the same environment as code blocks.\n",
    "    For example, if the code blocks define a pandas DataFrame, the function could be used to\n",
    "    check that the shape or data of the DataFrame matches expectations.\n",
    "\n",
    "    The difference between this class and PythonFunctionTest is that this class is responsible\n",
    "    for running tests against the code blocks returned by the LLM, whereas PythonFunctionTest\n",
    "    is responsible for running tests against the (string) responses returned by the LLM.\n",
    "    \"\"\"  # noqa: D404\n",
    "\n",
    "    def __init__(self,\n",
    "            eval_uuid: str,\n",
    "            code_setup: str | None = None,\n",
    "            checks: list[dict] | None = None,\n",
    "            # function: str | None = None,\n",
    "            # function_name: str | None = None,\n",
    "            # function_file: str | None = None,\n",
    "            metadata: dict | None = None) -> None:\n",
    "        super().__init__(eval_uuid=eval_uuid, metadata=metadata)\n",
    "        # if function is None:\n",
    "        #     assert function_name is not None and function_file is not None, \\\n",
    "        #         \"Either function or function_name and function_file must be provided.\"  # noqa: PT018\n",
    "        # self._function_str = function\n",
    "        # self._function_name = function_name\n",
    "        # self._function_file = function_file\n",
    "        self._checks = checks\n",
    "        self._code_setup = code_setup\n",
    "\n",
    "    def __call__(self, responses: list[str]) -> None:\n",
    "        \"\"\"TODO document.\"\"\"\n",
    "        # extract code blocks\n",
    "        # run code setup if provided\n",
    "        # run code blocks\n",
    "        # run function in same environent as code blocks\n",
    "        pass\n",
    "\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    \"\"\"TODO document.\"\"\"\n",
    "\n",
    "    prompt: str\n",
    "    ideal_response: str | None = None\n",
    "\n",
    "\n",
    "class EvalResult(BaseModel):\n",
    "    \"\"\"\n",
    "    An EvalResult is the result of evaluating a specific LLM against a specific Eval, potentially\n",
    "    using specific hardware. The hardware is not applicable for services like OpenAI's API, but\n",
    "    would be applicable for running locally or against specific/configurable hardware like Hugging\n",
    "    Face Endpoints or a custom server.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_id: str\n",
    "    eval_id: str\n",
    "    system: dict\n",
    "    # potential duplication of information, but i think we need it on this object\n",
    "    responses: list[str]\n",
    "    total_time: float\n",
    "    response_characters: int\n",
    "    characters_per_second: float\n",
    "    # this depends on a particular type of test, not sure i like that\n",
    "    num_code_blocks: int\n",
    "    code_blocks_passed: int\n",
    "    test_results: list[object]\n",
    "\n",
    "# need to Register the different types of Tests\n",
    "\n",
    "class Eval:\n",
    "    \"\"\"\n",
    "    An Eval defines a set of one or more prompts and tests that can be used to evaluate an LLM. If\n",
    "    more than one prompt is provided, the intent is evaluate the the conversation and, therefore,\n",
    "    it's expected that the underlying model/object will maintain state between prompts.\n",
    "\n",
    "    The Eval object is evaluated by calling it with a single model_id and a callable (wrapping the\n",
    "    LLM) that takes a prompt (string) and returns a response (string).\n",
    "\n",
    "    An Eval corresponds to a set of prompts, while the result of the Eval corresponds to the Eval\n",
    "    and a specific LLM, and potentially specific to the hardware used to run the LLM.\n",
    "\n",
    "    The tests are ran after all the prompts have been evaluated. Each test is passed a list of\n",
    "    responses (strings) and returns a TestResult object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            uuid: str,\n",
    "            metadata: dict,\n",
    "            prompts: list[Prompt],\n",
    "            tests: list[EvalTest],\n",
    "            ):\n",
    "        self.uuid = uuid\n",
    "        self.metadata = metadata\n",
    "        self.prompts = prompts\n",
    "        self.tests = tests\n",
    "        self.results = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config: dict, results: dict | None = None) -> 'Eval':  # noqa: ANN102\n",
    "        \"\"\"Creates an Eval object from a config/dictionary.\"\"\"\n",
    "        assert 'uuid' in config, \"uuid is a required field when creating an Eval object\"\n",
    "        prompts = [Prompt(**prompt) for prompt in config['prompts']]\n",
    "        # need to register the different types of tests\n",
    "        # tests = [EvalTest(**test) for test in config['tests']]\n",
    "        tests = []\n",
    "        for test in config['tests']:\n",
    "            test['eval_uuid'] = config['uuid']\n",
    "            tests.append(TEST_REGISTRY.create_test(\n",
    "                test_type=TestType.to_enum(test.pop('type')),\n",
    "                params=test,\n",
    "            ))\n",
    "        # tests = [\n",
    "        #     TEST_REGISTRY.create_test(test_type=TestType.to_enum(t.pop('type')), params=t)\n",
    "        #     for t in config['tests']\n",
    "        # ]\n",
    "        obj = cls(\n",
    "            uuid=config['uuid'],\n",
    "            metadata=config['metadata'] if 'metadata' in config else {},\n",
    "            prompts=prompts,\n",
    "            tests=tests,\n",
    "        )\n",
    "        if results is not None:\n",
    "            obj.results = EvalResult(**results)\n",
    "        return obj\n",
    "\n",
    "\n",
    "    def __call__(self, llm_id: str, llm: Callable[[str], str]) -> dict:\n",
    "        \"\"\"Evaluates the model against the prompts and tests.\"\"\"\n",
    "        start = time.time()\n",
    "        responses = [llm(p.prompt) for p in self.prompts]\n",
    "        end = time.time()\n",
    "        self._duration = end - start\n",
    "\n",
    "        # TODO\n",
    "        results = [test(responses) for test in self.tests]\n",
    "\n",
    "        self.results = EvalResult(\n",
    "            llm_id=llm_id,\n",
    "            eval_id=self.uuid,\n",
    "            system=self.metadata,\n",
    "            responses=responses,\n",
    "            total_time=self._duration,\n",
    "            response_characters=sum([len(r) for r in responses]),\n",
    "            characters_per_second=sum([len(r) for r in responses]) / self._duration,\n",
    "            num_code_blocks=0,\n",
    "            code_blocks_passed=0,\n",
    "            test_results=results,\n",
    "        )\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Returns a string representation of the Eval.\"\"\"\n",
    "        from textwrap import dedent\n",
    "        prompts = ',\\n                '.join([str(p) for p in self.prompts])\n",
    "        metadata = '' if not self.metadata else f'\\n            metadata={self.metadata},'\n",
    "        return dedent(f\"\"\"\n",
    "        Eval(\n",
    "            uuid={self.uuid},{metadata}\n",
    "            prompts=[\n",
    "                {prompts}\n",
    "            ],\n",
    "            tests=[{', '.join([str(type(t)) for t in self.tests])}]\n",
    "        )\n",
    "        \"\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_fibonacci.yaml') as f:\n",
    "    eval_fibonacci_config = yaml.safe_load(f)\n",
    "\n",
    "eval_fib = Eval.from_dict(eval_fibonacci_config)\n",
    "eval_fib(llm_id='test', llm=lambda x: x)\n",
    "print(eval_fib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_fib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(eval_fib.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fib.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    name: str | None = None\n",
    "    description: str | None = None\n",
    "    difficulty: int | None = None\n",
    "    tags: list[str] | None = None\n",
    "    source: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Callable, Dict\n",
    "import functools\n",
    "\n",
    "# TestResult class to encapsulate the result of each test\n",
    "class TestResult:\n",
    "    def __init__(self, passed: bool, description: str):\n",
    "        self.passed = passed\n",
    "        self.description = description\n",
    "\n",
    "# Abstract base class for tests\n",
    "class Test(ABC):\n",
    "    @abstractmethod\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        pass\n",
    "\n",
    "# Registry for test types\n",
    "test_registry = {}\n",
    "\n",
    "# Decorator to register test functions\n",
    "def register_test(test_type: str):\n",
    "    def decorator(test_func):\n",
    "        test_registry[test_type] = test_func\n",
    "        @functools.wraps(test_func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return test_func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Specific test implementations\n",
    "@register_test(\"match\")\n",
    "class MatchTest(Test):\n",
    "    def __init__(self, value: str):\n",
    "        self.value = value\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        return [TestResult(response == self.value, \"Match test\")]\n",
    "\n",
    "@register_test(\"code_blocks\")\n",
    "class CodeBlockTest(Test):\n",
    "    # Implementation for code block tests\n",
    "    ...\n",
    "\n",
    "@register_test(\"python\")\n",
    "class PythonFunctionTest(Test):\n",
    "    # Implementation for python function tests\n",
    "    ...\n",
    "\n",
    "@register_test(\"llm-similarity\")\n",
    "class LLMSimilarityTest(Test):\n",
    "    # Implementation for LLM similarity tests\n",
    "    ...\n",
    "\n",
    "# Test runner\n",
    "def run_tests(tests_config, response: str):\n",
    "    results = []\n",
    "    for test_config in tests_config:\n",
    "        test_type = test_config['type']\n",
    "        test_class = test_registry.get(test_type)\n",
    "        if test_class:\n",
    "            test = test_class(**test_config)  # Assuming other necessary parameters are passed\n",
    "            results.extend(test.run_test(response))\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"match\", \"value\": \"Expected response\"},\n",
    "    # Other test configurations\n",
    "]\n",
    "\n",
    "code_blocks = \"Some response to test\"\n",
    "test_results = run_tests(yaml_config, code_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import sys\n",
    "\n",
    "\n",
    "class CodeBlocksTestResult:\n",
    "    def __init__(\n",
    "            self,\n",
    "            code_blocks: list[list[str]],\n",
    "            ran_successfully: list[bool],\n",
    "            results: list[list[bool]],\n",
    "            metadata: dict | None = None):\n",
    "        self.passed = passed\n",
    "        self.description = description\n",
    "\n",
    "\n",
    "@register_test(\"code_blocks\")\n",
    "class PythonCodeBlockTest(Test):\n",
    "    def __init__(self, setup: str = None, checks: List[str] = None):\n",
    "        self.setup = setup\n",
    "        self.checks = checks or []\n",
    "\n",
    "    def run_test(self, code_blocks: list[list[str]]) -> List[TestResult]:\n",
    "        # each list item corresponds to a single response and may contain multiple code blocks\n",
    "\n",
    "\n",
    "        # Create a separate environment to run the code\n",
    "        local_env = {}\n",
    "        results = []\n",
    "\n",
    "        # Redirect stdout to capture print statements\n",
    "        stdout = io.StringIO()\n",
    "        with contextlib.redirect_stdout(stdout):\n",
    "            try:\n",
    "                # Execute setup code if present\n",
    "                if self.setup:\n",
    "                    exec(self.setup, globals(), local_env)\n",
    "\n",
    "                # Execute the main code block (response)\n",
    "                exec(code_blocks, globals(), local_env)\n",
    "\n",
    "                # Execute checks if present\n",
    "                for check in self.checks:\n",
    "                    exec(check, globals(), local_env)\n",
    "\n",
    "                # If no errors, the code block test passes\n",
    "                results.append(TestResult(True, \"Code executed without errors\"))\n",
    "            except Exception as e:\n",
    "                # If there's an error, the test fails\n",
    "                results.append(TestResult(False, f\"Error executing code: {e}\"))\n",
    "\n",
    "        # Optionally, include captured stdout in the test result\n",
    "        captured_output = stdout.getvalue()\n",
    "        if captured_output:\n",
    "            results.append(TestResult(True, f\"Captured stdout: {captured_output}\"))\n",
    "\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"code_blocks\", \"setup\": \"import math\", \"checks\": [\"assert math.sqrt(4) == 2\"]},\n",
    "]\n",
    "\n",
    "response = \"print('Hello, world!')\"\n",
    "test_results = run_tests(yaml_config, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import types\n",
    "\n",
    "\n",
    "# TODO this needs to be ran in the same environment as the code block so we need to pass in the local_env\n",
    "\n",
    "\n",
    "@register_test(\"python\")\n",
    "class PythonFunctionTest(Test):\n",
    "    def __init__(self, file: str = None, function: str = None):\n",
    "        self.file = file\n",
    "        self.function_code = function\n",
    "\n",
    "    def run_test(self, response: str) -> List[TestResult]:\n",
    "        test_function = self._load_function()\n",
    "        if test_function:\n",
    "            return test_function(response)\n",
    "        else:\n",
    "            return [TestResult(False, \"Failed to load test function\")]\n",
    "\n",
    "    def _load_function(self) -> Callable:\n",
    "        if self.function_code:\n",
    "            # Execute inline-defined function\n",
    "            exec(self.function_code)\n",
    "            return locals()['test_function']\n",
    "        elif self.file:\n",
    "            # Dynamically import function from a file\n",
    "            spec = importlib.util.spec_from_file_location(\"module.name\", self.file)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(module)\n",
    "            return getattr(module, 'test_function')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "yaml_config = [\n",
    "    {\"type\": \"python\", \"function\": \"def test_function(response): return [TestResult(response == 'expected response', 'Python function test')]\"},\n",
    "]\n",
    "\n",
    "code_blocks = \"expected response\"\n",
    "test_results = run_tests(yaml_config, code_blocks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
