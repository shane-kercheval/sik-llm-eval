metadata:
  name: example
eval_type: PROMPT_COMPARISON
prompt_comparison:
  - shared_context: |
    def add(a, b):
       return a - b
  - prompt: |
    {{context}}

    Fix this code and add doc strings and type hints.
  - prompt: |
    Fix this code and add doc strings and type hints.

    ```
    {{context}}
    ```
  - checks:
      - check_type: CONTAINS
        value: "def add(int a, int b) -> int:"
      - check_type: PYTHON_CODE_BLOCKS_PRESENT
      - check_type: PYTHON_CODE_BLOCK_TESTS
          code_block_timeout: 5
          code_test_timeout: 5
          code_tests:
            - assert add(1, 2) == 3
            - assert add(5, 5) == 10
            - assert add(0, -1) == -1
            - assert add(10, 10) == 0  # fails


If we were to generate multiple Evals from this file, then how would we differentiate between them?
Would each prompt have a seperate id? Or would the entire Eval have a single id?
How to make this easily summarized by user?

- In our other Evals we don't care about comparing evals against each other, only comparing models (aggregiated or between a single eval)
- Now we care about comparing evals (prompts) against each other, and against models
- We need to be able to easily summarize the results of these comparisons
- We also want to compare single prompts against each other, and between models




